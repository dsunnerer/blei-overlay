diff --git a/Makefile b/Makefile
index 09a9bb824afa..d2c1686f0268 100644
--- a/Makefile
+++ b/Makefile
@@ -677,6 +677,9 @@ endif # KBUILD_EXTMOD
 # Defaults to vmlinux, but the arch makefile usually adds further targets
 all: vmlinux
 
+CFLAGS_PGO_CLANG := -fprofile-generate
+export CFLAGS_PGO_CLANG
+
 CFLAGS_GCOV	:= -fprofile-arcs -ftest-coverage
 ifdef CONFIG_CC_IS_GCC
 CFLAGS_GCOV	+= -fno-tree-loop-im
diff --git a/arch/Kconfig b/arch/Kconfig
index d3c4ab249e9c..3de7ad6f7e1a 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -1313,6 +1313,7 @@ config DYNAMIC_SIGFRAME
 	bool
 
 source "kernel/gcov/Kconfig"
+source "kernel/pgo/Kconfig"
 
 source "scripts/gcc-plugins/Kconfig"
 
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 5c2ccb85f2ef..b645e9b3cd54 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -108,6 +108,7 @@ config X86
 	select ARCH_SUPPORTS_KMAP_LOCAL_FORCE_MAP	if NR_CPUS <= 4096
 	select ARCH_SUPPORTS_LTO_CLANG
 	select ARCH_SUPPORTS_LTO_CLANG_THIN
+	select ARCH_SUPPORTS_PGO_CLANG		if X86_64
 	select ARCH_USE_BUILTIN_BSWAP
 	select ARCH_USE_MEMTEST
 	select ARCH_USE_QUEUED_RWLOCKS
diff --git a/arch/x86/boot/Makefile b/arch/x86/boot/Makefile
index b5aecb524a8a..5e0816f5c367 100644
--- a/arch/x86/boot/Makefile
+++ b/arch/x86/boot/Makefile
@@ -71,6 +71,7 @@ KBUILD_AFLAGS	:= $(KBUILD_CFLAGS) -D__ASSEMBLY__
 KBUILD_CFLAGS	+= $(call cc-option,-fmacro-prefix-map=$(srctree)/=)
 KBUILD_CFLAGS	+= -fno-asynchronous-unwind-tables
 GCOV_PROFILE := n
+PGO_PROFILE := n
 UBSAN_SANITIZE := n
 
 $(obj)/bzImage: asflags-y  := $(SVGA_MODE)
diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index e11813646051..6cbbeb180627 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -59,6 +59,7 @@ CFLAGS_sev.o += -I$(objtree)/arch/x86/lib/
 
 KBUILD_AFLAGS  := $(KBUILD_CFLAGS) -D__ASSEMBLY__
 GCOV_PROFILE := n
+PGO_PROFILE := n
 UBSAN_SANITIZE :=n
 
 KBUILD_LDFLAGS := -m elf_$(UTS_MACHINE)
diff --git a/arch/x86/crypto/Makefile b/arch/x86/crypto/Makefile
index f307c93fc90a..a177ff8ad782 100644
--- a/arch/x86/crypto/Makefile
+++ b/arch/x86/crypto/Makefile
@@ -86,6 +86,9 @@ nhpoly1305-sse2-y := nh-sse2-x86_64.o nhpoly1305-sse2-glue.o
 obj-$(CONFIG_CRYPTO_NHPOLY1305_AVX2) += nhpoly1305-avx2.o
 nhpoly1305-avx2-y := nh-avx2-x86_64.o nhpoly1305-avx2-glue.o
 
+# Disable PGO for curve25519-x86_64. With PGO enabled, clang runs out of
+# registers for some of the functions.
+PGO_PROFILE_curve25519-x86_64.o := n
 obj-$(CONFIG_CRYPTO_CURVE25519_X86) += curve25519-x86_64.o
 
 obj-$(CONFIG_CRYPTO_SM4_AESNI_AVX_X86_64) += sm4-aesni-avx-x86_64.o
diff --git a/arch/x86/entry/vdso/Makefile b/arch/x86/entry/vdso/Makefile
index a2dddcc189f6..73e207ad3257 100644
--- a/arch/x86/entry/vdso/Makefile
+++ b/arch/x86/entry/vdso/Makefile
@@ -180,6 +180,7 @@ quiet_cmd_vdso = VDSO    $@
 VDSO_LDFLAGS = -shared --hash-style=both --build-id=sha1 \
 	$(call ld-option, --eh-frame-hdr) -Bsymbolic
 GCOV_PROFILE := n
+PGO_PROFILE := n
 
 quiet_cmd_vdso_and_check = VDSO    $@
       cmd_vdso_and_check = $(cmd_vdso); $(cmd_vdso_check)
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 2ff3e600f426..5cd1e99519fd 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -32,6 +32,9 @@ KASAN_SANITIZE_paravirt.o				:= n
 KASAN_SANITIZE_sev.o					:= n
 KASAN_SANITIZE_cc_platform.o				:= n
 
+# Cannot write to profiling regions before the page tables are set up.
+PGO_PROFILE_head$(BITS).o				:= n
+
 # With some compiler versions the generated code results in boot hangs, caused
 # by several compilation units. To be safe, disable all instrumentation.
 KCSAN_SANITIZE := n
diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S
index 3d6dc12d198f..ea720b5c050a 100644
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@ -184,6 +184,8 @@ SECTIONS
 
 	BUG_TABLE
 
+	PGO_CLANG_DATA
+
 	ORC_UNWIND_TABLE
 
 	. = ALIGN(PAGE_SIZE);
diff --git a/arch/x86/platform/efi/Makefile b/arch/x86/platform/efi/Makefile
index 84b09c230cbd..5f22b31446ad 100644
--- a/arch/x86/platform/efi/Makefile
+++ b/arch/x86/platform/efi/Makefile
@@ -2,6 +2,7 @@
 OBJECT_FILES_NON_STANDARD_efi_thunk_$(BITS).o := y
 KASAN_SANITIZE := n
 GCOV_PROFILE := n
+PGO_PROFILE := n
 
 obj-$(CONFIG_EFI) 		+= quirks.o efi.o efi_$(BITS).o efi_stub_$(BITS).o
 obj-$(CONFIG_EFI_MIXED)		+= efi_thunk_$(BITS).o
diff --git a/arch/x86/purgatory/Makefile b/arch/x86/purgatory/Makefile
index 95ea17a9d20c..36f20e99da0b 100644
--- a/arch/x86/purgatory/Makefile
+++ b/arch/x86/purgatory/Makefile
@@ -23,6 +23,7 @@ targets += purgatory.ro purgatory.chk
 
 # Sanitizer, etc. runtimes are unavailable and cannot be linked here.
 GCOV_PROFILE	:= n
+PGO_PROFILE	:= n
 KASAN_SANITIZE	:= n
 UBSAN_SANITIZE	:= n
 KCSAN_SANITIZE	:= n
diff --git a/arch/x86/realmode/rm/Makefile b/arch/x86/realmode/rm/Makefile
index 83f1b6a56449..21797192f958 100644
--- a/arch/x86/realmode/rm/Makefile
+++ b/arch/x86/realmode/rm/Makefile
@@ -76,4 +76,5 @@ KBUILD_CFLAGS	:= $(REALMODE_CFLAGS) -D_SETUP -D_WAKEUP \
 KBUILD_AFLAGS	:= $(KBUILD_CFLAGS) -D__ASSEMBLY__
 KBUILD_CFLAGS	+= -fno-asynchronous-unwind-tables
 GCOV_PROFILE := n
+PGO_PROFILE := n
 UBSAN_SANITIZE := n
diff --git a/arch/x86/um/vdso/Makefile b/arch/x86/um/vdso/Makefile
index 5943387e3f35..54f5768f5853 100644
--- a/arch/x86/um/vdso/Makefile
+++ b/arch/x86/um/vdso/Makefile
@@ -64,6 +64,7 @@ quiet_cmd_vdso = VDSO    $@
 
 VDSO_LDFLAGS = -fPIC -shared -Wl,--hash-style=sysv
 GCOV_PROFILE := n
+PGO_PROFILE := n
 
 #
 # Install the unstripped copy of vdso*.so listed in $(vdso-install-y).
diff --git a/drivers/firmware/efi/libstub/Makefile b/drivers/firmware/efi/libstub/Makefile
index d0537573501e..2c20e503cccb 100644
--- a/drivers/firmware/efi/libstub/Makefile
+++ b/drivers/firmware/efi/libstub/Makefile
@@ -43,6 +43,7 @@ KBUILD_CFLAGS := $(filter-out $(CC_FLAGS_SCS), $(KBUILD_CFLAGS))
 KBUILD_CFLAGS := $(filter-out $(CC_FLAGS_LTO), $(KBUILD_CFLAGS))
 
 GCOV_PROFILE			:= n
+PGO_PROFILE			:= n
 # Sanitizer runtimes are unavailable and cannot be linked here.
 KASAN_SANITIZE			:= n
 KCSAN_SANITIZE			:= n
diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 42f3866bca69..2189839fe895 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -331,6 +331,37 @@
 #define DTPM_TABLE()
 #endif
 
+#ifdef CONFIG_PGO_CLANG
+#define PGO_CLANG_DATA							\
+	__llvm_prf_data : AT(ADDR(__llvm_prf_data) - LOAD_OFFSET) {	\
+		__llvm_prf_data_start = .;				\
+		*(__llvm_prf_data)					\
+		__llvm_prf_data_end = .;				\
+	}								\
+	__llvm_prf_cnts : AT(ADDR(__llvm_prf_cnts) - LOAD_OFFSET) {	\
+		__llvm_prf_cnts_start = .;				\
+		*(__llvm_prf_cnts)					\
+		__llvm_prf_cnts_end = .;				\
+	}								\
+	__llvm_prf_names : AT(ADDR(__llvm_prf_names) - LOAD_OFFSET) {	\
+		__llvm_prf_names_start = .;				\
+		*(__llvm_prf_names)					\
+		__llvm_prf_names_end = .;				\
+	}								\
+	__llvm_prf_vals : AT(ADDR(__llvm_prf_vals) - LOAD_OFFSET) {	\
+		__llvm_prf_vals_start = .;				\
+		*(__llvm_prf_vals)					\
+		__llvm_prf_vals_end = .;				\
+	}								\
+	__llvm_prf_vnds : AT(ADDR(__llvm_prf_vnds) - LOAD_OFFSET) {	\
+		__llvm_prf_vnds_start = .;				\
+		*(__llvm_prf_vnds)					\
+		__llvm_prf_vnds_end = .;				\
+	}
+#else
+#define PGO_CLANG_DATA
+#endif
+
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
 	__dtb_start = .;						\
@@ -1148,6 +1179,7 @@
 		CONSTRUCTORS						\
 	}								\
 	BUG_TABLE							\
+	PGO_CLANG_DATA
 
 #define INIT_TEXT_SECTION(inittext_align)				\
 	. = ALIGN(inittext_align);					\
diff --git a/include/linux/module.h b/include/linux/module.h
index c9f1200b2312..1cdc0e9e0265 100644
--- a/include/linux/module.h
+++ b/include/linux/module.h
@@ -498,6 +498,21 @@ struct module {
 	unsigned long *kprobe_blacklist;
 	unsigned int num_kprobe_blacklist;
 #endif
+#ifdef CONFIG_PGO_CLANG
+	/*
+	 * Keep in sync with the PGO_CLANG_DATA sections
+	 * in include/asm-generic/vmlinux.lds.h
+	 * The prf_xxx_size is the section size in bytes.
+	 */
+	void *prf_data; /* struct llvm_prf_data */
+	int prf_data_size;
+	void *prf_cnts;
+	int prf_cnts_size;
+	const void *prf_names;
+	int prf_names_size;
+	void *prf_vnds; /* struct llvm_prf_value_node */
+	int prf_vnds_size;
+#endif
 #ifdef CONFIG_HAVE_STATIC_CALL_INLINE
 	int num_static_call_sites;
 	struct static_call_site *static_call_sites;
diff --git a/kernel/Makefile b/kernel/Makefile
index 186c49582f45..2c8143232df6 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -114,6 +114,7 @@ obj-$(CONFIG_KCSAN) += kcsan/
 obj-$(CONFIG_SHADOW_CALL_STACK) += scs.o
 obj-$(CONFIG_HAVE_STATIC_CALL_INLINE) += static_call.o
 obj-$(CONFIG_CFI_CLANG) += cfi.o
+obj-$(CONFIG_PGO_CLANG) += pgo/
 
 obj-$(CONFIG_PERF_EVENTS) += events/
 
diff --git a/kernel/module.c b/kernel/module.c
index f25e7653aa15..4a539f323d4e 100644
--- a/kernel/module.c
+++ b/kernel/module.c
@@ -3404,6 +3404,13 @@ static int find_module_sections(struct module *mod, struct load_info *info)
 					      sizeof(*mod->static_call_sites),
 					      &mod->num_static_call_sites);
 #endif
+#ifdef CONFIG_PGO_CLANG
+	mod->prf_data = section_objs(info, "__llvm_prf_data", 1, &mod->prf_data_size);
+	mod->prf_cnts = section_objs(info, "__llvm_prf_cnts", 1, &mod->prf_cnts_size);
+	mod->prf_names = section_objs(info, "__llvm_prf_names", 1, &mod->prf_names_size);
+	mod->prf_vnds = section_objs(info, "__llvm_prf_vnds", 1, &mod->prf_vnds_size);
+#endif
+
 	mod->extable = section_objs(info, "__ex_table",
 				    sizeof(*mod->extable), &mod->num_exentries);
 
diff --git a/kernel/pgo/Kconfig b/kernel/pgo/Kconfig
new file mode 100644
index 000000000000..1ab152d028fa
--- /dev/null
+++ b/kernel/pgo/Kconfig
@@ -0,0 +1,45 @@
+# SPDX-License-Identifier: GPL-2.0-only
+menu "Profile Guided Optimization (PGO) (EXPERIMENTAL)"
+
+config ARCH_SUPPORTS_PGO_CLANG
+	bool
+
+config PGO_CLANG
+	bool "Enable clang's PGO-based kernel profiling"
+	depends on DEBUG_FS
+	depends on ARCH_SUPPORTS_PGO_CLANG
+	depends on CC_IS_CLANG
+	depends on !ARCH_WANTS_NO_INSTR || CC_HAS_NO_PROFILE_FN_ATTR
+	help
+	  This option enables clang's PGO (Profile Guided Optimization) based
+	  code profiling to better optimize the kernel.
+
+	  If unsure, say N.
+
+	  Run a representative workload for your application on a kernel
+	  compiled with this option and read the raw profile files from
+	  /sys/kernel/debug/pgo/*.profraw. These files need to be
+	  processed with llvm-profdata.
+
+	  The profiler creates files for each online cpu and running module:
+	  vmlinux.*.profraw, ext4.*.profraw, and so on.
+	  The profile data files may be merged with other collected
+	  raw profiles.
+
+	  Copy the processed profile file into vmlinux.profdata, and enable
+	  KCFLAGS=-fprofile-use=vmlinux.profdata to produce an optimized
+	  kernel.
+	  PGO_CLANG must be disabled when compiling optimized kernel.
+	  The used .config should otherwise be the same. Clang can tolerate
+	  small mismatches in the profile data but with caveat that any
+	  mismatching profile data is ignored.
+
+	  Note that a kernel compiled with profiling flags will be
+	  significantly larger and run slower. Also be sure to exclude files
+	  from profiling which are not linked to the kernel image to prevent
+	  linker errors.
+
+	  Note that the debugfs filesystem has to be mounted to access
+	  profiling data.
+
+endmenu
diff --git a/kernel/pgo/Makefile b/kernel/pgo/Makefile
new file mode 100644
index 000000000000..41e27cefd9a4
--- /dev/null
+++ b/kernel/pgo/Makefile
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0
+GCOV_PROFILE	:= n
+PGO_PROFILE	:= n
+
+obj-y	+= fs.o instrument.o
diff --git a/kernel/pgo/fs.c b/kernel/pgo/fs.c
new file mode 100644
index 000000000000..3c238bc0f4f9
--- /dev/null
+++ b/kernel/pgo/fs.c
@@ -0,0 +1,705 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2019 Google, Inc.
+ *
+ * Author:
+ *	Sami Tolvanen <samitolvanen@google.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#define pr_fmt(fmt)	"pgo: " fmt
+
+#include <linux/kernel.h>
+#include <linux/debugfs.h>
+#include <linux/fs.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include "pgo.h"
+
+static struct dentry *directory;
+
+struct prf_private_data {
+	void *buffer;
+	size_t size;
+	struct prf_cpu_object *link;
+};
+
+/* vmlinux's prf object */
+static struct prf_object prf_vmlinux;
+
+/* The prf_list */
+LIST_HEAD(prf_list);
+
+/*
+ * Raw profile data format:
+ *
+ *	- llvm_prf_header
+ *	- __llvm_prf_data
+ *	- __llvm_prf_cnts
+ *	- __llvm_prf_names
+ *	- zero padding to 8 bytes
+ *	- for each llvm_prf_data in __llvm_prf_data:
+ *		- llvm_prf_value_data
+ *			- llvm_prf_value_record + site count array
+ *				- llvm_prf_value_node_data
+ *				...
+ *			...
+ *		...
+ */
+
+static void prf_fill_header(struct prf_object *po, void **buffer)
+{
+	struct llvm_prf_header *header = *(struct llvm_prf_header **)buffer;
+
+#ifdef CONFIG_64BIT
+	header->magic = LLVM_INSTR_PROF_RAW_MAGIC_64;
+#else
+	header->magic = LLVM_INSTR_PROF_RAW_MAGIC_32;
+#endif
+	header->version = LLVM_VARIANT_MASK_IR_PROF | LLVM_INSTR_PROF_RAW_VERSION;
+	header->data_size = prf_data_count(po);
+	header->binary_ids_size = 0;
+	header->padding_bytes_before_counters = 0;
+	header->counters_size = prf_cnts_count(po);
+	header->padding_bytes_after_counters = 0;
+	header->names_size = prf_names_count(po);
+	header->counters_delta = (u64)po->cnts;
+	header->names_delta = (u64)po->names;
+	header->value_kind_last = LLVM_INSTR_PROF_IPVK_LAST;
+
+	*buffer += sizeof(*header);
+}
+
+/*
+ * Copy the source into the buffer, incrementing the pointer into buffer in the
+ * process.
+ */
+static void prf_copy_to_buffer(void **buffer, const void *src, unsigned long size)
+{
+	memcpy(*buffer, src, size);
+	*buffer += size;
+}
+
+static u32 __prf_get_value_size(struct llvm_prf_data *p, u32 *value_kinds)
+{
+	struct llvm_prf_value_node **nodes =
+		(struct llvm_prf_value_node **)p->values;
+	u32 kinds = 0;
+	u32 size = 0;
+	unsigned int kind;
+	unsigned int n;
+	unsigned int s = 0;
+
+	for (kind = 0; kind < ARRAY_SIZE(p->num_value_sites); kind++) {
+		unsigned int sites = p->num_value_sites[kind];
+
+		if (!sites)
+			continue;
+
+		/* Record + site count array */
+		size += prf_get_value_record_size(sites);
+		kinds++;
+
+		if (!nodes)
+			continue;
+
+		for (n = 0; n < sites; n++) {
+			u32 count = 0;
+			struct llvm_prf_value_node *site = nodes[s + n];
+
+			while (site && ++count <= U8_MAX)
+				site = site->next;
+
+			size += count *
+				sizeof(struct llvm_prf_value_node_data);
+		}
+
+		s += sites;
+	}
+
+	if (size)
+		size += sizeof(struct llvm_prf_value_data);
+
+	if (value_kinds)
+		*value_kinds = kinds;
+
+	return size;
+}
+
+static u32 prf_get_value_size(struct prf_cpu_object *pco)
+{
+	u32 size = 0;
+	struct llvm_prf_data *p;
+	struct llvm_prf_data *end = pco->data + pco->obj->data_num;
+
+	for (p = pco->data; p < end; p++)
+		size += __prf_get_value_size(p, NULL);
+
+	return size;
+}
+
+/* Serialize the profiling's value. */
+static void prf_serialize_value(struct llvm_prf_data *p, void **buffer)
+{
+	struct llvm_prf_value_data header;
+	struct llvm_prf_value_node **nodes =
+		(struct llvm_prf_value_node **)p->values;
+	unsigned int kind;
+	unsigned int n;
+	unsigned int s = 0;
+
+	header.total_size = __prf_get_value_size(p, &header.num_value_kinds);
+
+	if (!header.num_value_kinds)
+		/* Nothing to write. */
+		return;
+
+	prf_copy_to_buffer(buffer, &header, sizeof(header));
+
+	for (kind = 0; kind < ARRAY_SIZE(p->num_value_sites); kind++) {
+		struct llvm_prf_value_record *record;
+		u8 *counts;
+		unsigned int sites = p->num_value_sites[kind];
+
+		if (!sites)
+			continue;
+
+		/* Profiling value record. */
+		record = *(struct llvm_prf_value_record **)buffer;
+		*buffer += prf_get_value_record_header_size();
+
+		record->kind = kind;
+		record->num_value_sites = sites;
+
+		/* Site count array. */
+		counts = *(u8 **)buffer;
+		*buffer += prf_get_value_record_site_count_size(sites);
+
+		/*
+		 * If we don't have nodes, we can skip updating the site count
+		 * array, because the buffer is zero filled.
+		 */
+		if (!nodes)
+			continue;
+
+		for (n = 0; n < sites; n++) {
+			u32 count = 0;
+			struct llvm_prf_value_node *site = nodes[s + n];
+
+			while (site && ++count <= U8_MAX) {
+				prf_copy_to_buffer(buffer, site,
+						   sizeof(struct llvm_prf_value_node_data));
+				site = site->next;
+			}
+
+			counts[n] = (u8)count;
+		}
+
+		s += sites;
+	}
+}
+
+static void prf_serialize_values(struct prf_cpu_object *pco, void **buffer)
+{
+	struct llvm_prf_data *p;
+	struct llvm_prf_data *end = pco->data + pco->obj->data_num;
+
+	for (p = pco->data; p < end; p++)
+		prf_serialize_value(p, buffer);
+}
+
+static inline unsigned long prf_get_padding(unsigned long size)
+{
+	return 7 & (sizeof(u64) - size % sizeof(u64));
+}
+
+/* Note: caller *must* take prf_lock_exclusive() */
+static unsigned long prf_buffer_size(struct prf_cpu_object *pco)
+{
+	return sizeof(struct llvm_prf_header) +
+			prf_data_size(pco->obj)	+
+			prf_cnts_size(pco->obj) +
+			prf_names_size(pco->obj) +
+			prf_get_padding(prf_names_size(pco->obj)) +
+			prf_get_value_size(pco);
+}
+
+/*
+ * Serialize the profiling data into a format LLVM's tools can understand.
+ * Note: p->buffer must point into vzalloc()'d
+ * area of at least prf_buffer_size() in size.
+ * Note: caller *must* take prf_lock_exclusive()
+ */
+static void prf_serialize(struct prf_private_data *p)
+{
+	void *buffer;
+	struct prf_cpu_object *pco = p->link;
+
+	buffer = p->buffer;
+
+	prf_fill_header(pco->obj, &buffer);
+	prf_copy_to_buffer(&buffer, pco->data,  prf_data_size(pco->obj));
+	prf_copy_to_buffer(&buffer, pco->obj->cnts,  prf_cnts_size(pco->obj));
+	prf_copy_to_buffer(&buffer, pco->obj->names, prf_names_size(pco->obj));
+	buffer += prf_get_padding(prf_names_size(pco->obj));
+
+	prf_serialize_values(pco, &buffer);
+}
+
+/* open() implementation for PGO. Creates a copy of the profiling data set. */
+static int prf_open(struct inode *inode, struct file *file)
+{
+	struct prf_private_data *data;
+	int err = 0;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	/* take exclusive lock and stop the profiler. */
+	prf_lock_exclusive();
+
+	/* get prf_cpu_object of this inode */
+	data->link = inode->i_private;
+
+	/* allocate buffer for the profile data */
+	data->size = prf_buffer_size(data->link);
+	data->buffer = vzalloc(data->size);
+
+	if (!data->buffer) {
+		err = -ENOMEM;
+		goto out_err;
+	}
+
+	/* serialize the profiler dataset */
+	prf_serialize(data);
+
+	file->private_data = data;
+
+out_err:
+	if (err) {
+		/* clean up on error */
+		if (data)
+			kfree(data->buffer);
+
+		kfree(data);
+	}
+
+	prf_unlock_exclusive();
+
+	return err;
+}
+
+/* read() implementation for PGO. */
+static ssize_t prf_read(struct file *file, char __user *buf, size_t count,
+			loff_t *ppos)
+{
+	struct prf_private_data *data = file->private_data;
+
+	if (WARN_ON_ONCE(!data))
+		return -ENOMEM;
+
+	return simple_read_from_buffer(buf, count, ppos, data->buffer,
+				       data->size);
+}
+
+/* release() implementation for PGO. Release resources allocated by open(). */
+static int prf_release(struct inode *inode, struct file *file)
+{
+	struct prf_private_data *data = file->private_data;
+
+	if (data) {
+		vfree(data->buffer);
+		kfree(data);
+	}
+
+	return 0;
+}
+
+static const struct file_operations prf_fops = {
+	.owner		= THIS_MODULE,
+	.open		= prf_open,
+	.read		= prf_read,
+	.llseek		= default_llseek,
+	.release	= prf_release
+};
+
+static void pgo_percpu_free(struct prf_cpu_object *pco)
+{
+	int cpu;
+
+	if (pco) {
+		for_each_possible_cpu(cpu) {
+			kfree(pco[cpu].nodes);
+			kfree(pco[cpu].data);
+			kfree(pco[cpu].vnds);
+			debugfs_remove(pco[cpu].file);
+		}
+	}
+	kfree(pco);
+}
+
+/* Count number of value sites of prf_object. */
+static unsigned long pgo_num_value_sites(struct prf_object *po)
+{
+	u32 num_sites = 0;
+	struct llvm_prf_data *p;
+	struct llvm_prf_data *end = po->data + po->data_num;
+	int kind;
+
+	for (p = po->data; p < end; ++p) {
+		if (p->values) {
+			for (kind = 0; kind < ARRAY_SIZE(p->num_value_sites); kind++)
+				num_sites += p->num_value_sites[kind];
+		}
+	}
+	return num_sites;
+}
+
+/*
+ * Reset profiler data object.
+ * Note: caller *must* take prf_lock_exclusive()
+ */
+static void pgo_reset(struct prf_object *po)
+{
+	int cpu;
+	unsigned long num_value_sites;
+
+	/* zero all edge profile counters */
+	memset(po->cnts, 0, po->cnts_num * sizeof(po->cnts[0]));
+
+	/* get number of value sites */
+	num_value_sites = pgo_num_value_sites(po);
+
+	for_each_online_cpu(cpu) {
+		struct prf_cpu_object *pco = &po->pcpu[cpu];
+		struct llvm_prf_value_node **vnodes = pco->nodes;
+		u32 i;
+
+		/*
+		 * walk all vnodes and zero counters.
+		 */
+		for (i = 0; i < num_value_sites; i++) {
+			struct llvm_prf_value_node *curr = vnodes[i];
+
+			while (curr) {
+				curr->count = 0;
+				curr = curr->next;
+			}
+		}
+	}
+}
+
+/*
+ * write() implementation for resetting
+ * the entire profiler state.
+ * Todo: allow resetting single profiler objects
+ */
+static ssize_t reset_write(struct file *file, const char __user *addr,
+	size_t len, loff_t *pos)
+{
+	struct prf_object *po;
+
+	prf_lock_exclusive();
+
+	rcu_read_lock();
+
+	list_for_each_entry_rcu(po, &prf_list, link) {
+
+		pgo_reset(po);
+	}
+
+	rcu_read_unlock();
+
+	prf_unlock_exclusive();
+
+	return len;
+}
+
+static const struct file_operations prf_reset_fops = {
+	.owner      = THIS_MODULE,
+	.write      = reset_write,
+	.llseek     = noop_llseek,
+};
+
+/* Patch pco->data[].values to point into per-cpu area */
+static void pgo_percpu_init_site_ptrs(struct prf_cpu_object *pco)
+{
+	struct llvm_prf_data *p = pco->data;
+	struct llvm_prf_data *end = p + pco->obj->data_num;
+	struct llvm_prf_value_node **pcurr = pco->nodes;
+	int kind;
+
+	for (; p < end; ++p) {
+		if (p->values) {
+			p->values = pcurr;
+			/* advance the ptr */
+			for (kind = 0; kind < ARRAY_SIZE(p->num_value_sites); kind++)
+				pcurr += p->num_value_sites[kind];
+		}
+	}
+}
+
+/*
+ * Take prf_object and initialize ->pcpu array
+ * based on the set prf sections and name.
+ * This creates debugfs entries for the object:
+ * vmlinux.0.profraw, vmlinux.1.profraw, etc.
+ */
+static struct prf_cpu_object *pgo_percpu_init(struct prf_object *po)
+{
+	int cpu;
+	struct prf_cpu_object *pco;
+	char fname[MODULE_NAME_LEN + 11]; /* +strlen("000.profraw") */
+	int num_value_sites = pgo_num_value_sites(po);
+
+	/* alloc percpu structures */
+	pco = kcalloc(num_online_cpus(), sizeof(po->pcpu[0]), GFP_KERNEL);
+	if (!pco)
+		goto err_free;
+
+	for_each_online_cpu(cpu) {
+		pco[cpu].cpu = cpu;
+		pco[cpu].obj = po;
+
+		/* alloc per-cpu site ptr table */
+		pco[cpu].nodes =
+			kcalloc(num_value_sites, sizeof(pco[0].nodes), GFP_KERNEL);
+		if (!pco[cpu].nodes)
+			goto err_free;
+
+		/* init per-cpu __llvm_prf_data data */
+		pco[cpu].data =
+			kmemdup(po->data, sizeof(po->data[0]) * po->data_num, GFP_KERNEL);
+		if (!pco[cpu].data)
+			goto err_free;
+
+		/* assign site ptr table to pco[cpu].data */
+		pgo_percpu_init_site_ptrs(&pco[cpu]);
+
+		/* alloc per-cpu __llvm_prf_vnds memory */
+		pco[cpu].vnds =
+			kcalloc(po->vnds_num, sizeof(*pco[cpu].vnds), GFP_KERNEL);
+		if (!pco[cpu].vnds)
+			goto err_free;
+
+		/* create debugfs entry for the cpu */
+		fname[0] = 0;
+		snprintf(fname, sizeof(fname), "%s.%d.profraw", po->name, cpu);
+
+		pco[cpu].file = debugfs_create_file(fname, 0600, directory, &pco[cpu], &prf_fops);
+		if (!pco[cpu].file) {
+			pr_err("Failed to setup pgo: %s", fname);
+			goto err_free;
+		}
+	}
+	return pco;
+
+err_free:
+	pr_err("-ENOMEM");
+	/* free pcpu array */
+	pgo_percpu_free(pco);
+	return NULL;
+}
+
+static void pgo_module_init(struct module *mod)
+{
+	struct prf_object *po;
+
+	/* Alloc prf_object entry for the module */
+	po = kzalloc(sizeof(*po), GFP_KERNEL);
+	if (!po)
+		return; /* -ENOMEM */
+
+	/* Setup prf_object instance */
+	po->name = mod->name;
+
+	po->data = mod->prf_data;
+	po->data_num = prf_get_count(mod->prf_data,
+					(char *)mod->prf_data + mod->prf_data_size,
+					sizeof(po->data[0]));
+
+	po->cnts = mod->prf_cnts;
+	po->cnts_num = prf_get_count(mod->prf_cnts,
+					(char *)mod->prf_cnts + mod->prf_cnts_size,
+					sizeof(po->cnts[0]));
+
+	po->names = mod->prf_names;
+	po->names_num = prf_get_count(mod->prf_names,
+					(char *)mod->prf_names + mod->prf_names_size,
+					sizeof(po->names[0]));
+
+	po->vnds_num = prf_get_count(mod->prf_vnds,
+					(char *)mod->prf_vnds + mod->prf_vnds_size,
+					sizeof(struct llvm_prf_value_node));
+
+	/* Initialize rest of the structure */
+	po->pcpu = pgo_percpu_init(po);
+	if (!po->pcpu)
+		return;
+
+	/* Enable profiling for the module */
+	prf_lock_exclusive();
+
+	/* Ensure profiling counters are zeroed */
+	pgo_reset(po);
+
+	list_add_tail_rcu(&po->link, &prf_list);
+	prf_unlock_exclusive();
+}
+
+static void pgo_module_free(struct rcu_head *rp)
+{
+	struct prf_object *po = container_of(rp, struct prf_object, rcu);
+
+	pgo_percpu_free(po->pcpu);
+	kfree(po);
+}
+
+static int pgo_module_notifier(struct notifier_block *nb, unsigned long event,
+			       void *pdata)
+{
+	struct module *mod = pdata;
+	struct prf_object *po;
+	int cpu;
+
+	if (event == MODULE_STATE_LIVE) {
+		/* Can we enable profiling for the module? */
+		if (mod->prf_data && mod->prf_cnts && mod->prf_names &&
+		    mod->prf_vnds && mod->prf_vnds_size > 0) {
+			/* Setup module profiling */
+			pgo_module_init(mod);
+
+			pr_info("%s: Enabled", mod->name);
+		} else {
+			/* Some modules can't be profiled */
+			pr_warn("%s: Disabled, no counters", mod->name);
+		}
+	}
+
+	if (event == MODULE_STATE_GOING) {
+		/* Find the prf_object from the list */
+		rcu_read_lock();
+
+		list_for_each_entry_rcu(po, &prf_list, link) {
+			if (strcmp(po->name, mod->name) == 0)
+				goto mod_found;
+		}
+		/* No such module */
+		po = NULL;
+
+mod_found:
+		rcu_read_unlock();
+
+		if (po) {
+			/* Remove from profiled modules */
+			prf_lock_exclusive();
+			list_del_rcu(&po->link);
+
+			/* Unlink debugfs entries now */
+			for_each_possible_cpu(cpu) {
+				debugfs_remove(po->pcpu[cpu].file);
+				po->pcpu[cpu].file = NULL;
+			}
+
+			prf_unlock_exclusive();
+
+			/* Cleanup memory */
+			call_rcu(&po->rcu, pgo_module_free);
+
+			pr_debug("%s: Unregister", mod->name);
+		}
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block pgo_module_nb = {
+	.notifier_call = pgo_module_notifier
+};
+
+/* Create debugfs entries. */
+static int __init pgo_init(void)
+{
+	directory = debugfs_create_dir("pgo", NULL);
+	if (!directory)
+		goto err_remove;
+
+	/* Setup vmlinux profiler object */
+	memset(&prf_vmlinux, 0, sizeof(prf_vmlinux));
+
+	prf_vmlinux.name = "vmlinux";
+	prf_vmlinux.data = __llvm_prf_data_start;
+	prf_vmlinux.data_num =
+		prf_get_count(__llvm_prf_data_start, __llvm_prf_data_end,
+			      sizeof(__llvm_prf_data_start[0]));
+
+	prf_vmlinux.cnts = __llvm_prf_cnts_start;
+	prf_vmlinux.cnts_num =
+		prf_get_count(__llvm_prf_cnts_start, __llvm_prf_cnts_end,
+			      sizeof(__llvm_prf_cnts_start[0]));
+
+	prf_vmlinux.names = __llvm_prf_names_start;
+	prf_vmlinux.names_num =
+		prf_get_count(__llvm_prf_names_start, __llvm_prf_names_end,
+			      sizeof(__llvm_prf_names_start[0]));
+
+	prf_vmlinux.vnds_num =
+		prf_get_count(__llvm_prf_vnds_start, __llvm_prf_vnds_end,
+			      sizeof(__llvm_prf_vnds_start[0]));
+
+	/* Init the vmlinux per-cpu entries */
+	prf_vmlinux.pcpu = pgo_percpu_init(&prf_vmlinux);
+	if (!prf_vmlinux.pcpu)
+		goto err_remove;
+
+	/* Enable profiling. */
+	prf_lock_exclusive();
+
+	/*
+	 * Ensure profiling counters are zeroed:
+	 * This especially important for vmlinux, because
+	 * the edge profile counters have likely changed
+	 * between boot and pgo_init()
+	 */
+	pgo_reset(&prf_vmlinux);
+
+	list_add_tail_rcu(&prf_vmlinux.link, &prf_list);
+	prf_unlock_exclusive();
+
+	/* add reset knob to debugfs */
+	if (!debugfs_create_file("reset", 0200, directory, NULL, &prf_reset_fops))
+		goto err_remove;
+
+	/* Show notice why the system slower: */
+	pr_info("Clang PGO instrumentation is active");
+
+	/* Register module notifer. */
+	register_module_notifier(&pgo_module_nb);
+
+	return 0;
+
+err_remove:
+	pr_err("initialization failed\n");
+	return -EIO;
+}
+
+/* Remove debugfs entries. */
+static void __exit pgo_exit(void)
+{
+	debugfs_remove_recursive(directory);
+}
+
+module_init(pgo_init);
+module_exit(pgo_exit);
diff --git a/kernel/pgo/instrument.c b/kernel/pgo/instrument.c
new file mode 100644
index 000000000000..9000180282d8
--- /dev/null
+++ b/kernel/pgo/instrument.c
@@ -0,0 +1,252 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2019 Google, Inc.
+ *
+ * Author:
+ *	Sami Tolvanen <samitolvanen@google.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#define pr_fmt(fmt)	"pgo: " fmt
+
+#include <asm/sections.h>
+#include <linux/bitops.h>
+#include <linux/kernel.h>
+#include <linux/export.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <linux/mutex.h>
+#include <linux/atomic.h>
+#include <linux/rcupdate.h>
+#include "pgo.h"
+
+/*
+ * This mutex protects the profile data serialization
+ * and the prf_list structure.
+ */
+static DEFINE_MUTEX(pgo_mutex);
+
+/*
+ * Atomic flag to disable profiler hook.
+ * - the profiler hook is disabled during boot
+ *   to keep the profiler state consistent until
+ *   pgo is fully initialized.
+ */
+static atomic_t prf_disable = ATOMIC_INIT(1);
+
+void prf_lock_exclusive(void)
+{
+	/* take pgo_mutex */
+	mutex_lock(&pgo_mutex);
+
+	/* disable profiler hook */
+	atomic_set(&prf_disable, 1);
+
+	/*
+	 * wait for GP:
+	 * No cpu may be running with prf_disable == 0
+	 */
+	synchronize_rcu();
+}
+
+void prf_unlock_exclusive(void)
+{
+	/* enable profiler hook again */
+	atomic_set(&prf_disable, 0);
+
+	/* unlock */
+	mutex_unlock(&pgo_mutex);
+}
+
+/*
+ * check if profiler hook is enabled.
+ * Must be used within RCU read side lock.
+ */
+static inline int prf_is_enabled(void)
+{
+	return atomic_read(&prf_disable) == 0;
+}
+
+/*
+ * Return prf_object for the llvm_prf_data or NULL
+ * if we should not attempt any profiling.
+ */
+static struct prf_object *find_prf_object(struct llvm_prf_data *p)
+{
+	struct prf_object *po = NULL;
+	struct llvm_prf_data *data_end;
+
+	list_for_each_entry_rcu(po, &prf_list, link) {
+		/*
+		 * Check that p is within:
+		 * [po->data, po->data + prf_data_count(po)] section.
+		 */
+		data_end = po->data + prf_data_count(po);
+		if (memory_contains(po->data, data_end, p, sizeof(*p)))
+			goto found;
+	}
+	/* not found */
+	po = NULL;
+
+found:
+	return po;
+}
+
+static inline unsigned int prf_get_index(const void *_start, const void *_end,
+	unsigned int objsize)
+{
+	unsigned long start = (unsigned long)_start;
+	unsigned long end =	(unsigned long)_end;
+
+	return (end - start) / objsize;
+}
+
+/*
+ * Counts the number of times a target value is seen.
+ *
+ * Records the target value for the index if not seen before. Otherwise,
+ * increments the counter associated w/ the target value.
+ */
+void __llvm_profile_instrument_target(u64 target_value, void *data, u32 index)
+{
+	struct llvm_prf_data *p = (struct llvm_prf_data *)data;
+	struct llvm_prf_value_node **counters;
+	struct llvm_prf_value_node *curr;
+	struct llvm_prf_value_node *min = NULL;
+	struct llvm_prf_value_node *prev = NULL;
+	u64 min_count = U64_MAX;
+	u8 values = 0;
+	unsigned long flags;
+	struct prf_object *po;
+	struct prf_cpu_object *pco;
+	int cpu;
+	int bucket;
+
+	rcu_read_lock();
+
+	/* check if profiling is allowed */
+	if (!prf_is_enabled())
+		goto out_rcu;
+
+	if (!p || !p->values)
+		goto out_rcu;
+
+	/* get prf_object */
+	po = find_prf_object(p);
+	if (!po)
+		goto out_rcu;
+
+	/* get index to p within po->pcpu[].data */
+	bucket = prf_get_index(po->data, data, sizeof(*p));
+
+	/* get prf_cpu_object */
+	preempt_disable();
+	cpu = smp_processor_id();
+	pco = &po->pcpu[cpu];
+
+	counters = (struct llvm_prf_value_node **)pco->data[bucket].values;
+	curr = counters[index];
+
+	while (curr) {
+		if (target_value == curr->value) {
+			curr->count++;
+			goto out_unlock;
+		}
+
+		if (curr->count < min_count) {
+			min_count = curr->count;
+			min = curr;
+		}
+
+		prev = curr;
+		curr = curr->next;
+		values++;
+	}
+
+	if (values >= LLVM_INSTR_PROF_MAX_NUM_VAL_PER_SITE) {
+		if (!min->count || !(--min->count)) {
+			curr = min;
+			curr->value = target_value;
+			curr->count++;
+		}
+		goto out_unlock;
+	}
+
+	if (WARN_ON_ONCE(pco->current_node >= po->vnds_num))
+		goto out_unlock; /* Out of nodes */
+
+	local_irq_save(flags);
+
+	/* reserve the vnode */
+	curr = &pco->vnds[pco->current_node++];
+
+	curr->value = target_value;
+	curr->count++;
+
+	if (!counters[index])
+		WRITE_ONCE(counters[index], curr);
+	else if (prev && !prev->next)
+		WRITE_ONCE(prev->next, curr);
+
+	local_irq_restore(flags);
+out_unlock:
+	preempt_enable_no_resched();
+out_rcu:
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL(__llvm_profile_instrument_target);
+
+/* Counts the number of times a range of targets values are seen. */
+void __llvm_profile_instrument_range(u64 target_value, void *data,
+				     u32 index, s64 precise_start,
+				     s64 precise_last, s64 large_value)
+{
+	if (large_value != S64_MIN && (s64)target_value >= large_value)
+		target_value = large_value;
+	else if ((s64)target_value < precise_start ||
+		 (s64)target_value > precise_last)
+		target_value = precise_last + 1;
+
+	__llvm_profile_instrument_target(target_value, data, index);
+}
+EXPORT_SYMBOL(__llvm_profile_instrument_range);
+
+static u64 inst_prof_get_range_rep_value(u64 value)
+{
+	if (value <= 8)
+		/* The first ranges are individually tracked, use it as is. */
+		return value;
+	else if (value >= 513)
+		/* The last range is mapped to its lowest value. */
+		return 513;
+	else if (hweight64(value) == 1)
+		/* If it's a power of two, use it as is. */
+		return value;
+
+	/* Otherwise, take to the previous power of two + 1. */
+	return ((u64)1 << (64 - __builtin_clzll(value) - 1)) + 1;
+}
+
+/*
+ * The target values are partitioned into multiple ranges. The range spec is
+ * defined in compiler-rt/include/profile/InstrProfData.inc.
+ */
+void __llvm_profile_instrument_memop(u64 target_value, void *data,
+				     u32 counter_index)
+{
+	u64 rep_value;
+
+	/* Map the target value to the representative value of its range. */
+	rep_value = inst_prof_get_range_rep_value(target_value);
+	__llvm_profile_instrument_target(rep_value, data, counter_index);
+}
+EXPORT_SYMBOL(__llvm_profile_instrument_memop);
diff --git a/kernel/pgo/pgo.h b/kernel/pgo/pgo.h
new file mode 100644
index 000000000000..8225cade05c2
--- /dev/null
+++ b/kernel/pgo/pgo.h
@@ -0,0 +1,277 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2019 Google, Inc.
+ *
+ * Author:
+ *	Sami Tolvanen <samitolvanen@google.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _PGO_H
+#define _PGO_H
+
+#include <linux/rculist.h>
+
+/*
+ * Note: These internal LLVM definitions must match the compiler version.
+ * See llvm/include/llvm/ProfileData/InstrProfData.inc in LLVM's source code.
+ */
+
+#define LLVM_INSTR_PROF_RAW_MAGIC_64	\
+		((u64)255 << 56 |	\
+		 (u64)'l' << 48 |	\
+		 (u64)'p' << 40 |	\
+		 (u64)'r' << 32 |	\
+		 (u64)'o' << 24 |	\
+		 (u64)'f' << 16 |	\
+		 (u64)'r' << 8  |	\
+		 (u64)129)
+#define LLVM_INSTR_PROF_RAW_MAGIC_32	\
+		((u64)255 << 56 |	\
+		 (u64)'l' << 48 |	\
+		 (u64)'p' << 40 |	\
+		 (u64)'r' << 32 |	\
+		 (u64)'o' << 24 |	\
+		 (u64)'f' << 16 |	\
+		 (u64)'R' << 8  |	\
+		 (u64)129)
+
+#define LLVM_INSTR_PROF_RAW_VERSION 7
+#define LLVM_INSTR_PROF_DATA_ALIGNMENT		8
+#define LLVM_INSTR_PROF_IPVK_FIRST		0
+#define LLVM_INSTR_PROF_IPVK_LAST		1
+#define LLVM_INSTR_PROF_MAX_NUM_VAL_PER_SITE	255
+
+#define LLVM_VARIANT_MASK_IR_PROF	(0x1ULL << 56)
+#define LLVM_VARIANT_MASK_CSIR_PROF	(0x1ULL << 57)
+
+/**
+ * struct llvm_prf_header - represents the raw profile header data structure.
+ * @magic: the magic token for the file format.
+ * @version: the version of the file format.
+ * @binary_ids_size: the number of binary ids. (since LLVM_INSTR_PROF_RAW_VERSION >= 7)
+ * @data_size: the number of entries in the profile data section.
+ * @padding_bytes_before_counters: the number of padding bytes before the
+ *   counters.
+ * @counters_size: the size in bytes of the LLVM profile section containing the
+ *   counters.
+ * @padding_bytes_after_counters: the number of padding bytes after the
+ *   counters.
+ * @names_size: the size in bytes of the LLVM profile section containing the
+ *   counters' names.
+ * @counters_delta: the beginning of the LLMV profile counters section.
+ * @names_delta: the beginning of the LLMV profile names section.
+ * @value_kind_last: the last profile value kind.
+ */
+struct llvm_prf_header {
+	u64 magic;
+	u64 version;
+	u64 binary_ids_size;
+	u64 data_size;
+	u64 padding_bytes_before_counters;
+	u64 counters_size;
+	u64 padding_bytes_after_counters;
+	u64 names_size;
+	u64 counters_delta;
+	u64 names_delta;
+	u64 value_kind_last;
+};
+
+/**
+ * struct llvm_prf_data - represents the per-function control structure.
+ * @name_ref: the reference to the function's name.
+ * @func_hash: the hash value of the function.
+ * @counter_ptr: a pointer to the profile counter.
+ * @function_ptr: a pointer to the function.
+ * @values: the profiling values associated with this function.
+ * @num_counters: the number of counters in the function.
+ * @num_value_sites: the number of value profile sites.
+ */
+struct llvm_prf_data {
+	const u64 name_ref;
+	const u64 func_hash;
+	const void *counter_ptr;
+	const void *function_ptr;
+	void *values;
+	const u32 num_counters;
+	const u16 num_value_sites[LLVM_INSTR_PROF_IPVK_LAST + 1];
+} __aligned(LLVM_INSTR_PROF_DATA_ALIGNMENT);
+
+/**
+ * struct llvm_prf_value_node_data - represents the data part of the struct
+ *   llvm_prf_value_node data structure.
+ * @value: the value counters.
+ * @count: the counters' count.
+ */
+struct llvm_prf_value_node_data {
+	u64 value;
+	u64 count;
+};
+
+/**
+ * struct llvm_prf_value_node - represents an internal data structure used by
+ *   the value profiler.
+ * @value: the value counters.
+ * @count: the counters' count.
+ * @next: the next value node.
+ */
+struct llvm_prf_value_node {
+	u64 value;
+	u64 count;
+	struct llvm_prf_value_node *next;
+};
+
+/**
+ * struct llvm_prf_value_data - represents the value profiling data in indexed
+ *   format.
+ * @total_size: the total size in bytes including this field.
+ * @num_value_kinds: the number of value profile kinds that has value profile
+ *   data.
+ */
+struct llvm_prf_value_data {
+	u32 total_size;
+	u32 num_value_kinds;
+};
+
+/**
+ * struct llvm_prf_value_record - represents the on-disk layout of the value
+ *   profile data of a particular kind for one function.
+ * @kind: the kind of the value profile record.
+ * @num_value_sites: the number of value profile sites.
+ * @site_count_array: the first element of the array that stores the number
+ *   of profiled values for each value site.
+ */
+struct llvm_prf_value_record {
+	u32 kind;
+	u32 num_value_sites;
+	u8 site_count_array[];
+};
+
+#define prf_get_value_record_header_size()		\
+	offsetof(struct llvm_prf_value_record, site_count_array)
+#define prf_get_value_record_site_count_size(sites)	\
+	roundup((sites), 8)
+#define prf_get_value_record_size(sites)		\
+	(prf_get_value_record_header_size() +		\
+	 prf_get_value_record_site_count_size((sites)))
+
+/*
+ * struct prf_cpu_object - per-cpu entry for prf_object
+ */
+struct prf_cpu_object {
+	/* work copy of llvm_prf_data */
+	struct llvm_prf_data *data;
+	/* site ptr table */
+	struct llvm_prf_value_node **nodes;
+	/* vnode data */
+	struct llvm_prf_value_node *vnds;
+	/* index for next free vnode */
+	int current_node;
+
+	/* debugfs file of this profile data set */
+	int cpu;
+	struct dentry *file;
+	struct prf_object *obj;
+};
+
+/*
+ * struct prf_object - profiler data set object
+ * The prf_object maintains related information
+ * for the profiler hook to operate on and also
+ * the related information for serializing the data
+ */
+struct prf_object {
+	struct list_head link;
+	struct rcu_head rcu;
+
+	/*
+	 * name of this prf_object
+	 * refers to struct module->name
+	 * or "vmlinux"
+	 */
+	const char *name;
+
+	/* data provided by the compiler. read-only. */
+	struct llvm_prf_data *data;
+	int data_num;
+	u64 *cnts;
+	int cnts_num;
+	const char *names;
+	int names_num;
+	int vnds_num;
+
+	/* percpu profiler data */
+	struct prf_cpu_object *pcpu;
+};
+
+/*
+ * List of profiler objects.
+ * - readers must take rcu_read_lock()
+ * - updaters must take the prf_lock_exclusive()
+ */
+extern struct list_head prf_list;
+
+/* Data sections */
+extern struct llvm_prf_data __llvm_prf_data_start[];
+extern struct llvm_prf_data __llvm_prf_data_end[];
+
+extern u64 __llvm_prf_cnts_start[];
+extern u64 __llvm_prf_cnts_end[];
+
+extern char __llvm_prf_names_start[];
+extern char __llvm_prf_names_end[];
+
+extern struct llvm_prf_value_node __llvm_prf_vnds_start[];
+extern struct llvm_prf_value_node __llvm_prf_vnds_end[];
+
+/*
+ * Locking for the profiler data structures.
+ * This is needed to ensure exclusive access to profiler data.
+ */
+extern void prf_lock_exclusive(void);
+extern void prf_unlock_exclusive(void);
+
+/* Declarations for LLVM instrumentation. */
+void __llvm_profile_instrument_target(u64 target_value, void *data, u32 index);
+void __llvm_profile_instrument_range(u64 target_value, void *data,
+				     u32 index, s64 precise_start,
+				     s64 precise_last, s64 large_value);
+void __llvm_profile_instrument_memop(u64 target_value, void *data,
+				     u32 counter_index);
+
+#define __DEFINE_PRF_OBJ_SIZE(s)                                           \
+	static inline unsigned long prf_##s##_size(struct prf_object *po)      \
+	{                                                                      \
+		return po->s##_num * sizeof(po->s[0]);                         \
+	}                                                                      \
+	static inline unsigned long prf_##s##_count(struct prf_object *po)     \
+	{                                                                      \
+		return po->s##_num;                                            \
+	}
+
+__DEFINE_PRF_OBJ_SIZE(data);
+__DEFINE_PRF_OBJ_SIZE(cnts);
+__DEFINE_PRF_OBJ_SIZE(names);
+
+#undef __DEFINE_PRF_OBJ_SIZE
+
+/* count number of items in range */
+static inline unsigned int prf_get_count(const void *_start, const void *_end,
+					 unsigned int objsize)
+{
+	unsigned long start = (unsigned long)_start;
+	unsigned long end = (unsigned long)_end;
+
+	return roundup(end - start, objsize) / objsize;
+}
+
+#endif /* _PGO_H */
diff --git a/scripts/Makefile.lib b/scripts/Makefile.lib
index d1f865b8c0cb..75a59942a5cd 100644
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -150,6 +150,16 @@ _c_flags += $(if $(patsubst n%,, \
 		$(CFLAGS_GCOV))
 endif
 
+#
+# Enable clang's PGO profiling flags for a file or directory depending on
+# variables PGO_PROFILE_obj.o and PGO_PROFILE.
+#
+ifeq ($(CONFIG_PGO_CLANG),y)
+_c_flags += $(if $(patsubst n%,, \
+		$(PGO_PROFILE_$(basetarget).o)$(PGO_PROFILE)y), \
+		$(CFLAGS_PGO_CLANG))
+endif
+
 #
 # Enable address sanitizer flags for kernel except some files or directories
 # we don't want to check (depends on variables KASAN_SANITIZE_obj.o, KASAN_SANITIZE)
diff --git a/scripts/module.lds.S b/scripts/module.lds.S
index 1d0e1e4dc3d2..94b52d2a5dca 100644
--- a/scripts/module.lds.S
+++ b/scripts/module.lds.S
@@ -61,6 +61,39 @@ SECTIONS {
 		*(.text .text.[0-9a-zA-Z_]* .text..L.cfi*)
 	}
 #endif
+#ifdef CONFIG_PGO_CLANG
+	/*
+	 * With CONFIG_PGO_CLANG the compiler may split __llvm_prf_xxx
+	 * objects into multiple sections. Merge them in final .ko object.
+	 * However leave .rela__llvm_prf_data sections as-is
+	 * since they are needed by the module loader.
+	 */
+	__llvm_prf_data : AT(ADDR(__llvm_prf_data)) {
+		__llvm_prf_data_start = .;
+		KEEP(*(SORT(__llvm_prf_data)))
+		__llvm_prf_data_end = .;
+	}
+	__llvm_prf_cnts : AT(ADDR(__llvm_prf_cnts)) {
+		__llvm_prf_cnts_start = .;
+		KEEP(*(SORT(__llvm_prf_cnts)))
+		__llvm_prf_cnts_end = .;
+	}
+	__llvm_prf_names : AT(ADDR(__llvm_prf_names)) {
+		__llvm_prf_names_start = .;
+		KEEP(*(SORT(__llvm_prf_names)))
+		__llvm_prf_names_end = .;
+	}
+	__llvm_prf_vals : AT(ADDR(__llvm_prf_vals)) {
+		__llvm_prf_vals_start = .;
+		KEEP(*(SORT(__llvm_prf_vals)))
+		__llvm_prf_vals_end = .;
+	}
+	__llvm_prf_vnds : AT(ADDR(__llvm_prf_vnds)) {
+		__llvm_prf_vnds_start = .;
+		KEEP(*(SORT(__llvm_prf_vnds)))
+		__llvm_prf_vnds_end = .;
+	}
+#endif
 }
 
 /* bring in arch-specific sections */
