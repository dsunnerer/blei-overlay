From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 7002DC433FE
	for <linux-kernel@archiver.kernel.org>; Wed, 10 Nov 2021 08:30:13 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 56653610CB
	for <linux-kernel@archiver.kernel.org>; Wed, 10 Nov 2021 08:30:13 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S230356AbhKJIc7 (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Wed, 10 Nov 2021 03:32:59 -0500
Received: from us-smtp-delivery-124.mimecast.com ([216.205.24.124]:60359 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S230290AbhKJIc5 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 10 Nov 2021 03:32:57 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636533009;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=zc9KdahAguXkghVeJqdY3150Du1QUqNhK9U4RYp1RCU=;
        b=RByG1jlub1KTpZ2nQTePIh1H/Kdu7gduUC3zdqNmdtdGYU/FiNqUn6MpiFbPcInFw+Yvc3
        3pCVnwHKtwoq8cwhfcBP3lqgalqZfRIoI6wgCezppbJJeFsKq6DeopFMiHDH6RawAMHbpH
        3zakxDk5POvfF5qs36qHslGG8/7rgAc=
Received: from mail-pl1-f197.google.com (mail-pl1-f197.google.com
 [209.85.214.197]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-469-Ucn6SWz5POm13inzkJkT1g-1; Wed, 10 Nov 2021 03:30:08 -0500
X-MC-Unique: Ucn6SWz5POm13inzkJkT1g-1
Received: by mail-pl1-f197.google.com with SMTP id k9-20020a170902ba8900b00141f601d5c8so1347629pls.1
        for <linux-kernel@vger.kernel.org>; Wed, 10 Nov 2021 00:30:08 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=zc9KdahAguXkghVeJqdY3150Du1QUqNhK9U4RYp1RCU=;
        b=WV4mf5+ZvhNeBOb/eHlGd3V66ojvi3BcsPrYYpRtBnY8M+UbwZlX4fJOfam/FVfTpo
         TDBCIbICaIgge0YjnJo4MyobjRigmMw3ovhzPUvLEKILK7NyMCeY0QELi1dNY0pXelCB
         PreJGeGKMGwvIBhyGK6T7oHtPS3YYGrmZodiS8CpLzVzTRigH6eB+yXOhLQrMqe+ZKRP
         VgwaX60REsgpilCcbFGz4PggnffdrtAfN82ugQPeDt/ckUyFgjNPGBVLsNdVLabkuDX9
         x2WAHWhDdlRUsebsvOu/JhDrNT30zhZh5xjcQIGKAocBBWxQyQXHMZ/XwsxNJXoG1RfE
         ZQoA==
X-Gm-Message-State: AOAM530CP90fjPpO641EbnhKzj1goNda8h0CXIEda121ATwPLtF+ltI8
        aCxF0JP931cBEfZkw9qkHe89GzgZyYzwtLGi9BLuFAjeHO7sG7rAkK2sOmsVdun19VmXePzZnLM
        8qf3V8M9TV0XV7gy1wGbYInLnvk0Q7VRJYYiZyjuXasDm8zrL17FoiBNRXnCmet7iBROS9LCVgA
        ==
X-Received: by 2002:a05:6a00:234f:b0:3eb:3ffd:6da2 with SMTP id j15-20020a056a00234f00b003eb3ffd6da2mr14429792pfj.15.1636533006883;
        Wed, 10 Nov 2021 00:30:06 -0800 (PST)
X-Google-Smtp-Source: ABdhPJz5G/zio2lUCBLScLXJPHr7YuBo+SEEpRo7ak25/Vhyewmy2QoxXzy6zMkmUgeA4ABnG5roEw==
X-Received: by 2002:a05:6a00:234f:b0:3eb:3ffd:6da2 with SMTP id j15-20020a056a00234f00b003eb3ffd6da2mr14429751pfj.15.1636533006522;
        Wed, 10 Nov 2021 00:30:06 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.35])
        by smtp.gmail.com with ESMTPSA id s7sm23709986pfu.139.2021.11.10.00.30.01
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 10 Nov 2021 00:30:06 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc:     peterx@redhat.com, Andrew Morton <akpm@linux-foundation.org>,
        Yang Shi <shy828301@gmail.com>,
        Hugh Dickins <hughd@google.com>,
        David Hildenbrand <david@redhat.com>,
        Alistair Popple <apopple@nvidia.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Vlastimil Babka <vbabka@suse.cz>,
        "Kirill A . Shutemov" <kirill@shutemov.name>
Subject: [PATCH RFC 1/2] mm: Don't skip swap entry even if zap_details specified
Date:   Wed, 10 Nov 2021 16:29:51 +0800
Message-Id: <20211110082952.19266-2-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211110082952.19266-1-peterx@redhat.com>
References: <20211110082952.19266-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This check existed since the 1st git commit of Linux repository, but at that
time there's no page migration yet so I think it's okay.

With page migration enabled, it should logically be possible that we zap some
shmem pages during migration.  When that happens, IIUC the old code could have
the RSS counter accounted wrong on MM_SHMEMPAGES because we will zap the ptes
without decreasing the counters for the migrating entries.  I have no unit test
to prove it as I don't know an easy way to trigger this condition, though.

Besides, the optimization itself is already confusing IMHO to me in a few points:

  - The wording "skip swap entries" is confusing, because we're not skipping all
    swap entries - we handle device private/exclusive pages before that.

  - The skip behavior is enabled as long as zap_details pointer passed over.
    It's very hard to figure that out for a new zap caller because it's unclear
    why we should skip swap entries when we have zap_details specified.

  - With modern systems, especially performance critical use cases, swap
    entries should be rare, so I doubt the usefulness of this optimization
    since it should be on a slow path anyway.

  - It is not aligned with what we do with huge pmd swap entries, where in
    zap_huge_pmd() we'll do the accounting unconditionally.

This patch drops that trick, so we handle swap ptes coherently.  Meanwhile we
should do the same mapping check upon migration entries too.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/memory.c | 6 ++----
 1 file changed, 2 insertions(+), 4 deletions(-)

diff --git a/mm/memory.c b/mm/memory.c
index 8f1de811a1dc..e454f3c6aeb9 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1382,16 +1382,14 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			continue;
 		}

-		/* If details->check_mapping, we leave swap entries. */
-		if (unlikely(details))
-			continue;
-
 		if (!non_swap_entry(entry))
 			rss[MM_SWAPENTS]--;
 		else if (is_migration_entry(entry)) {
 			struct page *page;

 			page = pfn_swap_entry_to_page(entry);
+			if (unlikely(zap_skip_check_mapping(details, page)))
+				continue;
 			rss[mm_counter(page)]--;
 		}
 		if (unlikely(!free_swap_and_cache(entry)))
--
2.32.0

From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id B8686C433EF
	for <linux-kernel@archiver.kernel.org>; Wed, 10 Nov 2021 08:30:18 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 9D9EF61076
	for <linux-kernel@archiver.kernel.org>; Wed, 10 Nov 2021 08:30:18 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S230037AbhKJIdE (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Wed, 10 Nov 2021 03:33:04 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:44539 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S230362AbhKJIdB (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 10 Nov 2021 03:33:01 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636533014;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=/Mopq8eqVCQt3k2LgKCs45drQOHTZZHv/QrpqMo5cqQ=;
        b=EayQi99fOa6zjgJeXtX1eY7m7qSjrovVN/8AIEG0XSapHT9Dlw/F68KfW0h9MBqw8+Yx+w
        ShqQ6OEEWLj/iTRq4xMninaIG6IdZuYrkyA8P5W8cxDzKrTyacjdB6emTzk45GYMq2X7Zm
        2S8cw5XDNzCFdayzRNasyRQ/Z+b/UHM=
Received: from mail-pg1-f200.google.com (mail-pg1-f200.google.com
 [209.85.215.200]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-488-E2q-MBkLMNmZZERQmWGMwQ-1; Wed, 10 Nov 2021 03:30:13 -0500
X-MC-Unique: E2q-MBkLMNmZZERQmWGMwQ-1
Received: by mail-pg1-f200.google.com with SMTP id z7-20020a63c047000000b0026b13e40309so1118398pgi.19
        for <linux-kernel@vger.kernel.org>; Wed, 10 Nov 2021 00:30:13 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=/Mopq8eqVCQt3k2LgKCs45drQOHTZZHv/QrpqMo5cqQ=;
        b=oumdw23ew99uycTwUr/mLYLPsVdllPdMdlajCcXP4GbeFcDfeNX05joonGhF7jdUCJ
         M9f9snkJsKOEh4axaF0SaJ1UqjEe+7TcU6p5lPIbNQzqVsS9YbloBmHSovwO8JtdiMk3
         3IfuKGpAtc7UZpBkZKlTMizjlsFYAdKk4f9c5w9EjzaHZZnNKiNicUoSsWk0mGOVx8Xd
         EA0oTC0FhzloR5qS1zvE1EK2L+FjR+hiwzAiE/W06JhDLjrKGbO/Q6epo3AfZjjvFh0+
         GoHVF6k9HunFvh292b5Gb4I2Heun+S7zdpSpup/a6I1Nd8QW8IfGRKvEtbTPkUQdSRni
         ir6A==
X-Gm-Message-State: AOAM533YY5ArI5cAI5bPIR2OXmrl1K8udsYU9rt7CPpM85unLkl6wG7u
        QB6lvKPu+3v05xpJFxrpe/Y2QWqKWXvLonVv+Uew0031PWjuoZPFVTQCSsHJ7vvEm5CfDu7NfXM
        KTsRh8h6f7u6w3dg0dGvvHMeCiM0ZBSMIV0IFjLmV4Koz3+nvQm6nSU0RrYx3j65f7lbsKrH/aQ
        ==
X-Received: by 2002:a17:902:744b:b0:142:46fe:7fbf with SMTP id e11-20020a170902744b00b0014246fe7fbfmr13548627plt.83.1636533011889;
        Wed, 10 Nov 2021 00:30:11 -0800 (PST)
X-Google-Smtp-Source: ABdhPJzDXOtXqxrc8KYS1LcooPeq0n9NmJXaoIsx8EsNg1RzMgg48pAAzDRcIkBGlG9WlDWha2HVhg==
X-Received: by 2002:a17:902:744b:b0:142:46fe:7fbf with SMTP id e11-20020a170902744b00b0014246fe7fbfmr13548581plt.83.1636533011533;
        Wed, 10 Nov 2021 00:30:11 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.35])
        by smtp.gmail.com with ESMTPSA id s7sm23709986pfu.139.2021.11.10.00.30.06
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 10 Nov 2021 00:30:11 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc:     peterx@redhat.com, Andrew Morton <akpm@linux-foundation.org>,
        Yang Shi <shy828301@gmail.com>,
        Hugh Dickins <hughd@google.com>,
        David Hildenbrand <david@redhat.com>,
        Alistair Popple <apopple@nvidia.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Vlastimil Babka <vbabka@suse.cz>,
        "Kirill A . Shutemov" <kirill@shutemov.name>
Subject: [PATCH RFC 2/2] mm: Rework swap handling of zap_pte_range
Date:   Wed, 10 Nov 2021 16:29:52 +0800
Message-Id: <20211110082952.19266-3-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211110082952.19266-1-peterx@redhat.com>
References: <20211110082952.19266-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Clean the code up by merging the device private/exclusive swap entry handling
with the rest, then we merge the pte clear operation too.

struct* page is defined in multiple places in the function, move it upward.

free_swap_and_cache() is only useful for !non_swap_entry() case, put it into
the condition.

No functional change intended.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/memory.c | 25 ++++++++-----------------
 1 file changed, 8 insertions(+), 17 deletions(-)

diff --git a/mm/memory.c b/mm/memory.c
index e454f3c6aeb9..e5d59a6b6479 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1326,6 +1326,8 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 	arch_enter_lazy_mmu_mode();
 	do {
 		pte_t ptent = *pte;
+		struct page *page;
+
 		if (pte_none(ptent))
 			continue;

@@ -1333,8 +1335,6 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			break;

 		if (pte_present(ptent)) {
-			struct page *page;
-
 			page = vm_normal_page(vma, addr, ptent);
 			if (unlikely(zap_skip_check_mapping(details, page)))
 				continue;
@@ -1368,32 +1368,23 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 		entry = pte_to_swp_entry(ptent);
 		if (is_device_private_entry(entry) ||
 		    is_device_exclusive_entry(entry)) {
-			struct page *page = pfn_swap_entry_to_page(entry);
-
+			page = pfn_swap_entry_to_page(entry);
 			if (unlikely(zap_skip_check_mapping(details, page)))
 				continue;
-			pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
 			rss[mm_counter(page)]--;
-
 			if (is_device_private_entry(entry))
 				page_remove_rmap(page, false);
-
 			put_page(page);
-			continue;
-		}
-
-		if (!non_swap_entry(entry))
-			rss[MM_SWAPENTS]--;
-		else if (is_migration_entry(entry)) {
-			struct page *page;
-
+		} else if (is_migration_entry(entry)) {
 			page = pfn_swap_entry_to_page(entry);
 			if (unlikely(zap_skip_check_mapping(details, page)))
 				continue;
 			rss[mm_counter(page)]--;
+		} else if (!non_swap_entry(entry)) {
+			rss[MM_SWAPENTS]--;
+			if (unlikely(!free_swap_and_cache(entry)))
+				print_bad_pte(vma, addr, ptent, NULL);
 		}
-		if (unlikely(!free_swap_and_cache(entry)))
-			print_bad_pte(vma, addr, ptent, NULL);
 		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
 	} while (pte++, addr += PAGE_SIZE, addr != end);

--
2.32.0

From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 793A9C433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:55:44 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 5050E6321A
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:55:44 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229776AbhKOH6i (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 02:58:38 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:20885 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S229654AbhKOH6g (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:58:36 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962940;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:content-type:content-type:
         content-transfer-encoding:content-transfer-encoding;
        bh=OFYWOfIgh+R0pCCOGFbQusLursYsTaR7YBgSvw4x5lw=;
        b=TkmfGl0CLQd2ACJr8/+VN9I7QXvvTs6j8QPPgNtuTU00YLwqCZtjpFBe48BdIjHmiUCpXB
        TN1FI01OdetfZwV//Yu6JKj1i+R+79AJYVGah5ynRzUVAPkyTUxjm90zhRVfqr1wrxSlFB
        42AGUU7WUKbdZoaDkxjJqXjR+SoZMyA=
Received: from mail-pl1-f198.google.com (mail-pl1-f198.google.com
 [209.85.214.198]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-414-MPl6QiHxPsSjDhUn6qK8Dw-1; Mon, 15 Nov 2021 02:55:38 -0500
X-MC-Unique: MPl6QiHxPsSjDhUn6qK8Dw-1
Received: by mail-pl1-f198.google.com with SMTP id v23-20020a170902bf9700b001421d86afc4so72057pls.9
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:55:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:mime-version
         :content-transfer-encoding;
        bh=OFYWOfIgh+R0pCCOGFbQusLursYsTaR7YBgSvw4x5lw=;
        b=axsAbS3TGyECeCyWjLrR0ZdRd+JPj2nZSqQC3dI2w0HOHV5T077LwbSrKeyB1uIE1J
         Adn37kaHphnP/13OQUJ3B89PdiB+Bq+qsiHWaD6mIoG+wW6pRI6sfvgKAxnLxeZOHBm5
         7B9JMkEx4aoeG+pyapSdTLr/1EfuhtSTNB09nYJCqN9dEF4C447fsrmL9xZujxvaT5oC
         BfzOB7euTW+T1Acf1S34Jc1LNgR6CjTWd8puB2h3lPvJf7i9IyyNxXFS7Xhhl0uRsE/j
         4JsfNRw9r5FBubX9AzMETAlBYo4Ac3dNJ50/UqNtM8/+WRgGGz7s/WmemOA58gknsWpW
         IhSA==
X-Gm-Message-State: AOAM533zzvWQQdL+kzqfsljwVf29LCKCJX0iBdA3G3R1fJ9EtLp9HKUE
        impGm47i5taZZZai7j/rO/4aIZypV5ywRKWciAgVKB6PgQhlBE1RSWD9zfXF4UA8DPOcgdVyolk
        moihEfDN99JGIijtAfmeiuGWw
X-Received: by 2002:a17:902:b716:b0:141:d36c:78fc with SMTP id d22-20020a170902b71600b00141d36c78fcmr32876783pls.59.1636962937636;
        Sun, 14 Nov 2021 23:55:37 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwAd51rC565DmR4S0Px83qtaC3tncJfoqUkji9bNAdVboAcS0spgZmn9DtgCef8nGAYJErnow==
X-Received: by 2002:a17:902:b716:b0:141:d36c:78fc with SMTP id d22-20020a170902b71600b00141d36c78fcmr32876737pls.59.1636962937216;
        Sun, 14 Nov 2021 23:55:37 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.55.29
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:55:36 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 00/23] userfaultfd-wp: Support shmem and hugetlbfs
Date:   Mon, 15 Nov 2021 15:54:59 +0800
Message-Id: <20211115075522.73795-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This is v6 of the series to add shmem+hugetlbfs support for userfaultfd write
protection.  It is based on v5.16-rc1 (fa55b7dcdc43), with below two patches
applied first:

  Subject: [PATCH RFC 0/2] mm: Rework zap ptes on swap entries
  https://lore.kernel.org/lkml/20211110082952.19266-1-peterx@redhat.com/

The whole tree can be found here for testing:

  https://github.com/xzpeter/linux/tree/uffd-wp-shmem-hugetlbfs

Previous versions:

  RFC: https://lore.kernel.org/lkml/20210115170907.24498-1-peterx@redhat.com/
  v1:  https://lore.kernel.org/lkml/20210323004912.35132-1-peterx@redhat.com/
  v2:  https://lore.kernel.org/lkml/20210427161317.50682-1-peterx@redhat.com/
  v3:  https://lore.kernel.org/lkml/20210527201927.29586-1-peterx@redhat.com/
  v4:  https://lore.kernel.org/lkml/20210714222117.47648-1-peterx@redhat.com/
  v5:  https://lore.kernel.org/lkml/20210715201422.211004-1-peterx@redhat.com/

Overview
==================

This is the first version of this work to rebase the uffd-wp logic work upon
PTE markers.  The major logic will be the same as v5, but since there're quite
a few minor changes here and there, I decided to not provide a change log at
all as it'll stop to be helpful.  However I should have addressed all the
comments that were raised by reviewers, please shoot if I missed something.  I
still kept many of the Mike's Review-By tag when there's merely no change to
the patch content (I touched up quite a few commit messages), but it'll be nice
if Mike could still went over the patches even if there're R-bs standing.

PTE marker is a new type of swap entry that is ony applicable to file-backed
memories like shmem and hugetlbfs.  It's used to persist some pte-level
information even if the original present ptes in pgtable are zapped.  These
information could be one of:

  (1) Userfaultfd wr-protect information
  (2) PTE soft-dirty information
  (3) Or others

This series only uses the marker to store uffd-wp information across temporary
zappings of shmem/hugetlbfs pgtables, for example, when a shmem thp is split.
So even if ptes are temporarily zapped, the wr-protect information can still be
kept within the pgtables.  Then when the page fault triggers again, we'll know
this pte is wr-protected so we can treat the pte the same as a normal uffd
wr-protected pte.

The extra information is encoded into the swap entry, or swp_offset to be
explicit, with the swp_type being PTE_MARKER.  So far uffd-wp only uses one bit
out of the swap entry, the rest bits of swp_offset are still reserved for other
purposes.

There're two configs to enable/disable PTE markers:

  CONFIG_PTE_MARKER
  CONFIG_PTE_MARKER_UFFD_WP

We can set !PTE_MARKER to completely disable all the PTE markers, along with
uffd-wp support.  I made two config so we can also enable PTE marker but
disable uffd-wp file-backed for other purposes.  At the end of current series,
I'll enable CONFIG_PTE_MARKER by default, but that patch is standalone and if
anyone worries about having it by default, we can also consider turn it off by
dropping that oneliner patch.  So far I don't see a huge risk of doing so, so I
kept that patch.

In most cases, PTE markers should be treated as none ptes.  It is because that
unlike most of the other swap entry types, there's no PFN or block offset
information encoded into PTE markers but some extra well-defined bits showing
the status of the pte.  These bits should only be used as extra data when
servicing an upcoming page fault, and that should be it.

I did spend a lot of time observing all the pte_none() users this time. It is
indeed a challenge because there're a lot, and I hope I didn't miss a single of
them when we should take care of pte markers.  Luckily, I don't think it'll
need to be considered in many cases, for example: boot code, arch code
(especially non-x86), kernel-only page handlings (e.g. CPA), or device driver
codes when we're tackling with pure PFN mappings.

I introduced pte_none_mostly() in this series when we need to handle pte
markers the same as none pte, the "mostly" is the other way to write "either
none pte or a pte marker".

I didn't replace pte_none() to cover pte markers for below reasons:

  - Very rare case of pte_none() callers will handle pte markers.  E.g., all
    the kernel pages do not require knowledge of pte markers.  So we don't
    pollute the major use cases.

  - Unconditionally change pte_none() semantics could confuse people, because
    pte_none() existed for so long a time.

  - Unconditionally change pte_none() semantics could make pte_none() slower
    even if in many cases pte markers do not exist.

  - There're cases where we'd like to handle pte markers differntly from
    pte_none(), so a full replace is also impossible.  E.g. khugepaged should
    still treat pte markers as normal swap ptes rather than none ptes, because
    pte markers will always need a fault-in to merge the marker with a valid
    pte.  Or the smap code will need to parse PTE markers not none ptes.

Patch Layout
============

Introducing PTE marker and uffd-wp bit in PTE marker:

  mm: Introduce PTE_MARKER swap entry
  mm: Teach core mm about pte markers
  mm: Check against orig_pte for finish_fault()
  mm/uffd: PTE_MARKER_UFFD_WP

Adding support for shmem uffd-wp:

  mm/shmem: Take care of UFFDIO_COPY_MODE_WP
  mm/shmem: Handle uffd-wp special pte in page fault handler
  mm/shmem: Persist uffd-wp bit across zapping for file-backed
  mm/shmem: Allow uffd wr-protect none pte for file-backed mem
  mm/shmem: Allows file-back mem to be uffd wr-protected on thps
  mm/shmem: Handle uffd-wp during fork()

Adding support for hugetlbfs uffd-wp:

  mm/hugetlb: Introduce huge pte version of uffd-wp helpers
  mm/hugetlb: Hook page faults for uffd write protection
  mm/hugetlb: Take care of UFFDIO_COPY_MODE_WP
  mm/hugetlb: Handle UFFDIO_WRITEPROTECT
  mm/hugetlb: Handle pte markers in page faults
  mm/hugetlb: Allow uffd wr-protect none ptes
  mm/hugetlb: Only drop uffd-wp special pte if required
  mm/hugetlb: Handle uffd-wp during fork()

Misc handling on the rest mm for uffd-wp file-backed:

  mm/khugepaged: Don't recycle vma pgtable if uffd-wp registered
  mm/pagemap: Recognize uffd-wp bit for shmem/hugetlbfs

Enabling of uffd-wp on file-backed memory:

  mm/uffd: Enable write protection for shmem & hugetlbfs
  mm: Enable PTE markers by default
  selftests/uffd: Enable uffd-wp for shmem/hugetlbfs

Tests
==============

- x86_64
  - Compile tested on:
    - PTE_MARKER && PTE_MARKER_UFFD_WP,
    - PTE_MARKER && !PTE_MARKER_UFFD_WP,
    - !PTE_MARKER
    - !USERFAULTFD
  - Kernel userfaultfd selftests for shmem/hugetlb/hugetlb_shared
  - Umapsort [1,2] test for shmem/hugetlb, with swap on/off
- aarch64
  - Compile and smoke tested with !PTE_MARKER

[1] https://github.com/xzpeter/umap-apps/tree/peter
[2] https://github.com/xzpeter/umap/tree/peter-shmem-hugetlbfs

Peter Xu (23):
  mm: Introduce PTE_MARKER swap entry
  mm: Teach core mm about pte markers
  mm: Check against orig_pte for finish_fault()
  mm/uffd: PTE_MARKER_UFFD_WP
  mm/shmem: Take care of UFFDIO_COPY_MODE_WP
  mm/shmem: Handle uffd-wp special pte in page fault handler
  mm/shmem: Persist uffd-wp bit across zapping for file-backed
  mm/shmem: Allow uffd wr-protect none pte for file-backed mem
  mm/shmem: Allows file-back mem to be uffd wr-protected on thps
  mm/shmem: Handle uffd-wp during fork()
  mm/hugetlb: Introduce huge pte version of uffd-wp helpers
  mm/hugetlb: Hook page faults for uffd write protection
  mm/hugetlb: Take care of UFFDIO_COPY_MODE_WP
  mm/hugetlb: Handle UFFDIO_WRITEPROTECT
  mm/hugetlb: Handle pte markers in page faults
  mm/hugetlb: Allow uffd wr-protect none ptes
  mm/hugetlb: Only drop uffd-wp special pte if required
  mm/hugetlb: Handle uffd-wp during fork()
  mm/khugepaged: Don't recycle vma pgtable if uffd-wp registered
  mm/pagemap: Recognize uffd-wp bit for shmem/hugetlbfs
  mm/uffd: Enable write protection for shmem & hugetlbfs
  mm: Enable PTE markers by default
  selftests/uffd: Enable uffd-wp for shmem/hugetlbfs

 arch/s390/include/asm/hugetlb.h          |  15 ++
 fs/hugetlbfs/inode.c                     |  15 +-
 fs/proc/task_mmu.c                       |  11 ++
 fs/userfaultfd.c                         |  31 +---
 include/asm-generic/hugetlb.h            |  24 +++
 include/linux/hugetlb.h                  |  27 ++--
 include/linux/mm.h                       |  20 +++
 include/linux/mm_inline.h                |  45 ++++++
 include/linux/shmem_fs.h                 |   4 +-
 include/linux/swap.h                     |  15 +-
 include/linux/swapops.h                  |  79 ++++++++++
 include/linux/userfaultfd_k.h            |  67 +++++++++
 include/uapi/linux/userfaultfd.h         |  10 +-
 mm/Kconfig                               |  16 ++
 mm/filemap.c                             |   5 +
 mm/hmm.c                                 |   2 +-
 mm/hugetlb.c                             | 181 +++++++++++++++++-----
 mm/khugepaged.c                          |  14 +-
 mm/memcontrol.c                          |   8 +-
 mm/memory.c                              | 184 ++++++++++++++++++++---
 mm/mincore.c                             |   3 +-
 mm/mprotect.c                            |  76 +++++++++-
 mm/rmap.c                                |   8 +
 mm/shmem.c                               |   4 +-
 mm/userfaultfd.c                         |  61 +++++---
 tools/testing/selftests/vm/userfaultfd.c |   4 +-
 26 files changed, 798 insertions(+), 131 deletions(-)

--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 77B44C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:56:05 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 6183A63221
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:56:05 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229877AbhKOH6y (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 02:58:54 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:23455 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S229963AbhKOH6n (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:58:43 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962947;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=tB7XXxUDPClsvCRJSlQGXzWYmuo0RbrsIC/Jp1UIRWI=;
        b=YvkrP+zm74uJSOPGDY/+Gxj1y5q6uY4KsRP2N3emv9T72ZJxFRRgmVwfuM7OMN4qMc5Scj
        zk+GdvTjy0W7MlSNbKXnSLjORW9klEaPwsiU0XhGaBxVdv/vsM+z4n5Ie9kQgwBiXmGh/6
        1CQOV8r5UrDJtS2MzWcz+9z9nsJfpeM=
Received: from mail-pj1-f70.google.com (mail-pj1-f70.google.com
 [209.85.216.70]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-20-rs65WneROrSbWHIcOrRIjg-1; Mon, 15 Nov 2021 02:55:46 -0500
X-MC-Unique: rs65WneROrSbWHIcOrRIjg-1
Received: by mail-pj1-f70.google.com with SMTP id n2-20020a17090a2fc200b001a1bafb59bfso4872990pjm.1
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:55:46 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=tB7XXxUDPClsvCRJSlQGXzWYmuo0RbrsIC/Jp1UIRWI=;
        b=br1JdoiZclzhJ2TN6/LVcgs42UIFW66T2h9az5X9Ta7yQ4oa27XMt4QEw6aH8vm02t
         03Q2CtPtC0+CLJJKd0wUmCFD+quTmgmZBwAtrUBTtmunUmHpGpFw+iquHJEbjz1KabTG
         eQXlhXNdyBZwwI8aF8tqi6KBTi8EeIcA2vpzXR5d8ilbIKEpE76ABJ4kKRV1b/YZoWVi
         jSJqUA6cp/nXXF+UwCOoRHM6iCzDXDgr5J0qFHZY1cUtOPJi86qcoPHYdmdHUu1mBofD
         6Uv8CyL4y0bu+Hqu8SSnnnclS+lNTQ0eCcWW5/pO+y5cLj2JU+kFMLnputCCUSixAWEy
         zmCQ==
X-Gm-Message-State: AOAM531+oiXbNNg7ygNiV9uqA6o/I5OnGc2ycsr+7zl0WodixqhuJ7UD
        N0wYgGpq8DMDIFp+OpusGFxRBDT1mQre6fWYft49nkQNiK9bfTeMYkg+haUAjkQprsh/cpc2eO3
        ug7guXYF/eZH7U/Z0/75s8Xaj
X-Received: by 2002:a17:902:ced0:b0:142:189a:4284 with SMTP id d16-20020a170902ced000b00142189a4284mr33424049plg.79.1636962945486;
        Sun, 14 Nov 2021 23:55:45 -0800 (PST)
X-Google-Smtp-Source: ABdhPJxIh24oN/aOHwQNeFngZZiEj6k+znRNmUOXfm7wYUcH1IH7Mfl4beexyoTb5Mv7N+ELT1eiqg==
X-Received: by 2002:a17:902:ced0:b0:142:189a:4284 with SMTP id d16-20020a170902ced000b00142189a4284mr33424023plg.79.1636962945136;
        Sun, 14 Nov 2021 23:55:45 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.55.37
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:55:44 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 01/23] mm: Introduce PTE_MARKER swap entry
Date:   Mon, 15 Nov 2021 15:55:00 +0800
Message-Id: <20211115075522.73795-2-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This patch introduces a new swap entry type called PTE_MARKER.  It can be
installed for any pte that maps a file-backed memory when the pte is
temporarily zapped, so as to maintain per-pte information.

The information that kept in the pte is called a "marker".  Here we define the
marker as "unsigned long" just to match pgoff_t, however it will only work if
it still fits in swp_offset(), which is e.g. currently 58 bits on x86_64.

A new config CONFIG_PTE_MARKER is introduced too; it's by default off.  A bunch
of helpers are defined altogether to service the rest of the pte marker code.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/asm-generic/hugetlb.h |  9 ++++
 include/linux/swap.h          | 15 ++++++-
 include/linux/swapops.h       | 78 +++++++++++++++++++++++++++++++++++
 mm/Kconfig                    |  7 ++++
 4 files changed, 108 insertions(+), 1 deletion(-)

diff --git a/include/asm-generic/hugetlb.h b/include/asm-generic/hugetlb.h
index 8e1e6244a89d..f39cad20ffc6 100644
--- a/include/asm-generic/hugetlb.h
+++ b/include/asm-generic/hugetlb.h
@@ -2,6 +2,9 @@
 #ifndef _ASM_GENERIC_HUGETLB_H
 #define _ASM_GENERIC_HUGETLB_H

+#include <linux/swap.h>
+#include <linux/swapops.h>
+
 static inline pte_t mk_huge_pte(struct page *page, pgprot_t pgprot)
 {
 	return mk_pte(page, pgprot);
@@ -80,6 +83,12 @@ static inline int huge_pte_none(pte_t pte)
 }
 #endif

+/* Please refer to comments above pte_none_mostly() for the usage */
+static inline int huge_pte_none_mostly(pte_t pte)
+{
+	return huge_pte_none(pte) || is_pte_marker(pte);
+}
+
 #ifndef __HAVE_ARCH_HUGE_PTE_WRPROTECT
 static inline pte_t huge_pte_wrprotect(pte_t pte)
 {
diff --git a/include/linux/swap.h b/include/linux/swap.h
index d1ea44b31f19..cc9adcfd666f 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -55,6 +55,19 @@ static inline int current_is_kswapd(void)
  * actions on faults.
  */

+/*
+ * PTE markers are used to persist information onto PTEs that are mapped with
+ * file-backed memories.  As its name "PTE" hints, it should only be applied to
+ * the leaves of pgtables.
+ */
+#ifdef CONFIG_PTE_MARKER
+#define SWP_PTE_MARKER_NUM 1
+#define SWP_PTE_MARKER     (MAX_SWAPFILES + SWP_HWPOISON_NUM + \
+			    SWP_MIGRATION_NUM + SWP_DEVICE_NUM)
+#else
+#define SWP_PTE_MARKER_NUM 0
+#endif
+
 /*
  * Unaddressable device memory support. See include/linux/hmm.h and
  * Documentation/vm/hmm.rst. Short description is we need struct pages for
@@ -100,7 +113,7 @@ static inline int current_is_kswapd(void)

 #define MAX_SWAPFILES \
 	((1 << MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \
-	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)
+	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM - SWP_PTE_MARKER_NUM)

 /*
  * Magic header for a swap area. The first part of the union is
diff --git a/include/linux/swapops.h b/include/linux/swapops.h
index d356ab4047f7..5103d2a4ae38 100644
--- a/include/linux/swapops.h
+++ b/include/linux/swapops.h
@@ -247,6 +247,84 @@ static inline int is_writable_migration_entry(swp_entry_t entry)

 #endif

+typedef unsigned long pte_marker;
+
+#define  PTE_MARKER_MASK     (0)
+
+#ifdef CONFIG_PTE_MARKER
+
+static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
+{
+	return swp_entry(SWP_PTE_MARKER, marker);
+}
+
+static inline bool is_pte_marker_entry(swp_entry_t entry)
+{
+	return swp_type(entry) == SWP_PTE_MARKER;
+}
+
+static inline pte_marker pte_marker_get(swp_entry_t entry)
+{
+	return swp_offset(entry) & PTE_MARKER_MASK;
+}
+
+static inline bool is_pte_marker(pte_t pte)
+{
+	return is_swap_pte(pte) && is_pte_marker_entry(pte_to_swp_entry(pte));
+}
+
+#else /* CONFIG_PTE_MARKER */
+
+static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
+{
+	/* This should never be called if !CONFIG_PTE_MARKER */
+	WARN_ON_ONCE(1);
+	return swp_entry(0, 0);
+}
+
+static inline bool is_pte_marker_entry(swp_entry_t entry)
+{
+	return false;
+}
+
+static inline pte_marker pte_marker_get(swp_entry_t entry)
+{
+	return 0;
+}
+
+static inline bool is_pte_marker(pte_t pte)
+{
+	return false;
+}
+
+#endif /* CONFIG_PTE_MARKER */
+
+static inline pte_t make_pte_marker(pte_marker marker)
+{
+	return swp_entry_to_pte(make_pte_marker_entry(marker));
+}
+
+/*
+ * This is a special version to check pte_none() just to cover the case when
+ * the pte is a pte marker.  It existed because in many cases the pte marker
+ * should be seen as a none pte; it's just that we have stored some information
+ * onto the none pte so it becomes not-none any more.
+ *
+ * It should be used when the pte is file-backed, ram-based and backing
+ * userspace pages, like shmem.  It is not needed upon pgtables that do not
+ * support pte markers at all.  For example, it's not needed on anonymous
+ * memory, kernel-only memory (including when the system is during-boot),
+ * non-ram based generic file-system.  It's fine to be used even there, but the
+ * extra pte marker check will be pure overhead.
+ *
+ * For systems configured with !CONFIG_PTE_MARKER this will be automatically
+ * optimized to pte_none().
+ */
+static inline int pte_none_mostly(pte_t pte)
+{
+	return pte_none(pte) || is_pte_marker(pte);
+}
+
 static inline struct page *pfn_swap_entry_to_page(swp_entry_t entry)
 {
 	struct page *p = pfn_to_page(swp_offset(entry));
diff --git a/mm/Kconfig b/mm/Kconfig
index 068ce591a13a..66f23c6c2032 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -897,6 +897,13 @@ config IO_MAPPING
 config SECRETMEM
 	def_bool ARCH_HAS_SET_DIRECT_MAP && !EMBEDDED

+config PTE_MARKER
+	def_bool n
+	bool "Marker PTEs support"
+
+	help
+	  Allows to create marker PTEs for file-backed memory.
+
 source "mm/damon/Kconfig"

 endmenu
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id CB7DCC433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:56:20 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id B129663223
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:56:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S230462AbhKOH7H (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 02:59:07 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:23373 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S230056AbhKOH6u (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:58:50 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962955;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=XNTxE1iM+okUx3tL+hKy8xQDAnTI8ZhKWAdYtMcr0FQ=;
        b=edGwuEOGQzGDeoPSwiSGYBEbIQ2qqlPLmLX4Ramx8ywiNOf+BEChBIq2KsiPJIvecId4qe
        dSgWobAAEvldWvY3BjAmm/Q/hH9wNX4fOhxd3rRv16rpDOs5usgTMcXfxjF+gPvFedKdNK
        R3D2XTYasRTdpsZgjon9izsdIgwEx5U=
Received: from mail-pg1-f198.google.com (mail-pg1-f198.google.com
 [209.85.215.198]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-442-9ySDSCMoPneVaKjkS_N-nw-1; Mon, 15 Nov 2021 02:55:54 -0500
X-MC-Unique: 9ySDSCMoPneVaKjkS_N-nw-1
Received: by mail-pg1-f198.google.com with SMTP id s8-20020a63af48000000b002e6c10ac245so1396008pgo.21
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:55:54 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=XNTxE1iM+okUx3tL+hKy8xQDAnTI8ZhKWAdYtMcr0FQ=;
        b=G3yr6m7DTiyn2vSJWdyYr+p1z7Ir5UVwUsF2TQnxl/dNAwsN5nlGpRFXKb2WxwS3MM
         1+X2GN1WIR/uqd+oSHh6ycK2ONSwd+cQhy5EQVceZqRd72iIZRCXzy3cqTv6K3T+yFhN
         6cbVDqn5ub8QtI9mOs2Zg8ab0hD3gM4YMRPkPDgngxX/FnH5jjCDiDMmr/cBMLZAjB+L
         Yn0aK1Uiqxql5UPd9C5h5KtGtPRb26xHB1PVWsMdePSejIJ57lFjlpbv8IImMTlJpqjc
         J5TquTJtzgu07oa9cfttWZHItS4uJNPk4umU3TlJkFYubNFo5aeUQ2CBjAismQVZOJGg
         6F0g==
X-Gm-Message-State: AOAM533JUgwlLcyqoC4O75zg4rhRZFlqK5jquR5W/83aOCkVdcDNnm3G
        yV+vXWrfN/95FZFBaNczprqyiII7e9hluN+KVX6Erjq4/NhUoE1i+2VvbvhRPn8PLQBwJoaX1D8
        C1w7TdTeEDv4vUO7LOclbdnLF
X-Received: by 2002:a17:902:d703:b0:140:125b:40a5 with SMTP id w3-20020a170902d70300b00140125b40a5mr33288596ply.65.1636962953205;
        Sun, 14 Nov 2021 23:55:53 -0800 (PST)
X-Google-Smtp-Source: ABdhPJzr2k4IPsVPsWDOJS1pWtswgYrOd6P3YKvfA/vFCF6qlLE/nRrQuC+VisMGWkBR++WhCEY/sw==
X-Received: by 2002:a17:902:d703:b0:140:125b:40a5 with SMTP id w3-20020a170902d70300b00140125b40a5mr33288572ply.65.1636962952914;
        Sun, 14 Nov 2021 23:55:52 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.55.45
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:55:52 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 02/23] mm: Teach core mm about pte markers
Date:   Mon, 15 Nov 2021 15:55:01 +0800
Message-Id: <20211115075522.73795-3-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This patch still does not use pte marker in any way, however it teaches the
core mm about the pte marker idea.

For example, handle_pte_marker() is introduced that will parse and handle all
the pte marker faults.

Many of the places are more about commenting it up - so that we know there's
the possibility of pte marker showing up, and why we don't need special code
for the cases.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 fs/userfaultfd.c | 10 ++++++----
 mm/filemap.c     |  5 +++++
 mm/hmm.c         |  2 +-
 mm/memcontrol.c  |  8 ++++++--
 mm/memory.c      | 23 +++++++++++++++++++++++
 mm/mincore.c     |  3 ++-
 mm/mprotect.c    |  3 +++
 7 files changed, 46 insertions(+), 8 deletions(-)

diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 22bf14ab2d16..fa24c72a849e 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -245,9 +245,10 @@ static inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,

 	/*
 	 * Lockless access: we're in a wait_event so it's ok if it
-	 * changes under us.
+	 * changes under us.  PTE markers should be handled the same as none
+	 * ptes here.
 	 */
-	if (huge_pte_none(pte))
+	if (huge_pte_none_mostly(pte))
 		ret = true;
 	if (!huge_pte_write(pte) && (reason & VM_UFFD_WP))
 		ret = true;
@@ -326,9 +327,10 @@ static inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,
 	pte = pte_offset_map(pmd, address);
 	/*
 	 * Lockless access: we're in a wait_event so it's ok if it
-	 * changes under us.
+	 * changes under us.  PTE markers should be handled the same as none
+	 * ptes here.
 	 */
-	if (pte_none(*pte))
+	if (pte_none_mostly(*pte))
 		ret = true;
 	if (!pte_write(*pte) && (reason & VM_UFFD_WP))
 		ret = true;
diff --git a/mm/filemap.c b/mm/filemap.c
index daa0e23a6ee6..9a7228b95b30 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -3327,6 +3327,11 @@ vm_fault_t filemap_map_pages(struct vm_fault *vmf,
 		vmf->pte += xas.xa_index - last_pgoff;
 		last_pgoff = xas.xa_index;

+		/*
+		 * NOTE: If there're PTE markers, we'll leave them to be
+		 * handled in the specific fault path, and it'll prohibit the
+		 * fault-around logic.
+		 */
 		if (!pte_none(*vmf->pte))
 			goto unlock;

diff --git a/mm/hmm.c b/mm/hmm.c
index 842e26599238..a0f72a540dc3 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -239,7 +239,7 @@ static int hmm_vma_handle_pte(struct mm_walk *walk, unsigned long addr,
 	pte_t pte = *ptep;
 	uint64_t pfn_req_flags = *hmm_pfn;

-	if (pte_none(pte)) {
+	if (pte_none_mostly(pte)) {
 		required_fault =
 			hmm_pte_need_fault(hmm_vma_walk, pfn_req_flags, 0);
 		if (required_fault)
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 781605e92015..eaddbc77aa5a 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -5692,10 +5692,14 @@ static enum mc_target_type get_mctgt_type(struct vm_area_struct *vma,

 	if (pte_present(ptent))
 		page = mc_handle_present_pte(vma, addr, ptent);
+	else if (pte_none_mostly(ptent))
+		/*
+		 * PTE markers should be treated as a none pte here, separated
+		 * from other swap handling below.
+		 */
+		page = mc_handle_file_pte(vma, addr, ptent);
 	else if (is_swap_pte(ptent))
 		page = mc_handle_swap_pte(vma, ptent, &ent);
-	else if (pte_none(ptent))
-		page = mc_handle_file_pte(vma, addr, ptent);

 	if (!page && !ent.val)
 		return ret;
diff --git a/mm/memory.c b/mm/memory.c
index e5d59a6b6479..04662b010005 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -98,6 +98,8 @@ struct page *mem_map;
 EXPORT_SYMBOL(mem_map);
 #endif

+static vm_fault_t do_fault(struct vm_fault *vmf);
+
 /*
  * A number of key systems in x86 including ioremap() rely on the assumption
  * that high_memory defines the upper bound on direct map memory, then end
@@ -1380,6 +1382,8 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			if (unlikely(zap_skip_check_mapping(details, page)))
 				continue;
 			rss[mm_counter(page)]--;
+		} else if (is_pte_marker_entry(entry)) {
+			/* By default, simply drop all pte markers when zap */
 		} else if (!non_swap_entry(entry)) {
 			rss[MM_SWAPENTS]--;
 			if (unlikely(!free_swap_and_cache(entry)))
@@ -3448,6 +3452,23 @@ static vm_fault_t remove_device_exclusive_entry(struct vm_fault *vmf)
 	return 0;
 }

+static vm_fault_t handle_pte_marker(struct vm_fault *vmf)
+{
+	swp_entry_t entry = pte_to_swp_entry(vmf->orig_pte);
+	unsigned long marker = pte_marker_get(entry);
+
+	/*
+	 * PTE markers should always be with file-backed memories, and the
+	 * marker should never be empty.  If anything weird happened, the best
+	 * thing to do is to kill the process along with its mm.
+	 */
+	if (WARN_ON_ONCE(vma_is_anonymous(vmf->vma) || !marker))
+		return VM_FAULT_SIGBUS;
+
+	/* TODO: handle pte markers */
+	return 0;
+}
+
 /*
  * We enter with non-exclusive mmap_lock (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
@@ -3484,6 +3505,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 			ret = vmf->page->pgmap->ops->migrate_to_ram(vmf);
 		} else if (is_hwpoison_entry(entry)) {
 			ret = VM_FAULT_HWPOISON;
+		} else if (is_pte_marker_entry(entry)) {
+			ret = handle_pte_marker(vmf);
 		} else {
 			print_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);
 			ret = VM_FAULT_SIGBUS;
diff --git a/mm/mincore.c b/mm/mincore.c
index 9122676b54d6..736869f4b409 100644
--- a/mm/mincore.c
+++ b/mm/mincore.c
@@ -121,7 +121,8 @@ static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
 	for (; addr != end; ptep++, addr += PAGE_SIZE) {
 		pte_t pte = *ptep;

-		if (pte_none(pte))
+		/* We need to do cache lookup too for pte markers */
+		if (pte_none_mostly(pte))
 			__mincore_unmapped_range(addr, addr + PAGE_SIZE,
 						 vma, vec);
 		else if (pte_present(pte))
diff --git a/mm/mprotect.c b/mm/mprotect.c
index e552f5e0ccbd..890bc1f9ca24 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -173,6 +173,9 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 					newpte = pte_swp_mksoft_dirty(newpte);
 				if (pte_swp_uffd_wp(oldpte))
 					newpte = pte_swp_mkuffd_wp(newpte);
+			} else if (is_pte_marker_entry(entry)) {
+				/* Skip it, the same as none pte */
+				continue;
 			} else {
 				newpte = oldpte;
 			}
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 1305DC433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:57:04 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id E236163217
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:57:03 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236450AbhKOH7p (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 02:59:45 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.129.124]:49014 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S229963AbhKOH7A (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:59:00 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962963;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=oiRP6M0XDz2JYaGWMJAJYTL9K9of+5C/YsBL0nPA2ZA=;
        b=IhguAwvMbts4pFhE3gPLygpTS7tqs32m4tAPIlxj2mIpKCmcwXFElAKkl24T2KZamG3SYq
        mZeJKmvWhk05+mP8cBgE5MvtKuAM9LUrjrKf3+9XHKMK544A96aMkN6NphZCLFsniFGItA
        v28CTFmm+e7i7QZpZd86VozorpduB5s=
Received: from mail-pg1-f197.google.com (mail-pg1-f197.google.com
 [209.85.215.197]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-227-rmSb7d_RNKuUboenfHlumw-1; Mon, 15 Nov 2021 02:56:01 -0500
X-MC-Unique: rmSb7d_RNKuUboenfHlumw-1
Received: by mail-pg1-f197.google.com with SMTP id f7-20020a63f107000000b002db96febb74so8783895pgi.7
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:56:01 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=oiRP6M0XDz2JYaGWMJAJYTL9K9of+5C/YsBL0nPA2ZA=;
        b=MBNJdHXkPb1vWGiL1A8QBX8126k48AlrSg+KRJM4ezm4AQRoyNz2QKxfStcqHQmcCx
         PtTLbsGbeOPgQ8m6NwXHc24MiT6/+sGuZVTnYyjBrFlrgvG7b21fOXLFYHJdtdIxxqhY
         rfCS1DqT/KvzRxi/fnPRdCUs4U1SevXgF9pWUZFVIblIAxNWbJq/ZaMN7A/gZPghUX/o
         H6H2HeIjbBkNMccufovIAfPGsr/LQut6WfoX70atSUbQN4MdpPtbj9G+5pqgmnaFZkCl
         HJFHmo9kXYyE2CAtoc5r0nvlZXVHHuX3ieuYkFWGM2SdW9spqXQyXITfzOt/aDCT92Yi
         6uzw==
X-Gm-Message-State: AOAM5308rzEt+Jl03uJUINOm6cUtHMmdSfzKBsbEGrVxNsAsutkDzZjU
        pOY6G+10Rmqigktvp9UQeTJes0uOMIzYE9BmSOUsSitl45hZVm3J0OGTEL3oMr3qpMn/ASWcOww
        /WPFEevJHHQU/X3bfHxjEOhfg
X-Received: by 2002:a17:90a:df97:: with SMTP id p23mr44899741pjv.3.1636962960895;
        Sun, 14 Nov 2021 23:56:00 -0800 (PST)
X-Google-Smtp-Source: ABdhPJw80QtT18dNY1cg9toIlmKOYG28dAqiAY105pUOfD9FUEbM6Y8GthQX1coYmOY4ZZLz+1zQRg==
X-Received: by 2002:a17:90a:df97:: with SMTP id p23mr44899715pjv.3.1636962960688;
        Sun, 14 Nov 2021 23:56:00 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.55.53
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:56:00 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 03/23] mm: Check against orig_pte for finish_fault()
Date:   Mon, 15 Nov 2021 15:55:02 +0800
Message-Id: <20211115075522.73795-4-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

We used to check against none pte and in those cases orig_pte should always be
none pte anyway.

This change prepares us to be able to call do_fault() on !none ptes.  For
example, we should allow that to happen for pte marker so that we can restore
information out of the pte markers.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/memory.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/mm/memory.c b/mm/memory.c
index 04662b010005..d5966d9e24c3 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4052,7 +4052,7 @@ vm_fault_t finish_fault(struct vm_fault *vmf)
 				      vmf->address, &vmf->ptl);
 	ret = 0;
 	/* Re-check under ptl */
-	if (likely(pte_none(*vmf->pte)))
+	if (likely(pte_same(*vmf->pte, vmf->orig_pte)))
 		do_set_pte(vmf, page, vmf->address);
 	else
 		ret = VM_FAULT_NOPAGE;
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 8111DC433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:57:31 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 5E1C460E08
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:57:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236690AbhKOIAW (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:00:22 -0500
Received: from us-smtp-delivery-124.mimecast.com ([216.205.24.124]:55183 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S230354AbhKOH7J (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:59:09 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962971;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=PmskQuQ5v+E/C6hWNn1p+tojqPkwHTmjUCaKkLoPmAQ=;
        b=E50XXQNuXfckZr7BGWobqqQ1bY8cOJfQ8DWzP5MHaw3y15bUUh2ATk6onmiaTO1OuYzpm4
        I1aozSGT6Fr2FaBLXOLW4bpwO9w4pGdUJCKza10XDY4ZI/u/wVP65hYyktZPHdrOTOoLn2
        ekcT2GRweYCLfJmHGxS2tnGx23bKqUw=
Received: from mail-pj1-f69.google.com (mail-pj1-f69.google.com
 [209.85.216.69]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-525-No0TQVycOXKdIAuQ3ztKAg-1; Mon, 15 Nov 2021 02:56:10 -0500
X-MC-Unique: No0TQVycOXKdIAuQ3ztKAg-1
Received: by mail-pj1-f69.google.com with SMTP id x1-20020a17090a294100b001a6e7ba6b4eso8618541pjf.9
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:56:09 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=PmskQuQ5v+E/C6hWNn1p+tojqPkwHTmjUCaKkLoPmAQ=;
        b=EmH2iTF/V8nhG9rwP8VmwXE7PCS5UcdapgpG/fKutMc0QGrCj3wPIK3POUrYPnhJO/
         PqRl/Y+UO4z6L6Go5co9TykDfKV8e/6d0D0w4RAi2dRvwXntHnks7P+f2oFoblg5e+/z
         lOXenN29cODMzA4GnSl4aRKWJ5wWA5YRJtR5iXVQgV2pbbNm+EZX2/qj23a9Nf1n7kR/
         JkElb9bmEUaFPz0SjSC5r+9gr/ZhUK/zDY/ZAYpRAAQv/Mt3T2O/4tjLWgwRMJtzzzB6
         o20T2dHqtXmzcyFYVn22gbgjUUCCb6MCsdub8rl0gbzvW1W5zyIRh5u8MO+V1K/25f7I
         Rbog==
X-Gm-Message-State: AOAM530z1e9FPHbrfJkAWSE+Z0pIk4xdzKIkpnZPHtTyWY1rO62HGwNw
        4itjn2HCNa/HrWt0uz5J7pa4WaBl9zUfl5JLXSIBQJjgsR+OTzWN2IjucaRWEGFKoBAUWE0eWoN
        Zwe4PD+llC26El1nGfiGxMj/f
X-Received: by 2002:a05:6a00:1901:b0:44b:e041:f07f with SMTP id y1-20020a056a00190100b0044be041f07fmr31906441pfi.52.1636962968532;
        Sun, 14 Nov 2021 23:56:08 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwO6oDN0fGNLF/myzmtv7RWKcD4B2LB1pQ6VJVj/upm3PYad0YwxcCTvIC/G3q3fUkWS4EYpg==
X-Received: by 2002:a05:6a00:1901:b0:44b:e041:f07f with SMTP id y1-20020a056a00190100b0044be041f07fmr31906409pfi.52.1636962968263;
        Sun, 14 Nov 2021 23:56:08 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.56.01
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:56:07 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 04/23] mm/uffd: PTE_MARKER_UFFD_WP
Date:   Mon, 15 Nov 2021 15:55:03 +0800
Message-Id: <20211115075522.73795-5-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This patch introduces the 1st user of pte marker: the uffd-wp marker.

When the pte marker is installed with the uffd-wp bit set, it means this pte
was wr-protected by uffd.

We will use this special pte to arm the ptes that got either unmapped or
swapped out for a file-backed region that was previously wr-protected.  This
special pte could trigger a page fault just like swap entries.

This idea is greatly inspired by Hugh and Andrea in the discussion, which is
referenced in the links below.

Some helpers are introduced to detect whether a swap pte is uffd wr-protected.
After the pte marker introduced, one swap pte can be wr-protected in two forms:
either it is a normal swap pte and it has _PAGE_SWP_UFFD_WP set, or it's a pte
marker that has PTE_MARKER_UFFD_WP set.

Link: https://lore.kernel.org/lkml/20201126222359.8120-1-peterx@redhat.com/
Link: https://lore.kernel.org/lkml/20201130230603.46187-1-peterx@redhat.com/
Suggested-by: Andrea Arcangeli <aarcange@redhat.com>
Suggested-by: Hugh Dickins <hughd@google.com>
Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/linux/swapops.h       |  3 ++-
 include/linux/userfaultfd_k.h | 38 +++++++++++++++++++++++++++++++++++
 mm/Kconfig                    |  9 +++++++++
 3 files changed, 49 insertions(+), 1 deletion(-)

diff --git a/include/linux/swapops.h b/include/linux/swapops.h
index 5103d2a4ae38..2cec3ef355a7 100644
--- a/include/linux/swapops.h
+++ b/include/linux/swapops.h
@@ -249,7 +249,8 @@ static inline int is_writable_migration_entry(swp_entry_t entry)

 typedef unsigned long pte_marker;

-#define  PTE_MARKER_MASK     (0)
+#define  PTE_MARKER_UFFD_WP  BIT(0)
+#define  PTE_MARKER_MASK     (PTE_MARKER_UFFD_WP)

 #ifdef CONFIG_PTE_MARKER

diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 33cea484d1ad..7d7ffec53ddb 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -15,6 +15,8 @@

 #include <linux/fcntl.h>
 #include <linux/mm.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
 #include <asm-generic/pgtable_uffd.h>

 /* The set of all possible UFFD-related VM flags. */
@@ -236,4 +238,40 @@ static inline void userfaultfd_unmap_complete(struct mm_struct *mm,

 #endif /* CONFIG_USERFAULTFD */

+static inline bool is_pte_marker_uffd_wp(pte_t pte)
+{
+#ifdef CONFIG_PTE_MARKER_UFFD_WP
+	swp_entry_t entry;
+
+	if (!is_swap_pte(pte))
+		return false;
+
+	entry = pte_to_swp_entry(pte);
+
+	return is_pte_marker_entry(entry) &&
+	    (pte_marker_get(entry) & PTE_MARKER_UFFD_WP);
+#else
+	return false;
+#endif
+}
+
+/*
+ * Returns true if this is a swap pte and was uffd-wp wr-protected in either
+ * forms (pte marker or a normal swap pte), false otherwise.
+ */
+static inline bool pte_swp_uffd_wp_any(pte_t pte)
+{
+#ifdef CONFIG_PTE_MARKER_UFFD_WP
+	if (!is_swap_pte(pte))
+		return false;
+
+	if (pte_swp_uffd_wp(pte))
+		return true;
+
+	if (is_pte_marker_uffd_wp(pte))
+		return true;
+#endif
+	return false;
+}
+
 #endif /* _LINUX_USERFAULTFD_K_H */
diff --git a/mm/Kconfig b/mm/Kconfig
index 66f23c6c2032..f01c8e0afadf 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -904,6 +904,15 @@ config PTE_MARKER
 	help
 	  Allows to create marker PTEs for file-backed memory.

+config PTE_MARKER_UFFD_WP
+	bool "Marker PTEs support for userfaultfd write protection"
+	depends on PTE_MARKER && HAVE_ARCH_USERFAULTFD_WP
+
+	help
+	  Allows to create marker PTEs for userfaultfd write protection
+	  purposes.  It is required to enable userfaultfd write protection on
+	  file-backed memory types like shmem and hugetlbfs.
+
 source "mm/damon/Kconfig"

 endmenu
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 3E3F9C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:58:03 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 1AE0B60E8D
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:58:03 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236678AbhKOIAz (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:00:55 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:28565 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S230418AbhKOH7W (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:59:22 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962987;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=PQTAYR2U0B9NQT5q5Dh9pQHL0/MM7Qz5TQPJEv+v98g=;
        b=fVQDpNllnVkoHborceNRRJenMuUzKbgvBCOs6/U4tHY1Q+TvgtSNWFAwY9bZuNleRzD7xF
        XZRksP++zhsXvbxv+fAGvenzoXgoOHGvTgjj5ttYCfyC9CqW//IHr0tKlTGvuqaPI9S0Mx
        6e3bWeCD8xITUyGP8xcPHp18oXhvqXM=
Received: from mail-pj1-f69.google.com (mail-pj1-f69.google.com
 [209.85.216.69]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-414-hNMXCEovP02jqIykmRYpLQ-1; Mon, 15 Nov 2021 02:56:25 -0500
X-MC-Unique: hNMXCEovP02jqIykmRYpLQ-1
Received: by mail-pj1-f69.google.com with SMTP id o4-20020a17090a3d4400b001a66f10df6cso4880344pjf.0
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:56:25 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=PQTAYR2U0B9NQT5q5Dh9pQHL0/MM7Qz5TQPJEv+v98g=;
        b=VsxkS/1+KjRQbru9RazNMclgGJfxO9xS1y6PmTOXSmKh2MhCgCIxi7LRTj6T5Wfy2U
         A8XvBUTugZYMKyzRwM2b+Q3pVtF5sO+IqJzr0ZnFZLTOSbBDfRA2lZ8J3yO6vXLZrsMs
         IZbDNwAl3ojnmyryAmlLy5QKIP4ILIwvW8FkBr6EVV/z9Gw+lsFt9Dswwi1GuGtcXNS6
         zJGseyZBBNhYTr7VW9LeBfqwy/dyv8AxkZhISecPKBz34v2vips3sRqjbvyr23reH1h8
         S6XT3H82i4GvRvGLrOB1BmqHhQ2tTeQJc2FXgdCdBxhMD9qFGaXxwdphZIsJnADJyEZs
         2WoA==
X-Gm-Message-State: AOAM531yIC/s1rUImfHFU0+T7947erAODDHMGrC4ml0Z0gaupA74uCtR
        RDBAeidVSOIGu2WCLYNAMcxy0Gsw17FyBWxWv1Fhhg2GAbx/WNBuokxMwutw3SatbKzLHgMgCp3
        RVxB54PA/4fmjptHJp1VhMjcY
X-Received: by 2002:a17:90a:ca81:: with SMTP id y1mr61191340pjt.231.1636962984418;
        Sun, 14 Nov 2021 23:56:24 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwQVZcj01aFlbyqYO4BSw1DS5ur5lPKwB1eRxjFMR30tJL7G2a+EjobAshXx0ZYvcchxwbzZQ==
X-Received: by 2002:a17:90a:ca81:: with SMTP id y1mr61191302pjt.231.1636962984128;
        Sun, 14 Nov 2021 23:56:24 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.56.16
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:56:23 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 06/23] mm/shmem: Handle uffd-wp special pte in page fault handler
Date:   Mon, 15 Nov 2021 15:55:05 +0800
Message-Id: <20211115075522.73795-7-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

File-backed memories are prone to unmap/swap so the ptes are always unstable,
because they can be easily faulted back later using the page cache.  This could
lead to uffd-wp getting lost when unmapping or swapping out such memory.  One
example is shmem.  PTE markers are needed to store those information.

This patch prepares it by handling uffd-wp pte markers first it is applied
elsewhere, so that the page fault handler can recognize uffd-wp pte markers.

The handling of uffd-wp pte markers is similar to missing fault, it's just that
we'll handle this "missing fault" when we see the pte markers, meanwhile we
need to make sure the marker information is kept during processing the fault.

This is a slow path of uffd-wp handling, because zapping of wr-protected shmem
ptes should be rare.  So far it should only trigger in two conditions:

  (1) When trying to punch holes in shmem_fallocate(), there is an optimization
      to zap the pgtables before evicting the page.

  (2) When swapping out shmem pages.

Because of this, the page fault handling is simplifed too by not sending the
wr-protect message in the 1st page fault, instead the page will be installed
read-only, so the uffd-wp message will be generated in the next fault, which
will trigger the do_wp_page() path of general uffd-wp handling.

Disable fault-around for all uffd-wp registered ranges for extra safety just
like uffd-minor fault, and clean the code up.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/linux/userfaultfd_k.h | 17 +++++++++
 mm/memory.c                   | 71 ++++++++++++++++++++++++++++++-----
 2 files changed, 79 insertions(+), 9 deletions(-)

diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 7d7ffec53ddb..05cec02140cb 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -96,6 +96,18 @@ static inline bool uffd_disable_huge_pmd_share(struct vm_area_struct *vma)
 	return vma->vm_flags & (VM_UFFD_WP | VM_UFFD_MINOR);
 }

+/*
+ * Don't do fault around for either WP or MINOR registered uffd range.  For
+ * MINOR registered range, fault around will be a total disaster and ptes can
+ * be installed without notifications; for WP it should mostly be fine as long
+ * as the fault around checks for pte_none() before the installation, however
+ * to be super safe we just forbid it.
+ */
+static inline bool uffd_disable_fault_around(struct vm_area_struct *vma)
+{
+	return vma->vm_flags & (VM_UFFD_WP | VM_UFFD_MINOR);
+}
+
 static inline bool userfaultfd_missing(struct vm_area_struct *vma)
 {
 	return vma->vm_flags & VM_UFFD_MISSING;
@@ -236,6 +248,11 @@ static inline void userfaultfd_unmap_complete(struct mm_struct *mm,
 {
 }

+static inline bool uffd_disable_fault_around(struct vm_area_struct *vma)
+{
+	return false;
+}
+
 #endif /* CONFIG_USERFAULTFD */

 static inline bool is_pte_marker_uffd_wp(pte_t pte)
diff --git a/mm/memory.c b/mm/memory.c
index d5966d9e24c3..e8557d43a87d 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3452,6 +3452,43 @@ static vm_fault_t remove_device_exclusive_entry(struct vm_fault *vmf)
 	return 0;
 }

+static vm_fault_t pte_marker_clear(struct vm_fault *vmf)
+{
+	vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd,
+				       vmf->address, &vmf->ptl);
+	/*
+	 * Be careful so that we will only recover a special uffd-wp pte into a
+	 * none pte.  Otherwise it means the pte could have changed, so retry.
+	 */
+	if (is_pte_marker(*vmf->pte))
+		pte_clear(vmf->vma->vm_mm, vmf->address, vmf->pte);
+	pte_unmap_unlock(vmf->pte, vmf->ptl);
+	return 0;
+}
+
+/*
+ * This is actually a page-missing access, but with uffd-wp special pte
+ * installed.  It means this pte was wr-protected before being unmapped.
+ */
+static vm_fault_t pte_marker_handle_uffd_wp(struct vm_fault *vmf)
+{
+	/* Careful!  vmf->pte unmapped after return */
+	if (!pte_unmap_same(vmf))
+		return 0;
+
+	/*
+	 * Just in case there're leftover special ptes even after the region
+	 * got unregistered - we can simply clear them.  We can also do that
+	 * proactively when e.g. when we do UFFDIO_UNREGISTER upon some uffd-wp
+	 * ranges, but it should be more efficient to be done lazily here.
+	 */
+	if (unlikely(!userfaultfd_wp(vmf->vma) || vma_is_anonymous(vmf->vma)))
+		return pte_marker_clear(vmf);
+
+	/* do_fault() can handle pte markers too like none pte */
+	return do_fault(vmf);
+}
+
 static vm_fault_t handle_pte_marker(struct vm_fault *vmf)
 {
 	swp_entry_t entry = pte_to_swp_entry(vmf->orig_pte);
@@ -3465,8 +3502,11 @@ static vm_fault_t handle_pte_marker(struct vm_fault *vmf)
 	if (WARN_ON_ONCE(vma_is_anonymous(vmf->vma) || !marker))
 		return VM_FAULT_SIGBUS;

-	/* TODO: handle pte markers */
-	return 0;
+	if (marker & PTE_MARKER_UFFD_WP)
+		return pte_marker_handle_uffd_wp(vmf);
+
+	/* This is an unknown pte marker */
+	return VM_FAULT_SIGBUS;
 }

 /*
@@ -3968,6 +4008,7 @@ vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)
 void do_set_pte(struct vm_fault *vmf, struct page *page, unsigned long addr)
 {
 	struct vm_area_struct *vma = vmf->vma;
+	bool uffd_wp = is_pte_marker_uffd_wp(vmf->orig_pte);
 	bool write = vmf->flags & FAULT_FLAG_WRITE;
 	bool prefault = vmf->address != addr;
 	pte_t entry;
@@ -3982,6 +4023,8 @@ void do_set_pte(struct vm_fault *vmf, struct page *page, unsigned long addr)

 	if (write)
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+	if (unlikely(uffd_wp))
+		entry = pte_mkuffd_wp(pte_wrprotect(entry));
 	/* copy-on-write page */
 	if (write && !(vma->vm_flags & VM_SHARED)) {
 		inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
@@ -4155,9 +4198,21 @@ static vm_fault_t do_fault_around(struct vm_fault *vmf)
 	return vmf->vma->vm_ops->map_pages(vmf, start_pgoff, end_pgoff);
 }

+/* Return true if we should do read fault-around, false otherwise */
+static inline bool should_fault_around(struct vm_fault *vmf)
+{
+	/* No ->map_pages?  No way to fault around... */
+	if (!vmf->vma->vm_ops->map_pages)
+		return false;
+
+	if (uffd_disable_fault_around(vmf->vma))
+		return false;
+
+	return fault_around_bytes >> PAGE_SHIFT > 1;
+}
+
 static vm_fault_t do_read_fault(struct vm_fault *vmf)
 {
-	struct vm_area_struct *vma = vmf->vma;
 	vm_fault_t ret = 0;

 	/*
@@ -4165,12 +4220,10 @@ static vm_fault_t do_read_fault(struct vm_fault *vmf)
 	 * if page by the offset is not ready to be mapped (cold cache or
 	 * something).
 	 */
-	if (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {
-		if (likely(!userfaultfd_minor(vmf->vma))) {
-			ret = do_fault_around(vmf);
-			if (ret)
-				return ret;
-		}
+	if (should_fault_around(vmf)) {
+		ret = do_fault_around(vmf);
+		if (ret)
+			return ret;
 	}

 	ret = __do_fault(vmf);
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id C80CAC433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:58:13 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id B074663218
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:58:13 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236489AbhKOIAk (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:00:40 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.129.124]:32745 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236397AbhKOH7a (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:59:30 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962995;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=bNc70TgztEilFYXrr/S3QzbmII7tOwB+GzXzdIRG250=;
        b=I3d6NV6S79SbdI/EqQHpvnf+HKSOVRvJ4kPICHWZ3xcj/z22ci4DTJVG+z3GgSc0AatzrY
        hG9kBRQgQkpH3rhRFVdz3m5AXcN3dlQKdM0tpXNCF3mvdBdLM7K3VTxYmLah58ipdegRQr
        Szm+lMD9/sSNqswNQGlmnx/7C1jfKZ4=
Received: from mail-pj1-f72.google.com (mail-pj1-f72.google.com
 [209.85.216.72]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-297-zwEWQSU6PNWHYIvHwZpEyA-1; Mon, 15 Nov 2021 02:56:33 -0500
X-MC-Unique: zwEWQSU6PNWHYIvHwZpEyA-1
Received: by mail-pj1-f72.google.com with SMTP id l10-20020a17090a4d4a00b001a6f817f57eso8612410pjh.3
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:56:33 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=bNc70TgztEilFYXrr/S3QzbmII7tOwB+GzXzdIRG250=;
        b=D6KltdgCGxwLMmK3KiRRK0bqRMoJ3jLLpxLVMREt6ETnjl1dRmyG75tfEkYkWsZLRP
         gah2LwTny5IbiPhRCo8VPMahcWBiFZZsaEq5RwDEhLOuJJ5AYVW46nd40jokODkLEEhz
         7g0iPunPFvLsREC0wpQdXxR4En7hL61mtCLnz/7aNEZABOqEMVayTVTTun3IqZkfEbim
         DqgG8QgEW8CXbtbvB53xyY4T0fKc2GDRniRZrzmA34UWQa/osmgXhEQFR4v3Sg5FclV6
         6JNM3jVJFmX4wnGlLHIECAvW/24S2ftPPUvypWH2C+KKgSDWqUvkpPymR7h5U3YJU7zB
         ZH+Q==
X-Gm-Message-State: AOAM533VOZOaBaDxnow+GFoQaGbHP1XAOyc+C4ueD/MsXqsLSppS5hVL
        3cmsdVWwA4yGoP+2NSYlPPPh6IaHQtmmajID7rc0L5ko0WcxAdOV5jn3GGR4hvx1fjbH/yHKGdb
        ZgveRNFF1XbNqmGUsXh5KMEQ1
X-Received: by 2002:a17:90a:ec15:: with SMTP id l21mr6509920pjy.48.1636962991987;
        Sun, 14 Nov 2021 23:56:31 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwcWLiSu54dg3NGApWSpCAOU0KSpIFrzWp5wpXMgcgfTovXA+wja+X2pK0NLh2KBFy3bdsyhw==
X-Received: by 2002:a17:90a:ec15:: with SMTP id l21mr6509874pjy.48.1636962991714;
        Sun, 14 Nov 2021 23:56:31 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.56.24
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:56:31 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 07/23] mm/shmem: Persist uffd-wp bit across zapping for file-backed
Date:   Mon, 15 Nov 2021 15:55:06 +0800
Message-Id: <20211115075522.73795-8-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

File-backed memory is prone to being unmapped at any time.  It means all
information in the pte will be dropped, including the uffd-wp flag.

To persist the uffd-wp flag, we'll use the pte markers.  This patch teaches the
zap code to understand uffd-wp and know when to keep or drop the uffd-wp bit.

Add a new flag ZAP_FLAG_DROP_MARKER and set it in zap_details when we don't
want to persist such an information, for example, when destroying the whole
vma, or punching a hole in a shmem file.  For the rest cases we should never
drop the uffd-wp bit, or the wr-protect information will get lost.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/linux/mm.h        | 20 +++++++++++++++++
 include/linux/mm_inline.h | 45 +++++++++++++++++++++++++++++++++++++++
 mm/memory.c               | 38 +++++++++++++++++++++++++++++++--
 mm/rmap.c                 |  8 +++++++
 4 files changed, 109 insertions(+), 2 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index a7e4a9e7d807..015e287063a8 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1825,12 +1825,23 @@ static inline bool can_do_mlock(void) { return false; }
 extern int user_shm_lock(size_t, struct ucounts *);
 extern void user_shm_unlock(size_t, struct ucounts *);

+typedef unsigned int __bitwise zap_flags_t;
+
+/*
+ * Whether to drop the pte markers, for example, the uffd-wp information for
+ * file-backed memory.  This should only be specified when we will completely
+ * drop the page in the mm, either by truncation or unmapping of the vma.  By
+ * default, the flag is not set.
+ */
+#define  ZAP_FLAG_DROP_MARKER        ((__force zap_flags_t) BIT(0))
+
 /*
  * Parameter block passed down to zap_pte_range in exceptional cases.
  */
 struct zap_details {
 	struct address_space *zap_mapping;	/* Check page->mapping if set */
 	struct page *single_page;		/* Locked page to be unmapped */
+	zap_flags_t zap_flags;			/* Extra flags for zapping */
 };

 /*
@@ -1847,6 +1858,15 @@ zap_skip_check_mapping(struct zap_details *details, struct page *page)
 	    (details->zap_mapping != page_rmapping(page));
 }

+static inline bool
+zap_drop_file_uffd_wp(struct zap_details *details)
+{
+	if (!details)
+		return false;
+
+	return details->zap_flags & ZAP_FLAG_DROP_MARKER;
+}
+
 struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 			     pte_t pte);
 struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,
diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h
index e2ec68b0515c..ca861e910938 100644
--- a/include/linux/mm_inline.h
+++ b/include/linux/mm_inline.h
@@ -4,6 +4,8 @@

 #include <linux/huge_mm.h>
 #include <linux/swap.h>
+#include <linux/userfaultfd_k.h>
+#include <linux/swapops.h>

 /**
  * folio_is_file_lru - Should the folio be on a file LRU or anon LRU?
@@ -135,4 +137,47 @@ static __always_inline void del_page_from_lru_list(struct page *page,
 {
 	lruvec_del_folio(lruvec, page_folio(page));
 }
+
+/*
+ * If this pte is wr-protected by uffd-wp in any form, arm the special pte to
+ * replace a none pte.  NOTE!  This should only be called when *pte is already
+ * cleared so we will never accidentally replace something valuable.  Meanwhile
+ * none pte also means we are not demoting the pte so tlb flushed is not needed.
+ * E.g., when pte cleared the caller should have taken care of the tlb flush.
+ *
+ * Must be called with pgtable lock held so that no thread will see the none
+ * pte, and if they see it, they'll fault and serialize at the pgtable lock.
+ *
+ * This function is a no-op if PTE_MARKER_UFFD_WP is not enabled.
+ */
+static inline void
+pte_install_uffd_wp_if_needed(struct vm_area_struct *vma, unsigned long addr,
+			      pte_t *pte, pte_t pteval)
+{
+#ifdef CONFIG_PTE_MARKER_UFFD_WP
+	bool arm_uffd_pte = false;
+
+	/* The current status of the pte should be "cleared" before calling */
+	WARN_ON_ONCE(!pte_none(*pte));
+
+	if (vma_is_anonymous(vma))
+		return;
+
+	/* A uffd-wp wr-protected normal pte */
+	if (unlikely(pte_present(pteval) && pte_uffd_wp(pteval)))
+		arm_uffd_pte = true;
+
+	/*
+	 * A uffd-wp wr-protected swap pte.  Note: this should even cover an
+	 * existing pte marker with uffd-wp bit set.
+	 */
+	if (unlikely(pte_swp_uffd_wp_any(pteval)))
+		arm_uffd_pte = true;
+
+	if (unlikely(arm_uffd_pte))
+		set_pte_at(vma->vm_mm, addr, pte,
+			   make_pte_marker(PTE_MARKER_UFFD_WP));
+#endif
+}
+
 #endif
diff --git a/mm/memory.c b/mm/memory.c
index e8557d43a87d..fef6a91c5dfb 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -73,6 +73,7 @@
 #include <linux/perf_event.h>
 #include <linux/ptrace.h>
 #include <linux/vmalloc.h>
+#include <linux/mm_inline.h>

 #include <trace/events/kmem.h>

@@ -1306,6 +1307,21 @@ copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
 	return ret;
 }

+/*
+ * This function makes sure that we'll replace the none pte with an uffd-wp
+ * swap special pte marker when necessary. Must be with the pgtable lock held.
+ */
+static inline void
+zap_install_uffd_wp_if_needed(struct vm_area_struct *vma,
+			      unsigned long addr, pte_t *pte,
+			      struct zap_details *details, pte_t pteval)
+{
+	if (zap_drop_file_uffd_wp(details))
+		return;
+
+	pte_install_uffd_wp_if_needed(vma, addr, pte, pteval);
+}
+
 static unsigned long zap_pte_range(struct mmu_gather *tlb,
 				struct vm_area_struct *vma, pmd_t *pmd,
 				unsigned long addr, unsigned long end,
@@ -1343,6 +1359,8 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			ptent = ptep_get_and_clear_full(mm, addr, pte,
 							tlb->fullmm);
 			tlb_remove_tlb_entry(tlb, pte, addr);
+			zap_install_uffd_wp_if_needed(vma, addr, pte, details,
+						      ptent);
 			if (unlikely(!page))
 				continue;

@@ -1373,6 +1391,13 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			page = pfn_swap_entry_to_page(entry);
 			if (unlikely(zap_skip_check_mapping(details, page)))
 				continue;
+			/*
+			 * Both device private/exclusive mappings should only
+			 * work with anonymous page so far, so we don't need to
+			 * consider uffd-wp bit when zap. For more information,
+			 * see zap_install_uffd_wp_if_needed().
+			 */
+			WARN_ON_ONCE(!vma_is_anonymous(vma));
 			rss[mm_counter(page)]--;
 			if (is_device_private_entry(entry))
 				page_remove_rmap(page, false);
@@ -1383,13 +1408,18 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 				continue;
 			rss[mm_counter(page)]--;
 		} else if (is_pte_marker_entry(entry)) {
-			/* By default, simply drop all pte markers when zap */
+			/* Currently there's only uffd-wp marker bit */
+			WARN_ON_ONCE(!(pte_marker_get(entry) & PTE_MARKER_UFFD_WP));
+			/* Only drop the uffd-wp marker if explicitly requested */
+			if (!zap_drop_file_uffd_wp(details))
+				continue;
 		} else if (!non_swap_entry(entry)) {
 			rss[MM_SWAPENTS]--;
 			if (unlikely(!free_swap_and_cache(entry)))
 				print_bad_pte(vma, addr, ptent, NULL);
 		}
 		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
+		zap_install_uffd_wp_if_needed(vma, addr, pte, details, ptent);
 	} while (pte++, addr += PAGE_SIZE, addr != end);

 	add_mm_rss_vec(mm, rss);
@@ -1600,12 +1630,15 @@ void unmap_vmas(struct mmu_gather *tlb,
 		unsigned long end_addr)
 {
 	struct mmu_notifier_range range;
+	struct zap_details details = {
+		.zap_flags = ZAP_FLAG_DROP_MARKER,
+	};

 	mmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma, vma->vm_mm,
 				start_addr, end_addr);
 	mmu_notifier_invalidate_range_start(&range);
 	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next)
-		unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);
+		unmap_single_vma(tlb, vma, start_addr, end_addr, &details);
 	mmu_notifier_invalidate_range_end(&range);
 }

@@ -3350,6 +3383,7 @@ void unmap_mapping_page(struct page *page)

 	details.zap_mapping = mapping;
 	details.single_page = page;
+	details.zap_flags = ZAP_FLAG_DROP_MARKER;

 	i_mmap_lock_write(mapping);
 	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root)))
diff --git a/mm/rmap.c b/mm/rmap.c
index 163ac4e6bcee..89068e957486 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -73,6 +73,7 @@
 #include <linux/page_idle.h>
 #include <linux/memremap.h>
 #include <linux/userfaultfd_k.h>
+#include <linux/mm_inline.h>

 #include <asm/tlbflush.h>

@@ -1517,6 +1518,13 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 			pteval = ptep_clear_flush(vma, address, pvmw.pte);
 		}

+		/*
+		 * Now the pte is cleared.  If this is uffd-wp armed pte, we
+		 * may want to replace a none pte with a marker pte if it's
+		 * file-backed, so we don't lose the tracking information.
+		 */
+		pte_install_uffd_wp_if_needed(vma, address, pvmw.pte, pteval);
+
 		/* Move the dirty bit to the page. Now the pte is gone. */
 		if (pte_dirty(pteval))
 			set_page_dirty(page);
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id C1DFAC433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:58:19 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id A97BC60F5B
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 07:58:19 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S237042AbhKOIBN (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:01:13 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:39118 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S230056AbhKOH7N (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 02:59:13 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636962978;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=ubfx2uHai2Y0k1b/BYqRnZVepmNNtt1cqqHxspLPNNE=;
        b=UPywxh7W19DhseIZIj+xcPOkYgexNhpsH2OUNSQJbPii/cVb0cusHYglyuCFNA4S0mBQYA
        1DWPlAU6RAGfsRs5ivT1sZAw0Hq47nqYvKQGKB6sb8aQiR344J2f5g5AefCZKZx2lWsG0O
        mGvfI7DuJXgernaRGKlJwWyqbU0K21w=
Received: from mail-pj1-f71.google.com (mail-pj1-f71.google.com
 [209.85.216.71]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-528-NpKKNQvnNXuq1eNsbL2ypw-1; Mon, 15 Nov 2021 02:56:17 -0500
X-MC-Unique: NpKKNQvnNXuq1eNsbL2ypw-1
Received: by mail-pj1-f71.google.com with SMTP id jx2-20020a17090b46c200b001a62e9db321so8612934pjb.7
        for <linux-kernel@vger.kernel.org>; Sun, 14 Nov 2021 23:56:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=ubfx2uHai2Y0k1b/BYqRnZVepmNNtt1cqqHxspLPNNE=;
        b=F1je25uwPk3bL5+FwF3krZXzQy8Jab3P7T9heEfNhC9l9iCaxlYefnqXIG2H3bpoOo
         qNWLdoVGpdGeBeZYNA5z9aG1lJIqNm9FLDRouYAOODpZFyYpXhM4B2U+42clKbpWQIOg
         HDHVi8Zzldeig3HxNZxsq8NjmFyOWKGzi7jt7mxh4OhtxcVXYEnLCki4wvOQBIMHISbY
         GAKsTmrZgCvEfUJTIJYyOXj4Nxxoc9Q94q/O0q5GkXOWCiBd9VsmC25acLkarRSpBZWU
         y/wuY368iWYF2VvqBSVS4M32pQRzEVYcqMk2WO8boM9t9ITa+0i2HYmrInOs4Z5IX6lo
         6F3g==
X-Gm-Message-State: AOAM532BbYQJ25eZC6ebdX8sSyKL6/yKUGCX1bioPyAzgLANu417mIA5
        klKwD1vLN3gYyooB6NiFE7gDM7tV7qptXyPQCbQtY07vlg+MNa8dXeMwo8kpvJ/p58aUDxvwQ3B
        QCsvuPxI+sv9lAkpajqcGLKcX
X-Received: by 2002:a17:902:b615:b0:143:bbf0:aad0 with SMTP id b21-20020a170902b61500b00143bbf0aad0mr16671612pls.12.1636962976147;
        Sun, 14 Nov 2021 23:56:16 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwavIcr/CimjYOEqw/YJdq6Mqfcy+56LKYHxJhlNMkBwmXshfu7Q/OZQlTY3k8NhCo5kJhghQ==
X-Received: by 2002:a17:902:b615:b0:143:bbf0:aad0 with SMTP id b21-20020a170902b61500b00143bbf0aad0mr16671588pls.12.1636962975927;
        Sun, 14 Nov 2021 23:56:15 -0800 (PST)
Received: from localhost.localdomain ([191.101.132.223])
        by smtp.gmail.com with ESMTPSA id e10sm15792796pfv.140.2021.11.14.23.56.08
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 14 Nov 2021 23:56:15 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Alistair Popple <apopple@nvidia.com>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, peterx@redhat.com,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [PATCH v6 05/23] mm/shmem: Take care of UFFDIO_COPY_MODE_WP
Date:   Mon, 15 Nov 2021 15:55:04 +0800
Message-Id: <20211115075522.73795-6-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Pass wp_copy into shmem_mfill_atomic_pte() through the stack, then apply the
UFFD_WP bit properly when the UFFDIO_COPY on shmem is with UFFDIO_COPY_MODE_WP.
wp_copy lands mfill_atomic_install_pte() finally.

Note: we must do pte_wrprotect() if !writable in mfill_atomic_install_pte(), as
mk_pte() could return a writable pte (e.g., when VM_SHARED on a shmem file).

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/linux/shmem_fs.h |  4 ++--
 mm/shmem.c               |  4 ++--
 mm/userfaultfd.c         | 30 +++++++++++++++++++++---------
 3 files changed, 25 insertions(+), 13 deletions(-)

diff --git a/include/linux/shmem_fs.h b/include/linux/shmem_fs.h
index 166158b6e917..0ee0f437b14f 100644
--- a/include/linux/shmem_fs.h
+++ b/include/linux/shmem_fs.h
@@ -145,11 +145,11 @@ extern int shmem_mfill_atomic_pte(struct mm_struct *dst_mm, pmd_t *dst_pmd,
 				  struct vm_area_struct *dst_vma,
 				  unsigned long dst_addr,
 				  unsigned long src_addr,
-				  bool zeropage,
+				  bool zeropage, bool wp_copy,
 				  struct page **pagep);
 #else /* !CONFIG_SHMEM */
 #define shmem_mfill_atomic_pte(dst_mm, dst_pmd, dst_vma, dst_addr, \
-			       src_addr, zeropage, pagep)       ({ BUG(); 0; })
+			       src_addr, zeropage, wp_copy, pagep) ({ BUG(); 0; })
 #endif /* CONFIG_SHMEM */
 #endif /* CONFIG_USERFAULTFD */

diff --git a/mm/shmem.c b/mm/shmem.c
index dc038ce78700..167a46e6a1ff 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -2344,7 +2344,7 @@ int shmem_mfill_atomic_pte(struct mm_struct *dst_mm,
 			   struct vm_area_struct *dst_vma,
 			   unsigned long dst_addr,
 			   unsigned long src_addr,
-			   bool zeropage,
+			   bool zeropage, bool wp_copy,
 			   struct page **pagep)
 {
 	struct inode *inode = file_inode(dst_vma->vm_file);
@@ -2415,7 +2415,7 @@ int shmem_mfill_atomic_pte(struct mm_struct *dst_mm,
 		goto out_release;

 	ret = mfill_atomic_install_pte(dst_mm, dst_pmd, dst_vma, dst_addr,
-				       page, true, false);
+				       page, true, wp_copy);
 	if (ret)
 		goto out_delete_from_cache;

diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c
index ac6f036298cd..95e5a9ba3196 100644
--- a/mm/userfaultfd.c
+++ b/mm/userfaultfd.c
@@ -70,14 +70,22 @@ int mfill_atomic_install_pte(struct mm_struct *dst_mm, pmd_t *dst_pmd,

 	_dst_pte = mk_pte(page, dst_vma->vm_page_prot);
 	_dst_pte = pte_mkdirty(_dst_pte);
-	if (page_in_cache && !vm_shared)
+	/* Don't write if uffd-wp wr-protected */
+	if (wp_copy) {
+		_dst_pte = pte_mkuffd_wp(_dst_pte);
 		writable = false;
-	if (writable) {
-		if (wp_copy)
-			_dst_pte = pte_mkuffd_wp(_dst_pte);
-		else
-			_dst_pte = pte_mkwrite(_dst_pte);
 	}
+	/* Don't write if page cache privately mapped */
+	if (page_in_cache && !vm_shared)
+		writable = false;
+	if (writable)
+		_dst_pte = pte_mkwrite(_dst_pte);
+	else
+		/*
+		 * We need this to make sure write bit removed; as mk_pte()
+		 * could return a pte with write bit set.
+		 */
+		_dst_pte = pte_wrprotect(_dst_pte);

 	dst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);

@@ -92,7 +100,12 @@ int mfill_atomic_install_pte(struct mm_struct *dst_mm, pmd_t *dst_pmd,
 	}

 	ret = -EEXIST;
-	if (!pte_none(*dst_pte))
+	/*
+	 * We allow to overwrite a pte marker: consider when both MISSING|WP
+	 * registered, we firstly wr-protect a none pte which has no page cache
+	 * page backing it, then access the page.
+	 */
+	if (!pte_none_mostly(*dst_pte))
 		goto out_unlock;

 	if (page_in_cache)
@@ -467,11 +480,10 @@ static __always_inline ssize_t mfill_atomic_pte(struct mm_struct *dst_mm,
 			err = mfill_zeropage_pte(dst_mm, dst_pmd,
 						 dst_vma, dst_addr);
 	} else {
-		VM_WARN_ON_ONCE(wp_copy);
 		err = shmem_mfill_atomic_pte(dst_mm, dst_pmd, dst_vma,
 					     dst_addr, src_addr,
 					     mode != MCOPY_ATOMIC_NORMAL,
-					     page);
+					     wp_copy, page);
 	}

 	return err;
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id BFAA8C433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:00:54 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id A8E5763219
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:00:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236609AbhKOIDr (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:03:47 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:23797 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S229776AbhKOIDq (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:03:46 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963250;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=enD0wV1PzTMcVZir6gX0xAfGiKSstXRuvW8GppqUCLE=;
        b=Dkx4XmBvDT07MU+OF2hZO6fEPSdPiNvZuGgVSmOn6bGhRP37j9nER9I5GMMgqQe9U/Mifp
        u7SISyMgp9pMOda1nEJlV2ZWl4YlnjK0vLgLxUUIxn2rInwsr4NShEXx4ot+cQv7KY1/+N
        n0eIRC5daTM02LRuHZBeXsK1jFTmqK0=
Received: from mail-pg1-f197.google.com (mail-pg1-f197.google.com
 [209.85.215.197]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-136-Rr60l9JCPjyV8OQDY3dSsw-1; Mon, 15 Nov 2021 03:00:49 -0500
X-MC-Unique: Rr60l9JCPjyV8OQDY3dSsw-1
Received: by mail-pg1-f197.google.com with SMTP id s8-20020a63af48000000b002e6c10ac245so1401664pgo.21
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:00:49 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=enD0wV1PzTMcVZir6gX0xAfGiKSstXRuvW8GppqUCLE=;
        b=3Li6UbdMHrcZ6BGHJrmJKmTXUFPS4x0S4WGFQtte5GVjZaXykJGBSopCxg36KKzz1T
         LfnHRn3dpd5HMqlJXH0Y9loLBfWSGHVUDKaYW4Q0GeX9ohjZT0ni1w1pQZ8XGncYl1F6
         uRQtRl09RMOVDDNnF863pDFmlZXo03lbSTAoUhkvxAeNRrusK3YKlqL0c3aEuOMtuFiK
         N12AcShggdMBTQ4VlqvZwyr/vPanOZftig8dQaOCRCtBxqG3dN/sJNmx0VKSJimWsNri
         d9C3gxUgRvqczzi0GcPy07bi58RqnPawuxzkVkGmwmlEfrkxGhOSV8VDN2+nd1GzbAbp
         YOtg==
X-Gm-Message-State: AOAM533rL2D7Sk2VWpzAK8gxCVnFp+F6pP9fq2ustEQVDzNy0UER+UPA
        hOryU9KOAKnztm/rf+lPkTHZUgjHCMxAi27JijwdpIzkJZ72pBicq6BkkSYVItc/EAirBEAuJKW
        53edVx+QwrmO02cC966P9e9wz
X-Received: by 2002:a05:6a00:84c:b0:494:6d40:ed76 with SMTP id q12-20020a056a00084c00b004946d40ed76mr31113164pfk.65.1636963248187;
        Mon, 15 Nov 2021 00:00:48 -0800 (PST)
X-Google-Smtp-Source: ABdhPJxnNCoV7LVEKS2kGNyP+wQ8iY7s2Exa4jA+VMzI9tOHbNahQ0/af4BuXB848choZ6Ilf+Dr0g==
X-Received: by 2002:a05:6a00:84c:b0:494:6d40:ed76 with SMTP id q12-20020a056a00084c00b004946d40ed76mr31113125pfk.65.1636963247852;
        Mon, 15 Nov 2021 00:00:47 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id t40sm14468176pfg.107.2021.11.15.00.00.41
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:00:47 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 08/23] mm/shmem: Allow uffd wr-protect none pte for file-backed mem
Date:   Mon, 15 Nov 2021 16:00:34 +0800
Message-Id: <20211115080034.74526-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

File-backed memory differs from anonymous memory in that even if the pte is
missing, the data could still resides either in the file or in page/swap cache.
So when wr-protect a pte, we need to consider none ptes too.

We do that by installing the uffd-wp pte markers when necessary.  So when
there's a future write to the pte, the fault handler will go the special path
to first fault-in the page as read-only, then report to userfaultfd server with
the wr-protect message.

On the other hand, when unprotecting a page, it's also possible that the pte
got unmapped but replaced by the special uffd-wp marker.  Then we'll need to be
able to recover from a uffd-wp pte marker into a none pte, so that the next
access to the page will fault in correctly as usual when accessed the next
time.

Special care needs to be taken throughout the change_protection_range()
process.  Since now we allow user to wr-protect a none pte, we need to be able
to pre-populate the page table entries if we see (!anonymous && MM_CP_UFFD_WP)
requests, otherwise change_protection_range() will always skip when the pgtable
entry does not exist.

For example, the pgtable can be missing for a whole chunk of 2M pmd, but the
page cache can exist for the 2M range.  When we want to wr-protect one 4K page
within the 2M pmd range, we need to pre-populate the pgtable and install the
pte marker showing that we want to get a message and block the thread when the
page cache of that 4K page is written.  Without pre-populating the pmd,
change_protection() will simply skip that whole pmd.

Note that this patch only covers the small pages (pte level) but not covering
any of the transparent huge pages yet.  That will be done later, and this patch
will be a preparation for it too.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/mprotect.c | 63 ++++++++++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 62 insertions(+), 1 deletion(-)

diff --git a/mm/mprotect.c b/mm/mprotect.c
index 890bc1f9ca24..be837c4dbc64 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -29,6 +29,7 @@
 #include <linux/uaccess.h>
 #include <linux/mm_inline.h>
 #include <linux/pgtable.h>
+#include <linux/userfaultfd_k.h>
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
@@ -174,7 +175,16 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				if (pte_swp_uffd_wp(oldpte))
 					newpte = pte_swp_mkuffd_wp(newpte);
 			} else if (is_pte_marker_entry(entry)) {
-				/* Skip it, the same as none pte */
+				/*
+				 * If this is uffd-wp pte marker and we'd like
+				 * to unprotect it, drop it; the next page
+				 * fault will trigger without uffd trapping.
+				 */
+				if (uffd_wp_resolve &&
+				    (pte_marker_get(entry) & PTE_MARKER_UFFD_WP)) {
+					pte_clear(vma->vm_mm, addr, pte);
+					pages++;
+				}
 				continue;
 			} else {
 				newpte = oldpte;
@@ -189,6 +199,20 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				set_pte_at(vma->vm_mm, addr, pte, newpte);
 				pages++;
 			}
+		} else {
+			/* It must be an none page, or what else?.. */
+			WARN_ON_ONCE(!pte_none(oldpte));
+			if (unlikely(uffd_wp && !vma_is_anonymous(vma))) {
+				/*
+				 * For file-backed mem, we need to be able to
+				 * wr-protect a none pte, because even if the
+				 * pte is none, the page/swap cache could
+				 * exist.  Doing that by install a marker.
+				 */
+				set_pte_at(vma->vm_mm, addr, pte,
+					   make_pte_marker(PTE_MARKER_UFFD_WP));
+				pages++;
+			}
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();
@@ -222,6 +246,39 @@ static inline int pmd_none_or_clear_bad_unless_trans_huge(pmd_t *pmd)
 	return 0;
 }

+/* Return true if we're uffd wr-protecting file-backed memory, or false */
+static inline bool
+uffd_wp_protect_file(struct vm_area_struct *vma, unsigned long cp_flags)
+{
+	return (cp_flags & MM_CP_UFFD_WP) && !vma_is_anonymous(vma);
+}
+
+/*
+ * If wr-protecting the range for file-backed, populate pgtable for the case
+ * when pgtable is empty but page cache exists.  When {pte|pmd|...}_alloc()
+ * failed it means no memory, we don't have a better option but stop.
+ */
+#define  change_pmd_prepare(vma, pmd, cp_flags)				\
+	do {								\
+		if (unlikely(uffd_wp_protect_file(vma, cp_flags))) {	\
+			if (WARN_ON_ONCE(pte_alloc(vma->vm_mm, pmd)))	\
+				break;					\
+		}							\
+	} while (0)
+/*
+ * This is the general pud/p4d/pgd version of change_pmd_prepare(). We need to
+ * have separate change_pmd_prepare() because pte_alloc() returns 0 on success,
+ * while {pmd|pud|p4d}_alloc() returns the valid pointer on success.
+ */
+#define  change_prepare(vma, high, low, addr, cp_flags)			\
+	do {								\
+		if (unlikely(uffd_wp_protect_file(vma, cp_flags))) {	\
+			low##_t *p = low##_alloc(vma->vm_mm, high, addr); \
+			if (WARN_ON_ONCE(p == NULL))			\
+				break;					\
+		}							\
+	} while (0)
+
 static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 		pud_t *pud, unsigned long addr, unsigned long end,
 		pgprot_t newprot, unsigned long cp_flags)
@@ -240,6 +297,7 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,

 		next = pmd_addr_end(addr, end);

+		change_pmd_prepare(vma, pmd, cp_flags);
 		/*
 		 * Automatic NUMA balancing walks the tables with mmap_lock
 		 * held for read. It's possible a parallel update to occur
@@ -305,6 +363,7 @@ static inline unsigned long change_pud_range(struct vm_area_struct *vma,
 	pud = pud_offset(p4d, addr);
 	do {
 		next = pud_addr_end(addr, end);
+		change_prepare(vma, pud, pmd, addr, cp_flags);
 		if (pud_none_or_clear_bad(pud))
 			continue;
 		pages += change_pmd_range(vma, pud, addr, next, newprot,
@@ -325,6 +384,7 @@ static inline unsigned long change_p4d_range(struct vm_area_struct *vma,
 	p4d = p4d_offset(pgd, addr);
 	do {
 		next = p4d_addr_end(addr, end);
+		change_prepare(vma, p4d, pud, addr, cp_flags);
 		if (p4d_none_or_clear_bad(p4d))
 			continue;
 		pages += change_pud_range(vma, p4d, addr, next, newprot,
@@ -350,6 +410,7 @@ static unsigned long change_protection_range(struct vm_area_struct *vma,
 	inc_tlb_flush_pending(mm);
 	do {
 		next = pgd_addr_end(addr, end);
+		change_prepare(vma, pgd, p4d, addr, cp_flags);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
 		pages += change_p4d_range(vma, pgd, addr, next, newprot,
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 2EA3DC433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:01:52 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 16A2B60E8D
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:01:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S235183AbhKOIEo (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:04:44 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:60561 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236597AbhKOIEB (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:04:01 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963266;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=CT33CO1P4m9z3srlG1n5DsePRxe7FxchG1iBDIBzjEc=;
        b=C54FE1t8/2oxCNnbYNW6aX8lbrmXuY0H7pVAL1C21oP5NaquAkN0g32hv7PB1L9LWIoZd+
        lSlimUI/xDl0ALapM/vBo5E+6OkrPXBckLMzoVqk6CmMNdP5NZABf78rf/kKUoUB1LQv7l
        sJPrwjANHrtxMumWnPi2J+Ma95992nM=
Received: from mail-pf1-f198.google.com (mail-pf1-f198.google.com
 [209.85.210.198]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-228-mTHVQRXxONWDQIEBYFHjww-1; Mon, 15 Nov 2021 03:01:03 -0500
X-MC-Unique: mTHVQRXxONWDQIEBYFHjww-1
Received: by mail-pf1-f198.google.com with SMTP id c21-20020a62e815000000b004a29ebf0aa7so2627506pfi.2
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:01:03 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=CT33CO1P4m9z3srlG1n5DsePRxe7FxchG1iBDIBzjEc=;
        b=aK1fDP4i7qDV4q0DAx0jel5EMpzVjdf6fWEr8tC9jF7LJd74hYGtdd07h13ViswVRv
         1/uia7vLUftmzdXezK01tKAlJSsc2RsdR21laHe5AsrCEubNFDG+CTCm6SSt7cbpaL/w
         PAFEFLtBrqF7s/ZuYDgBPaTEokyyPbyGOhNEECpXaeGyX/nNksqH5P+IP0aj95v7sqPM
         17g7OG8hwTBePFw38QVLUXxTpoSlDNz+P/H6wI63XUzN2y7pf4XOoUsLMxWvQZq2xZqK
         826n6aWP3CslVA0OH79aAT4Kdp5tN/noLTQ4Q81jwgiQBpFCyWPZ5WN0B3tVEK//7Pxs
         2QVg==
X-Gm-Message-State: AOAM531AnkMOpdgk8YXlo2CtBqXAFbYUtYOOng3hI0+lU+ghCis+Z4uh
        0juf3Tfss82dAEktkCZ8hD7HX9ygP9qVc62ZunISq1s9jm/+aImbHklil3yyp8voSwuKrU//YoA
        68mkuP7MbetEvzK1Dw5zbFlQw
X-Received: by 2002:a17:90a:e60a:: with SMTP id j10mr63679958pjy.169.1636963262738;
        Mon, 15 Nov 2021 00:01:02 -0800 (PST)
X-Google-Smtp-Source: ABdhPJw5fbAzVfzXaTeCmw8VY664lWE42EUqztZ2N89jbHIauCly+ZiaTzVAzB2oLqZABMH3u9HTaw==
X-Received: by 2002:a17:90a:e60a:: with SMTP id j10mr63679926pjy.169.1636963262451;
        Mon, 15 Nov 2021 00:01:02 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id rj8sm2841393pjb.0.2021.11.15.00.00.54
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:01:01 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 09/23] mm/shmem: Allows file-back mem to be uffd wr-protected on thps
Date:   Mon, 15 Nov 2021 16:00:48 +0800
Message-Id: <20211115080048.74584-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

We don't have "huge" version of pte markers, instead when necessary we split
the thp.

However split the thp is not enough, because file-backed thp is handled totally
differently comparing to anonymous thps: rather than doing a real split, the
thp pmd will simply got cleared in __split_huge_pmd_locked().

That is not enough if e.g. when there is a thp covers range [0, 2M) but we want
to wr-protect small page resides in [4K, 8K) range, because after
__split_huge_pmd() returns, there will be a none pmd, and change_pmd_range()
will just skip it right after the split.

Here we leverage the previously introduced change_pmd_prepare() macro so that
we'll populate the pmd with a pgtable page after the pmd split (in which
process the pmd will be cleared for cases like shmem).  Then change_pte_range()
will do all the rest for us by installing the uffd-wp pte marker at any none
pte that we'd like to wr-protect.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/mprotect.c | 9 ++++++++-
 1 file changed, 8 insertions(+), 1 deletion(-)

diff --git a/mm/mprotect.c b/mm/mprotect.c
index be837c4dbc64..0d4bf755cee8 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -319,8 +319,15 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 		}

 		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
-			if (next - addr != HPAGE_PMD_SIZE) {
+			if ((next - addr != HPAGE_PMD_SIZE) ||
+			    uffd_wp_protect_file(vma, cp_flags)) {
 				__split_huge_pmd(vma, pmd, addr, false, NULL);
+				/*
+				 * For file-backed, the pmd could have been
+				 * cleared; make sure pmd populated if
+				 * necessary, then fall-through to pte level.
+				 */
+				change_pmd_prepare(vma, pmd, cp_flags);
 			} else {
 				int nr_ptes = change_huge_pmd(vma, pmd, addr,
 							      newprot, cp_flags);
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 94D0CC433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:02:57 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 7B7BE63219
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:02:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S237318AbhKOIFR (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:05:17 -0500
Received: from us-smtp-delivery-124.mimecast.com ([216.205.24.124]:43618 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S234884AbhKOIEo (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:04:44 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963309;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=3JN/qHWTDj3qXyIKJcMZC7BazoBUZifDcHVBgdG1ZHs=;
        b=fFT1/TiTKIKN5R0Uh1RT28Y4G2M35wca/BVV6V2KbgOKsyKBw+tmykwnK6WamJFy+EiUvx
        NW99+MG08ZERJCZYyfSTjlMhFJ6DH4Rpy22IXMbIamEwczHH5m31YNPJDevg2OsS5aJKyc
        bP6XvHz5YPT6po0CCtCyZrUoJfZrb5g=
Received: from mail-pj1-f72.google.com (mail-pj1-f72.google.com
 [209.85.216.72]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-585-vxrWgQMmM9CxwYW0E1lx_g-1; Mon, 15 Nov 2021 03:01:47 -0500
X-MC-Unique: vxrWgQMmM9CxwYW0E1lx_g-1
Received: by mail-pj1-f72.google.com with SMTP id r23-20020a17090a941700b001a74be6cf80so4879480pjo.2
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:01:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=3JN/qHWTDj3qXyIKJcMZC7BazoBUZifDcHVBgdG1ZHs=;
        b=LiDn6ChbQA1JUf++G4v5enmRhmeVi6HZocMxNpl1pi1r/j/d+ZD0aTRCIZB4Hk3F2J
         umYlx4SThj34G8dStY/96ogglZhmPmTIiVfZkWDBhx5W9/Rd4BphFblYLymAzNkZG2hD
         1ufYy7DEm2wLaRPxLpBJjip9SlOUHmAFiTMpSxQKdHCvoiLHaV+TP3BT/9J0pokbQNwu
         jOyoN+DHgvr640eKuiQkRP0VEdWqwsbZzwrxYlZ8ZLNuYONHQjDpKXKbe6u5yqhtb5il
         5sJypH5G2JV/FhIlFqp6XC0JF3V34RaFyyaqqKF4oWZD1mrWuAHn5d16h6WN68un99R1
         jfJQ==
X-Gm-Message-State: AOAM531UddGapqfzQ2lsbuZJm3/WE4yUkoG2TmW5A/cnzj/MDiaVoIne
        HebmD+TpOZGT9NbEPqgEhupKX+6TamOZoOn4Pe2nUCb1ZJALoPC0AH+bCikRPAbMX+AVNTs5JvR
        dwEcwHpiJuOIhSjFgLFbFAsok
X-Received: by 2002:a65:4c01:: with SMTP id u1mr23373802pgq.151.1636963306583;
        Mon, 15 Nov 2021 00:01:46 -0800 (PST)
X-Google-Smtp-Source: ABdhPJxAqokYq6ooU4nDXInwG5sxx6pe6ho32Uv0rSbhETu3lnvghB2QdRC5YvCrwwuikrQwoQlTpg==
X-Received: by 2002:a65:4c01:: with SMTP id u1mr23373775pgq.151.1636963306345;
        Mon, 15 Nov 2021 00:01:46 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id f130sm14450402pfa.81.2021.11.15.00.01.38
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:01:45 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 12/23] mm/hugetlb: Hook page faults for uffd write protection
Date:   Mon, 15 Nov 2021 16:01:32 +0800
Message-Id: <20211115080132.74754-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Hook up hugetlbfs_fault() with the capability to handle userfaultfd-wp faults.

We do this slightly earlier than hugetlb_cow() so that we can avoid taking some
extra locks that we definitely don't need.

Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/hugetlb.c | 19 +++++++++++++++++++
 1 file changed, 19 insertions(+)

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index e09159c957e3..3a10274b2e39 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -5658,6 +5658,25 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	if (unlikely(!pte_same(entry, huge_ptep_get(ptep))))
 		goto out_ptl;

+	/* Handle userfault-wp first, before trying to lock more pages */
+	if (userfaultfd_wp(vma) && huge_pte_uffd_wp(huge_ptep_get(ptep)) &&
+	    (flags & FAULT_FLAG_WRITE) && !huge_pte_write(entry)) {
+		struct vm_fault vmf = {
+			.vma = vma,
+			.address = haddr,
+			.flags = flags,
+		};
+
+		spin_unlock(ptl);
+		if (pagecache_page) {
+			unlock_page(pagecache_page);
+			put_page(pagecache_page);
+		}
+		mutex_unlock(&hugetlb_fault_mutex_table[hash]);
+		i_mmap_unlock_read(mapping);
+		return handle_userfault(&vmf, VM_UFFD_WP);
+	}
+
 	/*
 	 * hugetlb_cow() requires page locks of pte_page(entry) and
 	 * pagecache_page, so here we need take the former one
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 6218DC433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:03:22 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 4871361B97
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:03:22 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229721AbhKOIGP (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:06:15 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:58257 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S229944AbhKOIE3 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:04:29 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963294;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=Jyv1lzZ4mkTvzdL9kLWgnj86vlKE/ygyLYqVdoUf6Sw=;
        b=b3y9dmhjqk2gxziTiJD9KT6UfcyFQvyPK4A12LrlvSYbcYW30xmoFYp89QPq+k+HztCiRJ
        5D88+LHt4WLSTHTSXP8PLAyHNWnPdn5FRJGOddYD0YsRPfewtOh/mllmEMhiWDKC1XYg7G
        pqqRIjKRUxFaYVpOl2m9YP4ipEaYdik=
Received: from mail-pl1-f200.google.com (mail-pl1-f200.google.com
 [209.85.214.200]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-504-xhiONKP8Oc-zDM9T-3uyVQ-1; Mon, 15 Nov 2021 03:01:33 -0500
X-MC-Unique: xhiONKP8Oc-zDM9T-3uyVQ-1
Received: by mail-pl1-f200.google.com with SMTP id e4-20020a170902b78400b00143c2e300ddso519354pls.17
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:01:33 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=Jyv1lzZ4mkTvzdL9kLWgnj86vlKE/ygyLYqVdoUf6Sw=;
        b=raKV3oTRZD4Ms+3eUIIsyc+5DQW65D6ymQi4ZTjDe+NR3+PAbCXwDK0NicWuDe33/P
         Yw8xz7mt9d/RFgQChGb9KWn6dOu+YNP35I1QZC7md2lT/3FSBkaqLuMUCpstNvPCzoxj
         UVPhjQNaQsdiDyV8UsOGQtaYUZcDWRGpaAJfxn706KvKLDWahIL4hAVfOA73Y8TFZITV
         Yz1Bxuv4LXCxj31ue8mtyiaT0tqxY1ucWKqANjVywWh1L1Uv0aHT8B3D8chntWVD2fzp
         Bw1wa2Vh7bT/FOVxpcwjXo2zUE4cn52i73U0pC7Zde3LDrNvRNBpNDw362G60BPEdjuL
         i2YA==
X-Gm-Message-State: AOAM531HgLjk7yqWGRNvma8wOc4ep0zl5MN44eXPA4qpz3uwov7XUmN7
        TdGsCKUJcxB8b2CTlz3/WA04H2H3evdYsg3p0nQaAjAh2NIYKwaoi8ZUSA+Zrr68gpP7YkcNKLp
        GvGhOlSjLp6Ln2nlpC4zsumEX
X-Received: by 2002:a17:902:e294:b0:143:86a8:c56d with SMTP id o20-20020a170902e29400b0014386a8c56dmr33185946plc.22.1636963292122;
        Mon, 15 Nov 2021 00:01:32 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwMIv+3cF09VX35C/GmVeDIlebJd1GQRnqQzuF2YhF3HKAjwkxGT6acz2QFRWDpRQuvSoQbMw==
X-Received: by 2002:a17:902:e294:b0:143:86a8:c56d with SMTP id o20-20020a170902e29400b0014386a8c56dmr33185912plc.22.1636963291897;
        Mon, 15 Nov 2021 00:01:31 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id p20sm14708877pfw.96.2021.11.15.00.01.23
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:01:31 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 11/23] mm/hugetlb: Introduce huge pte version of uffd-wp helpers
Date:   Mon, 15 Nov 2021 16:01:17 +0800
Message-Id: <20211115080117.74699-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

They will be used in the follow up patches to either check/set/clear uffd-wp
bit of a huge pte.

So far it reuses all the small pte helpers.  Archs can overwrite these versions
when necessary (with __HAVE_ARCH_HUGE_PTE_UFFD_WP* macros) in the future.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 arch/s390/include/asm/hugetlb.h | 15 +++++++++++++++
 include/asm-generic/hugetlb.h   | 15 +++++++++++++++
 2 files changed, 30 insertions(+)

diff --git a/arch/s390/include/asm/hugetlb.h b/arch/s390/include/asm/hugetlb.h
index 60f9241e5e4a..19c4b4431d27 100644
--- a/arch/s390/include/asm/hugetlb.h
+++ b/arch/s390/include/asm/hugetlb.h
@@ -115,6 +115,21 @@ static inline pte_t huge_pte_modify(pte_t pte, pgprot_t newprot)
 	return pte_modify(pte, newprot);
 }

+static inline pte_t huge_pte_mkuffd_wp(pte_t pte)
+{
+	return pte;
+}
+
+static inline pte_t huge_pte_clear_uffd_wp(pte_t pte)
+{
+	return pte;
+}
+
+static inline int huge_pte_uffd_wp(pte_t pte)
+{
+	return 0;
+}
+
 static inline bool gigantic_page_runtime_supported(void)
 {
 	return true;
diff --git a/include/asm-generic/hugetlb.h b/include/asm-generic/hugetlb.h
index f39cad20ffc6..896f341f614d 100644
--- a/include/asm-generic/hugetlb.h
+++ b/include/asm-generic/hugetlb.h
@@ -35,6 +35,21 @@ static inline pte_t huge_pte_modify(pte_t pte, pgprot_t newprot)
 	return pte_modify(pte, newprot);
 }

+static inline pte_t huge_pte_mkuffd_wp(pte_t pte)
+{
+	return pte_mkuffd_wp(pte);
+}
+
+static inline pte_t huge_pte_clear_uffd_wp(pte_t pte)
+{
+	return pte_clear_uffd_wp(pte);
+}
+
+static inline int huge_pte_uffd_wp(pte_t pte)
+{
+	return pte_uffd_wp(pte);
+}
+
 #ifndef __HAVE_ARCH_HUGE_PTE_CLEAR
 static inline void huge_pte_clear(struct mm_struct *mm, unsigned long addr,
 		    pte_t *ptep, unsigned long sz)
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 7C104C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:03:41 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 5F3C263219
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:03:41 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S230354AbhKOIGZ (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:06:25 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:33187 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236690AbhKOIFA (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:05:00 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963324;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=6hmTwTIZrb3ZAEbduDIRoqEEL0oiwJCe8ujMVDm5FPs=;
        b=R893HvKdbkHUhXEm2SuNZCth5hJcYHKKveZGFzmH2hzLP+SbQPgFs0L9dtyolRTq5Wk9Ts
        FpCeGCDznb+40tURPDNnI5sOGfmzJvft+r4C1dYTKbuj/WShVQp/sZ3nyZSRxfcgOKsFak
        biXJlMjfBKPPyWs/E5z+XVJqeWwg9PI=
Received: from mail-pf1-f200.google.com (mail-pf1-f200.google.com
 [209.85.210.200]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-338-qQee54baNmm8FT35soGviQ-1; Mon, 15 Nov 2021 03:02:03 -0500
X-MC-Unique: qQee54baNmm8FT35soGviQ-1
Received: by mail-pf1-f200.google.com with SMTP id x14-20020a627c0e000000b0049473df362dso9585209pfc.12
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:02:03 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=6hmTwTIZrb3ZAEbduDIRoqEEL0oiwJCe8ujMVDm5FPs=;
        b=j2quM+qsZk9rnGcxw/B4kl1g8ZXOCuAxqzKewD++lmyZynx2Kus9CdpOxlBN+XE0Zn
         Vnp8WfWhbn5dfR1fIcTuz/2dqiVNmDXb5CgqUQAxwzV6LWRvQVljs9S1pyJ1TmB5E3cF
         fJz6PDZ9zB9a/IkBu539W7WbagdM5AFtKxTCKh+AeevbsaZrU4+YjzqZzGAB84RSrt4m
         W5tU3XmNfj65h95/24pZk0VQyQjuo3CmdCtFozelwcOlx9XuT1tQZk9fDExPNFJk6l2p
         hALRMHXocx8GTMdO9rDV4GEQsslEjKW7jGsHoQhKGhMcMHwrQ29x7RBCYBExWCg/4lBK
         5w0A==
X-Gm-Message-State: AOAM530QwTQ2Dn9twsvN81WvsjATWuhhDRUw3y/QRkfKhatIN6GBHXqE
        KyCdr3gZeNyIh2EcxcuL5JXhjL5AwD15bJemOCoyDUm1Jup/UkGKS+4sYRFfrjFgCqIJHvzM0o8
        D5hLvHJlo/Le3n1uahtJlRBBd
X-Received: by 2002:a17:90a:4b47:: with SMTP id o7mr25848426pjl.92.1636963320391;
        Mon, 15 Nov 2021 00:02:00 -0800 (PST)
X-Google-Smtp-Source: ABdhPJzlWxTaC0v+r/YuGJSh03eszu/Jv/h92Vt+54eS7+7+RnqDDaawpklrXtrShKxo0IbKQ4Ql0g==
X-Received: by 2002:a17:90a:4b47:: with SMTP id o7mr25848392pjl.92.1636963320125;
        Mon, 15 Nov 2021 00:02:00 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id f21sm9904786pfe.69.2021.11.15.00.01.51
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:01:59 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 13/23] mm/hugetlb: Take care of UFFDIO_COPY_MODE_WP
Date:   Mon, 15 Nov 2021 16:01:46 +0800
Message-Id: <20211115080146.74812-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Pass the wp_copy variable into hugetlb_mcopy_atomic_pte() thoughout the stack.
Apply the UFFD_WP bit if UFFDIO_COPY_MODE_WP is with UFFDIO_COPY.

Hugetlb pages are only managed by hugetlbfs, so we're safe even without setting
dirty bit in the huge pte if the page is installed as read-only.  However we'd
better still keep the dirty bit set for a read-only UFFDIO_COPY pte (when
UFFDIO_COPY_MODE_WP bit is set), not only to match what we do with shmem, but
also because the page does contain dirty data that the kernel just copied from
the userspace.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/linux/hugetlb.h |  6 ++++--
 mm/hugetlb.c            | 29 +++++++++++++++++++++++------
 mm/userfaultfd.c        | 14 +++++++++-----
 3 files changed, 36 insertions(+), 13 deletions(-)

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 00351ccb49a3..4da0c4b4159a 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -160,7 +160,8 @@ int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm, pte_t *dst_pte,
 				unsigned long dst_addr,
 				unsigned long src_addr,
 				enum mcopy_atomic_mode mode,
-				struct page **pagep);
+				struct page **pagep,
+				bool wp_copy);
 #endif /* CONFIG_USERFAULTFD */
 bool hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
@@ -355,7 +356,8 @@ static inline int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,
 						unsigned long dst_addr,
 						unsigned long src_addr,
 						enum mcopy_atomic_mode mode,
-						struct page **pagep)
+						struct page **pagep,
+						bool wp_copy)
 {
 	BUG();
 	return 0;
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 3a10274b2e39..8146240eefc6 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -5740,7 +5740,8 @@ int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,
 			    unsigned long dst_addr,
 			    unsigned long src_addr,
 			    enum mcopy_atomic_mode mode,
-			    struct page **pagep)
+			    struct page **pagep,
+			    bool wp_copy)
 {
 	bool is_continue = (mode == MCOPY_ATOMIC_CONTINUE);
 	struct hstate *h = hstate_vma(dst_vma);
@@ -5868,7 +5869,12 @@ int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,
 		goto out_release_unlock;

 	ret = -EEXIST;
-	if (!huge_pte_none(huge_ptep_get(dst_pte)))
+	/*
+	 * We allow to overwrite a pte marker: consider when both MISSING|WP
+	 * registered, we firstly wr-protect a none pte which has no page cache
+	 * page backing it, then access the page.
+	 */
+	if (!huge_pte_none_mostly(huge_ptep_get(dst_pte)))
 		goto out_release_unlock;

 	if (vm_shared) {
@@ -5878,17 +5884,28 @@ int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,
 		hugepage_add_new_anon_rmap(page, dst_vma, dst_addr);
 	}

-	/* For CONTINUE on a non-shared VMA, don't set VM_WRITE for CoW. */
-	if (is_continue && !vm_shared)
+	/*
+	 * For either: (1) CONTINUE on a non-shared VMA, or (2) UFFDIO_COPY
+	 * with wp flag set, don't set pte write bit.
+	 */
+	if (wp_copy || (is_continue && !vm_shared))
 		writable = 0;
 	else
 		writable = dst_vma->vm_flags & VM_WRITE;

 	_dst_pte = make_huge_pte(dst_vma, page, writable);
-	if (writable)
-		_dst_pte = huge_pte_mkdirty(_dst_pte);
+	/*
+	 * Always mark UFFDIO_COPY page dirty; note that this may not be
+	 * extremely important for hugetlbfs for now since swapping is not
+	 * supported, but we should still be clear in that this page cannot be
+	 * thrown away at will, even if write bit not set.
+	 */
+	_dst_pte = huge_pte_mkdirty(_dst_pte);
 	_dst_pte = pte_mkyoung(_dst_pte);

+	if (wp_copy)
+		_dst_pte = huge_pte_mkuffd_wp(_dst_pte);
+
 	set_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);

 	(void)huge_ptep_set_access_flags(dst_vma, dst_addr, dst_pte, _dst_pte,
diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c
index 95e5a9ba3196..6174a212c72f 100644
--- a/mm/userfaultfd.c
+++ b/mm/userfaultfd.c
@@ -291,7 +291,8 @@ static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,
 					      unsigned long dst_start,
 					      unsigned long src_start,
 					      unsigned long len,
-					      enum mcopy_atomic_mode mode)
+					      enum mcopy_atomic_mode mode,
+					      bool wp_copy)
 {
 	int vm_shared = dst_vma->vm_flags & VM_SHARED;
 	ssize_t err;
@@ -379,7 +380,7 @@ static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,
 		}

 		if (mode != MCOPY_ATOMIC_CONTINUE &&
-		    !huge_pte_none(huge_ptep_get(dst_pte))) {
+		    !huge_pte_none_mostly(huge_ptep_get(dst_pte))) {
 			err = -EEXIST;
 			mutex_unlock(&hugetlb_fault_mutex_table[hash]);
 			i_mmap_unlock_read(mapping);
@@ -387,7 +388,8 @@ static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,
 		}

 		err = hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma,
-					       dst_addr, src_addr, mode, &page);
+					       dst_addr, src_addr, mode, &page,
+					       wp_copy);

 		mutex_unlock(&hugetlb_fault_mutex_table[hash]);
 		i_mmap_unlock_read(mapping);
@@ -442,7 +444,8 @@ extern ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,
 				      unsigned long dst_start,
 				      unsigned long src_start,
 				      unsigned long len,
-				      enum mcopy_atomic_mode mode);
+				      enum mcopy_atomic_mode mode,
+				      bool wp_copy);
 #endif /* CONFIG_HUGETLB_PAGE */

 static __always_inline ssize_t mfill_atomic_pte(struct mm_struct *dst_mm,
@@ -562,7 +565,8 @@ static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,
 	 */
 	if (is_vm_hugetlb_page(dst_vma))
 		return  __mcopy_atomic_hugetlb(dst_mm, dst_vma, dst_start,
-						src_start, len, mcopy_mode);
+					       src_start, len, mcopy_mode,
+					       wp_copy);

 	if (!vma_is_anonymous(dst_vma) && !vma_is_shmem(dst_vma))
 		goto out_unlock;
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 53E3AC433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:05 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 3E03C61B97
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:05 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236700AbhKOIG6 (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:06:58 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:56153 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S235183AbhKOIFM (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:05:12 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963336;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=/Iv5CypmvIVE6oMVapm0QBWFP5IxCJXl/Bg1UzUVmko=;
        b=Azrr1UycFvJ406I8VXr/O+bI1sY9o6eICwX8VsEY1KUoODm1xR78J+QO8/2O3Nvd8x0CIu
        zG39oMSKcjEYLWHLgOlLCQzr7tT10PBMaeJ65zugQJ7BBvsUoOOKhaB0zmUSufUi9C2Fvg
        C/uKQYOU+KLwjFNrvWPnutNgi3ECArk=
Received: from mail-pl1-f200.google.com (mail-pl1-f200.google.com
 [209.85.214.200]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-390-IR2xB2G3M2uhiltCH_eZGQ-1; Mon, 15 Nov 2021 03:02:15 -0500
X-MC-Unique: IR2xB2G3M2uhiltCH_eZGQ-1
Received: by mail-pl1-f200.google.com with SMTP id z3-20020a170903018300b0014224dca4a1so5887410plg.0
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:02:15 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=/Iv5CypmvIVE6oMVapm0QBWFP5IxCJXl/Bg1UzUVmko=;
        b=IuQrqUW//tyAN0iODEIt/eFZLx3G9lqIBYVcXruW0s5lg252+fIaqnvqRLo3tvsla3
         wv+ALqLspQODt6B0Yj879zR8ZSsNx+VyIG2pky2p4VE7mPbiIASLgXf+0D4njLE7u14k
         V+FzJv8ZwvyKkbAx3ezl4r4s37JburyQc8VNmYD44fyGx8TCiDfn2O1zgwkpn2AgzSmS
         ++f7mIlFTgI2LJClZx51CalOOWGznkRQXAiV1EZdyyQYye9bnDOGjL/f1oQlJyY3fWFo
         remX2OJwirTg8EgWsnPFBR+x3UsAKT3GLPCYu9GZT776dTcx0EZji+yz/fdrY8+NF/C5
         N51w==
X-Gm-Message-State: AOAM53194cnf3aINrgF6e34nScReb5VBflKG2D6DLYeLyq3ltyhr+NoR
        sxCNwmhuZ42JWHRe6aKsQIcY3u/cdZZsZ7r8wrnySHelIf/1+xPmoGbZqUpDH0x4RIzfjWpz7x9
        LayXeJd1hrr/rQJ4Pa/qppWIK
X-Received: by 2002:a17:90b:38c5:: with SMTP id nn5mr61485311pjb.220.1636963334076;
        Mon, 15 Nov 2021 00:02:14 -0800 (PST)
X-Google-Smtp-Source: ABdhPJyNSkZ/vww+jPhvco2wNnlsy70sP9i0ZGZit2163wehM7xneRAKVX66NU/eBtol2Mp8mRmPcQ==
X-Received: by 2002:a17:90b:38c5:: with SMTP id nn5mr61485283pjb.220.1636963333787;
        Mon, 15 Nov 2021 00:02:13 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id h25sm10459878pgm.33.2021.11.15.00.02.06
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:02:13 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 14/23] mm/hugetlb: Handle UFFDIO_WRITEPROTECT
Date:   Mon, 15 Nov 2021 16:02:00 +0800
Message-Id: <20211115080200.74866-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This starts from passing cp_flags into hugetlb_change_protection() so hugetlb
will be able to handle MM_CP_UFFD_WP[_RESOLVE] requests.

huge_pte_clear_uffd_wp() is introduced to handle the case where the
UFFDIO_WRITEPROTECT is requested upon migrating huge page entries.

Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/linux/hugetlb.h |  6 ++++--
 mm/hugetlb.c            | 13 ++++++++++++-
 mm/mprotect.c           |  3 ++-
 mm/userfaultfd.c        |  8 ++++++++
 4 files changed, 26 insertions(+), 4 deletions(-)

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 4da0c4b4159a..a46011510e49 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -210,7 +210,8 @@ struct page *follow_huge_pgd(struct mm_struct *mm, unsigned long address,
 int pmd_huge(pmd_t pmd);
 int pud_huge(pud_t pud);
 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
-		unsigned long address, unsigned long end, pgprot_t newprot);
+		unsigned long address, unsigned long end, pgprot_t newprot,
+		unsigned long cp_flags);

 bool is_hugetlb_entry_migration(pte_t pte);
 void hugetlb_unshare_all_pmds(struct vm_area_struct *vma);
@@ -391,7 +392,8 @@ static inline void move_hugetlb_state(struct page *oldpage,

 static inline unsigned long hugetlb_change_protection(
 			struct vm_area_struct *vma, unsigned long address,
-			unsigned long end, pgprot_t newprot)
+			unsigned long end, pgprot_t newprot,
+			unsigned long cp_flags)
 {
 	return 0;
 }
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 8146240eefc6..7fc213c0ebf8 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6127,7 +6127,8 @@ long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 }

 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
-		unsigned long address, unsigned long end, pgprot_t newprot)
+		unsigned long address, unsigned long end,
+		pgprot_t newprot, unsigned long cp_flags)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long start = address;
@@ -6137,6 +6138,8 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 	unsigned long pages = 0;
 	bool shared_pmd = false;
 	struct mmu_notifier_range range;
+	bool uffd_wp = cp_flags & MM_CP_UFFD_WP;
+	bool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;

 	/*
 	 * In the case of shared PMDs, the area to flush could be beyond
@@ -6178,6 +6181,10 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 				entry = make_readable_migration_entry(
 							swp_offset(entry));
 				newpte = swp_entry_to_pte(entry);
+				if (uffd_wp)
+					newpte = pte_swp_mkuffd_wp(newpte);
+				else if (uffd_wp_resolve)
+					newpte = pte_swp_clear_uffd_wp(newpte);
 				set_huge_swap_pte_at(mm, address, ptep,
 						     newpte, huge_page_size(h));
 				pages++;
@@ -6192,6 +6199,10 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 			old_pte = huge_ptep_modify_prot_start(vma, address, ptep);
 			pte = pte_mkhuge(huge_pte_modify(old_pte, newprot));
 			pte = arch_make_huge_pte(pte, shift, vma->vm_flags);
+			if (uffd_wp)
+				pte = huge_pte_mkuffd_wp(huge_pte_wrprotect(pte));
+			else if (uffd_wp_resolve)
+				pte = huge_pte_clear_uffd_wp(pte);
 			huge_ptep_modify_prot_commit(vma, address, ptep, old_pte, pte);
 			pages++;
 		}
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 0d4bf755cee8..1cc4a6d1886b 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -441,7 +441,8 @@ unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 	BUG_ON((cp_flags & MM_CP_UFFD_WP_ALL) == MM_CP_UFFD_WP_ALL);

 	if (is_vm_hugetlb_page(vma))
-		pages = hugetlb_change_protection(vma, start, end, newprot);
+		pages = hugetlb_change_protection(vma, start, end, newprot,
+						  cp_flags);
 	else
 		pages = change_protection_range(vma, start, end, newprot,
 						cp_flags);
diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c
index 6174a212c72f..037f82719e64 100644
--- a/mm/userfaultfd.c
+++ b/mm/userfaultfd.c
@@ -690,6 +690,7 @@ int mwriteprotect_range(struct mm_struct *dst_mm, unsigned long start,
 			atomic_t *mmap_changing)
 {
 	struct vm_area_struct *dst_vma;
+	unsigned long page_mask;
 	pgprot_t newprot;
 	int err;

@@ -726,6 +727,13 @@ int mwriteprotect_range(struct mm_struct *dst_mm, unsigned long start,
 	if (!vma_is_anonymous(dst_vma))
 		goto out_unlock;

+	if (is_vm_hugetlb_page(dst_vma)) {
+		err = -EINVAL;
+		page_mask = vma_kernel_pagesize(dst_vma) - 1;
+		if ((start & page_mask) || (len & page_mask))
+			goto out_unlock;
+	}
+
 	if (enable_wp)
 		newprot = vm_get_page_prot(dst_vma->vm_flags & ~(VM_WRITE));
 	else
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 1AB66C433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:17 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id F0C1B63218
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236509AbhKOIHJ (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:07:09 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:33779 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236367AbhKOIF0 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:05:26 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963350;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=juo8CnrknUVx2CU+cPYbVJwKflQnt9kXKdPVkj+rnzk=;
        b=i8O5T2F4VU79dV2GwNApT6jVjlKJdsnQqi6/NzmwBkOCOn2RqxI5nInEhsfapPSDeI7VlH
        JGD+GQlA5iBgmu0Qq9IPURIlr/nqoQ+3NaFJsMeqtpAul/WJc+pbb2rNe4ByeHMlZlrj8v
        /GZvFZV0dQuDeTeD7nvTu5vxO8ny6YU=
Received: from mail-pj1-f70.google.com (mail-pj1-f70.google.com
 [209.85.216.70]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-274-XoMOd6wUM_6KPQ8LO0Ghwg-1; Mon, 15 Nov 2021 03:02:29 -0500
X-MC-Unique: XoMOd6wUM_6KPQ8LO0Ghwg-1
Received: by mail-pj1-f70.google.com with SMTP id r23-20020a17090a941700b001a74be6cf80so4880531pjo.2
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:02:28 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=juo8CnrknUVx2CU+cPYbVJwKflQnt9kXKdPVkj+rnzk=;
        b=r+op1PbMxHyVfTY8gkovU0Je4z6clpx6rpBI8QUSU22oSEBwbs1as6n64UDWBaBbEY
         blMSbyIrJq3NENgG/K5TzMDqOKPgzH2AwVSGWte9ZgHSgjJGvi4aVT5tmGQiHHAjLQcG
         rSA7HRZrggIMDhpH7bBWdU5RO2AqcDw9tojOSvzgSOByl1TISIluh3sfvpterc7BzP9f
         XnZ7jvjpLrjKqLqhI0VX63keVHsFkMB8fojGcAExpMY3guN2YnS9xOJT4LoJdjTStOCZ
         Uo9Z1Y1cjeLu2ekpXe9mWVhGBO1UNheqXX0juJQicgj9TDmLOIQa6fOha/6ONlyEWqxv
         cPYQ==
X-Gm-Message-State: AOAM531jzxPfWQCnHY2M0f0NcxILUZU29cXNJ3bYBH0elYZrY42tGOuL
        IgQ4PWjLtHP6YF6xV6U8zEIiTZuXT3eyOwVzTSEI/Bz90358/tUWjIaZZ8qSJT5v3n1KBPR91x/
        AQL1XDVKhVP3yizp2MSnKOYZC
X-Received: by 2002:a17:902:e804:b0:142:1c0b:c2a6 with SMTP id u4-20020a170902e80400b001421c0bc2a6mr32648871plg.23.1636963347860;
        Mon, 15 Nov 2021 00:02:27 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwDNuRUr56R2HgkYgfQqjH6+BcRz0mtYQUWUhFx5NvN9CbgJnvkwVUYkq/DiyMnk9FcFK3PfQ==
X-Received: by 2002:a17:902:e804:b0:142:1c0b:c2a6 with SMTP id u4-20020a170902e80400b001421c0bc2a6mr32648832plg.23.1636963347598;
        Mon, 15 Nov 2021 00:02:27 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id b4sm14912250pfl.60.2021.11.15.00.02.19
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:02:27 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 15/23] mm/hugetlb: Handle pte markers in page faults
Date:   Mon, 15 Nov 2021 16:02:14 +0800
Message-Id: <20211115080214.74926-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Allow hugetlb code to handle pte markers just like none ptes.  It's mostly
there, we just need to make sure we don't assume hugetlb_no_page() only handles
none pte, so when detecting pte change we should use pte_same() rather than
pte_none().  We need to pass in the old_pte to do the comparison.

Check the original pte to see whether it's a pte marker, if it is, we should
recover uffd-wp bit on the new pte to be installed, so that the next write will
be trapped by uffd.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/hugetlb.c | 18 ++++++++++++++----
 1 file changed, 14 insertions(+), 4 deletions(-)

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 7fc213c0ebf8..e8d01277af0f 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -5361,7 +5361,8 @@ static inline vm_fault_t hugetlb_handle_userfault(struct vm_area_struct *vma,
 static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 			struct vm_area_struct *vma,
 			struct address_space *mapping, pgoff_t idx,
-			unsigned long address, pte_t *ptep, unsigned int flags)
+			unsigned long address, pte_t *ptep,
+			pte_t old_pte, unsigned int flags)
 {
 	struct hstate *h = hstate_vma(vma);
 	vm_fault_t ret = VM_FAULT_SIGBUS;
@@ -5487,7 +5488,8 @@ static vm_fault_t hugetlb_no_page(struct mm_struct *mm,

 	ptl = huge_pte_lock(h, mm, ptep);
 	ret = 0;
-	if (!huge_pte_none(huge_ptep_get(ptep)))
+	/* If pte changed from under us, retry */
+	if (!pte_same(huge_ptep_get(ptep), old_pte))
 		goto backout;

 	if (anon_rmap) {
@@ -5497,6 +5499,12 @@ static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 		page_dup_rmap(page, true);
 	new_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)
 				&& (vma->vm_flags & VM_SHARED)));
+	/*
+	 * If this pte was previously wr-protected, keep it wr-protected even
+	 * if populated.
+	 */
+	if (unlikely(is_pte_marker_uffd_wp(old_pte)))
+		new_pte = huge_pte_wrprotect(huge_pte_mkuffd_wp(new_pte));
 	set_huge_pte_at(mm, haddr, ptep, new_pte);

 	hugetlb_count_add(pages_per_huge_page(h), mm);
@@ -5614,8 +5622,10 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	mutex_lock(&hugetlb_fault_mutex_table[hash]);

 	entry = huge_ptep_get(ptep);
-	if (huge_pte_none(entry)) {
-		ret = hugetlb_no_page(mm, vma, mapping, idx, address, ptep, flags);
+	/* PTE markers should be handled the same way as none pte */
+	if (huge_pte_none_mostly(entry)) {
+		ret = hugetlb_no_page(mm, vma, mapping, idx, address, ptep,
+				      entry, flags);
 		goto out_mutex;
 	}

--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id B3D87C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:29 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 9B6BF63218
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236617AbhKOIHS (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:07:18 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:26778 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236817AbhKOIEP (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:04:15 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963279;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=2a8iiYw9cHHedU0g0ICbwjIFnh904x/uUid5qyax/0o=;
        b=AHk0Tm4xF4ne0omtOz0Iab+4UbU7D7+ieFqKOwAmt1149m+fzmjj3R6MOtCvDn25L3LdqL
        IhNXRF1hXH1oo6IgvBAiXs4TPIvievcev3o3CL9SLaJGjI3bSRAl3lZCN7wALESEEchlal
        9ciGEv5VguBK2p/n5Ne6jeuBvlmbxYQ=
Received: from mail-pj1-f72.google.com (mail-pj1-f72.google.com
 [209.85.216.72]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-324--a7bkPVVNaKDM5iGF3gNDQ-1; Mon, 15 Nov 2021 03:01:18 -0500
X-MC-Unique: -a7bkPVVNaKDM5iGF3gNDQ-1
Received: by mail-pj1-f72.google.com with SMTP id u11-20020a17090a4bcb00b001a6e77f7312so8617225pjl.5
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:01:18 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=2a8iiYw9cHHedU0g0ICbwjIFnh904x/uUid5qyax/0o=;
        b=fShXBlSyF6zCwYZFi3NF0mz5limBMtC+bSHGqbpvVohGdXeMFkBiGl4wUWnCeUzB9u
         x8ETf4HZs/B9p8xguujfnl205oVUC9fvFikJWd/eYihZ7WQ/X3n0+k1CMi/iSFSiWvOJ
         lf6gDWwh/qN8ERjwI/+yVSM+YrnjY7AZjpBtWiPJXRyI0+UrKj/IjZPOGwu4cXuHVAiK
         eKysIb+MPVwYPAFd44LfVlt2nN2Y1eFcEeeYWwwTsQnwmGT2pOkHZsYGKQpNwBNU5NlU
         5pzCJY/xidtkpcXUCg0pDq14+a3W/INWuRzME8oTW3DrnlaP+Bc6u3xCjiotJt7xIgdX
         RvnQ==
X-Gm-Message-State: AOAM53103Ftbx9h/m+xMKbUPmFF6q5CVUp/k0WQ6yi4T3EYU9EZC+bNl
        TT9v+8GcYNblVfdUg8b6PtqQqERNEsXpu/S8RP6abpfvzxv6xUG6YzhShull86IUzIx/BOpkDHg
        7fbb7fbranR669ehHIGCTaKjC
X-Received: by 2002:a17:90a:e7c4:: with SMTP id kb4mr62908411pjb.237.1636963277375;
        Mon, 15 Nov 2021 00:01:17 -0800 (PST)
X-Google-Smtp-Source: ABdhPJxMaGzKjVGqxW+/p2kNrUGh0+wmkSEQkrNVusG8+B8od3HYJT9hN/7CV3ovfOezj2dLkfDFdA==
X-Received: by 2002:a17:90a:e7c4:: with SMTP id kb4mr62908368pjb.237.1636963277100;
        Mon, 15 Nov 2021 00:01:17 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id ng9sm19694926pjb.4.2021.11.15.00.01.08
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:01:16 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 10/23] mm/shmem: Handle uffd-wp during fork()
Date:   Mon, 15 Nov 2021 16:01:03 +0800
Message-Id: <20211115080103.74640-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Normally we skip copy page when fork() for VM_SHARED shmem, but we can't skip
it anymore if uffd-wp is enabled on dst vma.  This should only happen when the
src uffd has UFFD_FEATURE_EVENT_FORK enabled on uffd-wp shmem vma, so that
VM_UFFD_WP will be propagated onto dst vma too, then we should copy the
pgtables with uffd-wp bit and pte markers, because these information will be
lost otherwise.

Since the condition checks will become even more complicated for deciding
"whether a vma needs to copy the pgtable during fork()", introduce a helper
vma_needs_copy() for it, so everything will be clearer.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/memory.c | 49 +++++++++++++++++++++++++++++++++++++++++--------
 1 file changed, 41 insertions(+), 8 deletions(-)

diff --git a/mm/memory.c b/mm/memory.c
index fef6a91c5dfb..cc625c616645 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -859,6 +859,14 @@ copy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		if (try_restore_exclusive_pte(src_pte, src_vma, addr))
 			return -EBUSY;
 		return -ENOENT;
+	} else if (is_pte_marker_entry(entry)) {
+		/*
+		 * We're copying the pgtable should only because dst_vma has
+		 * uffd-wp enabled, do sanity check.
+		 */
+		WARN_ON_ONCE(!userfaultfd_wp(dst_vma));
+		set_pte_at(dst_mm, addr, dst_pte, pte);
+		return 0;
 	}
 	if (!userfaultfd_wp(dst_vma))
 		pte = pte_swp_clear_uffd_wp(pte);
@@ -1227,6 +1235,38 @@ copy_p4d_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 	return 0;
 }

+/*
+ * Return true if the vma needs to copy the pgtable during this fork().  Return
+ * false when we can speed up fork() by allowing lazy page faults later until
+ * when the child accesses the memory range.
+ */
+bool
+vma_needs_copy(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
+{
+	/*
+	 * Always copy pgtables when dst_vma has uffd-wp enabled even if it's
+	 * file-backed (e.g. shmem). Because when uffd-wp is enabled, pgtable
+	 * contains uffd-wp protection information, that's something we can't
+	 * retrieve from page cache, and skip copying will lose those info.
+	 */
+	if (userfaultfd_wp(dst_vma))
+		return true;
+
+	if (src_vma->vm_flags & (VM_HUGETLB | VM_PFNMAP | VM_MIXEDMAP))
+		return true;
+
+	if (src_vma->anon_vma)
+		return true;
+
+	/*
+	 * Don't copy ptes where a page fault will fill them correctly.  Fork
+	 * becomes much lighter when there are big shared or private readonly
+	 * mappings. The tradeoff is that copy_page_range is more efficient
+	 * than faulting.
+	 */
+	return false;
+}
+
 int
 copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
 {
@@ -1240,14 +1280,7 @@ copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
 	bool is_cow;
 	int ret;

-	/*
-	 * Don't copy ptes where a page fault will fill them correctly.
-	 * Fork becomes much lighter when there are big shared or private
-	 * readonly mappings. The tradeoff is that copy_page_range is more
-	 * efficient than faulting.
-	 */
-	if (!(src_vma->vm_flags & (VM_HUGETLB | VM_PFNMAP | VM_MIXEDMAP)) &&
-	    !src_vma->anon_vma)
+	if (!vma_needs_copy(dst_vma, src_vma))
 		return 0;

 	if (is_vm_hugetlb_page(src_vma))
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 36EDDC433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:49 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 218D261B97
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:49 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236317AbhKOIHe (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:07:34 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:51093 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S234884AbhKOIFm (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:05:42 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963366;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=qkupakg3emmkCuPdLA0U47j3HM8dJoa5zeaf2nr8YrE=;
        b=O1/v6JMId/inNqCslexgC86sf6yp70vXyfZR9KVpIwBCjYn/IpVlaG3oj6qm0kFWDZp/tT
        1+Gb6Zk5s6FsS2q0vgzToH/40d8MVTWANRn1FpabBdeZrPcEU6vUB9A6+3H/OSgcpv9RjC
        vkKQCl9841EwD/Qoerz1EpUH3lZWFFE=
Received: from mail-pl1-f198.google.com (mail-pl1-f198.google.com
 [209.85.214.198]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-154-QbftAqmcObqOppUWt8ZSxA-1; Mon, 15 Nov 2021 03:02:43 -0500
X-MC-Unique: QbftAqmcObqOppUWt8ZSxA-1
Received: by mail-pl1-f198.google.com with SMTP id p3-20020a170903248300b00143c00a5411so577997plw.12
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:02:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=qkupakg3emmkCuPdLA0U47j3HM8dJoa5zeaf2nr8YrE=;
        b=H1Jyc/SMuZemZUMxXJg3y2j/dRFUzG1Aq0/UptN8LsvE2kWkvddK9WSsn5E94Pl2Bh
         vPRvjb1DjP8SVKXFBQuzzEZVjw9xjVGTunxEgXNq+JdX3or5H7vxYSCA/1AIjrp9X57V
         cFX4cfxAD0EsspY1jhxSNO8bOHEp2Ncs/KGofm54l6zQ0kovhJYmTPlMQ88G2ezLj1mn
         4t2QclizKUSoHsOQeE9rJlOOq0vwGXkAW7YPWGm+5Aee6qkzT5BnHcIPcFh6leqP/4W/
         rYShDFFrTf8muz3vhRyveUrIoX8kxIqOzB8Y+w1N8jKk7qfxAnfC5BMUmk9Ugmopu04O
         0roQ==
X-Gm-Message-State: AOAM532dQkNnLEPq+dZ697D4t/hoNrpeW+phrlR+LsOsuH/o0BgH4BsP
        RJjAe6BBPV1B0M7lqR+sltCoDx//jkTmDswJkweN3QEoQjyTVM7TfRrgja+Q7vN1U4VzkppV37A
        7YaLL4abJzS6yQrS8wCc2cgQu
X-Received: by 2002:aa7:8019:0:b0:44d:d761:6f79 with SMTP id j25-20020aa78019000000b0044dd7616f79mr31742113pfi.3.1636963362556;
        Mon, 15 Nov 2021 00:02:42 -0800 (PST)
X-Google-Smtp-Source: ABdhPJyv5uP4O5V4xtiTKpwFZqqVXpd4X2FyO5vCLmFS1r+O+rAVPgEZZOgFH+PNonWxXoKguHRPXQ==
X-Received: by 2002:aa7:8019:0:b0:44d:d761:6f79 with SMTP id j25-20020aa78019000000b0044dd7616f79mr31742074pfi.3.1636963362297;
        Mon, 15 Nov 2021 00:02:42 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id 95sm11508978pjo.2.2021.11.15.00.02.34
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:02:41 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 16/23] mm/hugetlb: Allow uffd wr-protect none ptes
Date:   Mon, 15 Nov 2021 16:02:28 +0800
Message-Id: <20211115080228.74982-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Teach hugetlbfs code to wr-protect none ptes just in case the page cache
existed for that pte.  Meanwhile we also need to be able to recognize a uffd-wp
marker pte and remove it for uffd_wp_resolve.

Since at it, introduce a variable "psize" to replace all references to the huge
page size fetcher.

Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/hugetlb.c | 28 ++++++++++++++++++++++++----
 1 file changed, 24 insertions(+), 4 deletions(-)

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index e8d01277af0f..bba2ede5f6dc 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6145,7 +6145,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 	pte_t *ptep;
 	pte_t pte;
 	struct hstate *h = hstate_vma(vma);
-	unsigned long pages = 0;
+	unsigned long pages = 0, psize = huge_page_size(h);
 	bool shared_pmd = false;
 	struct mmu_notifier_range range;
 	bool uffd_wp = cp_flags & MM_CP_UFFD_WP;
@@ -6165,13 +6165,19 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,

 	mmu_notifier_invalidate_range_start(&range);
 	i_mmap_lock_write(vma->vm_file->f_mapping);
-	for (; address < end; address += huge_page_size(h)) {
+	for (; address < end; address += psize) {
 		spinlock_t *ptl;
-		ptep = huge_pte_offset(mm, address, huge_page_size(h));
+		ptep = huge_pte_offset(mm, address, psize);
 		if (!ptep)
 			continue;
 		ptl = huge_pte_lock(h, mm, ptep);
 		if (huge_pmd_unshare(mm, vma, &address, ptep)) {
+			/*
+			 * When uffd-wp is enabled on the vma, unshare
+			 * shouldn't happen at all.  Warn about it if it
+			 * happened due to some reason.
+			 */
+			WARN_ON_ONCE(uffd_wp || uffd_wp_resolve);
 			pages++;
 			spin_unlock(ptl);
 			shared_pmd = true;
@@ -6196,12 +6202,20 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 				else if (uffd_wp_resolve)
 					newpte = pte_swp_clear_uffd_wp(newpte);
 				set_huge_swap_pte_at(mm, address, ptep,
-						     newpte, huge_page_size(h));
+						     newpte, psize);
 				pages++;
 			}
 			spin_unlock(ptl);
 			continue;
 		}
+		if (unlikely(is_pte_marker_uffd_wp(pte))) {
+			/*
+			 * This is changing a non-present pte into a none pte,
+			 * no need for huge_ptep_modify_prot_start/commit().
+			 */
+			if (uffd_wp_resolve)
+				huge_pte_clear(mm, address, ptep, psize);
+		}
 		if (!huge_pte_none(pte)) {
 			pte_t old_pte;
 			unsigned int shift = huge_page_shift(hstate_vma(vma));
@@ -6215,6 +6229,12 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 				pte = huge_pte_clear_uffd_wp(pte);
 			huge_ptep_modify_prot_commit(vma, address, ptep, old_pte, pte);
 			pages++;
+		} else {
+			/* None pte */
+			if (unlikely(uffd_wp))
+				/* Safe to modify directly (none->non-present). */
+				set_huge_pte_at(mm, address, ptep,
+						make_pte_marker(PTE_MARKER_UFFD_WP));
 		}
 		spin_unlock(ptl);
 	}
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id CFF87C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:51 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id B8D4563219
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:51 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S235534AbhKOIHp (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:07:45 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.129.124]:23743 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S237360AbhKOIF4 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:05:56 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963380;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=YHxegeFAQpAm7QNoFX5FWZJCYVnUR8APDp1wCGQBN2s=;
        b=JOd2/ozamFEcIEJvd7UUm0JLSR6iLX7gNMLX9nKyCtVQpGuEv36/pzpmY8tck/yet6mrkZ
        hkeKl8aWxRGmIegckeQP3CGxMqih1HHdXHtXQlKM+wglQ+VRuoPSzPKlEYDuQgH4FM4vf/
        DjGRkL/lN+55ZhNCpJn3Nd7VROdwp08=
Received: from mail-pf1-f199.google.com (mail-pf1-f199.google.com
 [209.85.210.199]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-83-M4kXeCTtOc6iUCFic7Puug-1; Mon, 15 Nov 2021 03:02:57 -0500
X-MC-Unique: M4kXeCTtOc6iUCFic7Puug-1
Received: by mail-pf1-f199.google.com with SMTP id c21-20020a62e815000000b004a29ebf0aa7so2630571pfi.2
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:02:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=YHxegeFAQpAm7QNoFX5FWZJCYVnUR8APDp1wCGQBN2s=;
        b=AYwglgjpkuhFVMxUdhn32CisccG4V3wtR5nEAvG7zWiKe9o0rSZgK91V+zYH73HBMz
         2PZ2RJg8JDlTu1FPiBTn3Rb6bg+HkPsJctjuHZdDBAtLisSopL5xkqDK0Fk4+4gjmZbY
         frDaJ756AjhrpdoEDIAQcnTkqjneH81krfAcymOIUzVV/P0Bdqh+hiK4WtIkhi6xr7cj
         koJl4/8B2elXLOekXt5sWX805DPvumjgzOrRQGZqUqBNC6iXWSGx7cxkfBBB1NhKsjGG
         6HVsKOEYJXLnLV3oH2LTcPLt5DwKotyj+wex+GfCPpMcUR/zOy7KkeKKYeO/U9JTXZmk
         8C8g==
X-Gm-Message-State: AOAM531qAbX1sxAkwXBHLvDEhuHUVKuHMFtuF56Y42oqlrK7Izt+c8f2
        V7V5xwpjOmRQvRMyKO3rnJUdaZ+g3ehKxyWUFFeKVjshYx/1Bbg/b/e0vwwTFVo/e574M34iZT2
        NhgCRirT95QQmrtJGyzeLsqHl
X-Received: by 2002:a17:902:be06:b0:142:5a21:9e8a with SMTP id r6-20020a170902be0600b001425a219e8amr33100048pls.17.1636963376148;
        Mon, 15 Nov 2021 00:02:56 -0800 (PST)
X-Google-Smtp-Source: ABdhPJzi3mzgc10Ws3LtJnt+vKzwU+NxNytHynLNU3P3cefT6vAfJ+/+/0CGsrQ9Zf5D1jVuVMeZpQ==
X-Received: by 2002:a17:902:be06:b0:142:5a21:9e8a with SMTP id r6-20020a170902be0600b001425a219e8amr33100014pls.17.1636963375845;
        Mon, 15 Nov 2021 00:02:55 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id d2sm15074317pfj.42.2021.11.15.00.02.48
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:02:55 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 17/23] mm/hugetlb: Only drop uffd-wp special pte if required
Date:   Mon, 15 Nov 2021 16:02:43 +0800
Message-Id: <20211115080243.75040-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

As with shmem uffd-wp special ptes, only drop the uffd-wp special swap pte if
unmapping an entire vma or synchronized such that faults can not race with the
unmap operation.  This requires passing zap_flags all the way to the lowest
level hugetlb unmap routine: __unmap_hugepage_range.

In general, unmap calls originated in hugetlbfs code will pass the
ZAP_FLAG_DROP_MARKER flag as synchronization is in place to prevent faults.
The exception is hole punch which will first unmap without any synchronization.
Later when hole punch actually removes the page from the file, it will check to
see if there was a subsequent fault and if so take the hugetlb fault mutex
while unmapping again.  This second unmap will pass in ZAP_FLAG_DROP_MARKER.

The justification of "whether to apply ZAP_FLAG_DROP_MARKER flag when unmap a
hugetlb range" is (IMHO): we should never reach a state when a page fault could
errornously fault in a page-cache page that was wr-protected to be writable,
even in an extremely short period.  That could happen if e.g. we pass
ZAP_FLAG_DROP_MARKER when hugetlbfs_punch_hole() calls hugetlb_vmdelete_list(),
because if a page faults after that call and before remove_inode_hugepages() is
executed, the page cache can be mapped writable again in the small racy window,
that can cause unexpected data overwritten.

Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
Signed-off-by: Peter Xu <peterx@redhat.com>
---
 fs/hugetlbfs/inode.c    | 15 +++++++++------
 include/linux/hugetlb.h |  8 +++++---
 mm/hugetlb.c            | 33 +++++++++++++++++++++++++--------
 mm/memory.c             |  5 ++++-
 4 files changed, 43 insertions(+), 18 deletions(-)

diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index 49d2e686be74..92c8d1a47404 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -404,7 +404,8 @@ static void remove_huge_page(struct page *page)
 }

 static void
-hugetlb_vmdelete_list(struct rb_root_cached *root, pgoff_t start, pgoff_t end)
+hugetlb_vmdelete_list(struct rb_root_cached *root, pgoff_t start, pgoff_t end,
+		      unsigned long zap_flags)
 {
 	struct vm_area_struct *vma;

@@ -437,7 +438,7 @@ hugetlb_vmdelete_list(struct rb_root_cached *root, pgoff_t start, pgoff_t end)
 		}

 		unmap_hugepage_range(vma, vma->vm_start + v_offset, v_end,
-									NULL);
+				     NULL, zap_flags);
 	}
 }

@@ -515,7 +516,8 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,
 				mutex_lock(&hugetlb_fault_mutex_table[hash]);
 				hugetlb_vmdelete_list(&mapping->i_mmap,
 					index * pages_per_huge_page(h),
-					(index + 1) * pages_per_huge_page(h));
+					(index + 1) * pages_per_huge_page(h),
+					ZAP_FLAG_DROP_MARKER);
 				i_mmap_unlock_write(mapping);
 			}

@@ -581,7 +583,8 @@ static void hugetlb_vmtruncate(struct inode *inode, loff_t offset)
 	i_mmap_lock_write(mapping);
 	i_size_write(inode, offset);
 	if (!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root))
-		hugetlb_vmdelete_list(&mapping->i_mmap, pgoff, 0);
+		hugetlb_vmdelete_list(&mapping->i_mmap, pgoff, 0,
+				      ZAP_FLAG_DROP_MARKER);
 	i_mmap_unlock_write(mapping);
 	remove_inode_hugepages(inode, offset, LLONG_MAX);
 }
@@ -614,8 +617,8 @@ static long hugetlbfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		i_mmap_lock_write(mapping);
 		if (!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root))
 			hugetlb_vmdelete_list(&mapping->i_mmap,
-						hole_start >> PAGE_SHIFT,
-						hole_end  >> PAGE_SHIFT);
+					      hole_start >> PAGE_SHIFT,
+					      hole_end >> PAGE_SHIFT, 0);
 		i_mmap_unlock_write(mapping);
 		remove_inode_hugepages(inode, hole_start, hole_end);
 		inode_unlock(inode);
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index a46011510e49..4c3ea7ee8ce8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -143,11 +143,12 @@ long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			 unsigned long *, unsigned long *, long, unsigned int,
 			 int *);
 void unmap_hugepage_range(struct vm_area_struct *,
-			  unsigned long, unsigned long, struct page *);
+			  unsigned long, unsigned long, struct page *,
+			  unsigned long);
 void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 			  struct vm_area_struct *vma,
 			  unsigned long start, unsigned long end,
-			  struct page *ref_page);
+			  struct page *ref_page, unsigned long zap_flags);
 void hugetlb_report_meminfo(struct seq_file *);
 int hugetlb_report_node_meminfo(char *buf, int len, int nid);
 void hugetlb_show_meminfo(void);
@@ -400,7 +401,8 @@ static inline unsigned long hugetlb_change_protection(

 static inline void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 			struct vm_area_struct *vma, unsigned long start,
-			unsigned long end, struct page *ref_page)
+			unsigned long end, struct page *ref_page,
+			unsigned long zap_flags)
 {
 	BUG();
 }
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index bba2ede5f6dc..16fb9cd8d9c5 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -4926,7 +4926,7 @@ int move_hugetlb_page_tables(struct vm_area_struct *vma,

 static void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
 				   unsigned long start, unsigned long end,
-				   struct page *ref_page)
+				   struct page *ref_page, unsigned long zap_flags)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long address;
@@ -4983,7 +4983,18 @@ static void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct
 		 * unmapped and its refcount is dropped, so just clear pte here.
 		 */
 		if (unlikely(!pte_present(pte))) {
-			huge_pte_clear(mm, address, ptep, sz);
+			/*
+			 * If the pte was wr-protected by uffd-wp in any of the
+			 * swap forms, meanwhile the caller does not want to
+			 * drop the uffd-wp bit in this zap, then replace the
+			 * pte with a marker.
+			 */
+			if (pte_swp_uffd_wp_any(pte) &&
+			    !(zap_flags & ZAP_FLAG_DROP_MARKER))
+				set_huge_pte_at(mm, address, ptep,
+						make_pte_marker(PTE_MARKER_UFFD_WP));
+			else
+				huge_pte_clear(mm, address, ptep, sz);
 			spin_unlock(ptl);
 			continue;
 		}
@@ -5011,7 +5022,11 @@ static void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct
 		tlb_remove_huge_tlb_entry(h, tlb, ptep, address);
 		if (huge_pte_dirty(pte))
 			set_page_dirty(page);
-
+		/* Leave a uffd-wp pte marker if needed */
+		if (huge_pte_uffd_wp(pte) &&
+		    !(zap_flags & ZAP_FLAG_DROP_MARKER))
+			set_huge_pte_at(mm, address, ptep,
+					make_pte_marker(PTE_MARKER_UFFD_WP));
 		hugetlb_count_sub(pages_per_huge_page(h), mm);
 		page_remove_rmap(page, true);

@@ -5029,9 +5044,10 @@ static void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct

 void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 			  struct vm_area_struct *vma, unsigned long start,
-			  unsigned long end, struct page *ref_page)
+			  unsigned long end, struct page *ref_page,
+			  unsigned long zap_flags)
 {
-	__unmap_hugepage_range(tlb, vma, start, end, ref_page);
+	__unmap_hugepage_range(tlb, vma, start, end, ref_page, zap_flags);

 	/*
 	 * Clear this flag so that x86's huge_pmd_share page_table_shareable
@@ -5047,12 +5063,13 @@ void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 }

 void unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,
-			  unsigned long end, struct page *ref_page)
+			  unsigned long end, struct page *ref_page,
+			  unsigned long zap_flags)
 {
 	struct mmu_gather tlb;

 	tlb_gather_mmu(&tlb, vma->vm_mm);
-	__unmap_hugepage_range(&tlb, vma, start, end, ref_page);
+	__unmap_hugepage_range(&tlb, vma, start, end, ref_page, zap_flags);
 	tlb_finish_mmu(&tlb);
 }

@@ -5107,7 +5124,7 @@ static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
 		 */
 		if (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))
 			unmap_hugepage_range(iter_vma, address,
-					     address + huge_page_size(h), page);
+					     address + huge_page_size(h), page, 0);
 	}
 	i_mmap_unlock_write(mapping);
 }
diff --git a/mm/memory.c b/mm/memory.c
index cc625c616645..69a73d47513b 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1631,8 +1631,11 @@ static void unmap_single_vma(struct mmu_gather *tlb,
 			 * safe to do nothing in this case.
 			 */
 			if (vma->vm_file) {
+				unsigned long zap_flags = details ?
+				    details->zap_flags : 0;
 				i_mmap_lock_write(vma->vm_file->f_mapping);
-				__unmap_hugepage_range_final(tlb, vma, start, end, NULL);
+				__unmap_hugepage_range_final(tlb, vma, start, end,
+							     NULL, zap_flags);
 				i_mmap_unlock_write(vma->vm_file->f_mapping);
 			}
 		} else
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id A9219C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:59 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 897A261B97
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:04:59 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229967AbhKOIHu (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:07:50 -0500
Received: from us-smtp-delivery-124.mimecast.com ([216.205.24.124]:44940 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S237373AbhKOIGJ (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:06:09 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963392;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=nlsVhTui5YYYuoTB5Im50dnZjt4bHIyLERdEUNhIhqY=;
        b=UmP5a45vtfZzsTvMuMtv9KGCILNf9ja7NMLs7mgfR+0Jg090SsmRYB0Qxd3b4OBJWxdEds
        2c734DdkwnVi8rKzQGLLlpCrGxgMSeIYCNHY9NiJ7006pSWEaD74Qy4of7l2EReXuglc7c
        rCzvPh+MQSkbk0TkSSg9zlelImUL/zE=
Received: from mail-pj1-f72.google.com (mail-pj1-f72.google.com
 [209.85.216.72]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-463-49GsGH__MB-tXAggVzckzg-1; Mon, 15 Nov 2021 03:03:10 -0500
X-MC-Unique: 49GsGH__MB-tXAggVzckzg-1
Received: by mail-pj1-f72.google.com with SMTP id r23-20020a17090a941700b001a74be6cf80so4881495pjo.2
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:03:10 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=nlsVhTui5YYYuoTB5Im50dnZjt4bHIyLERdEUNhIhqY=;
        b=u1B8IQzBjUKlq9Owqg24QpnlPmj5MGyc7hMwAzdfn8yGNjs4zJeZ1ugEmUOhcVRzVp
         A99HyuU0kWQYIXTsZlKd6rwjH4jxVgtuKBpGydXJF8MhwX58Jh5xhhE8BX7Kl3FXZ+kI
         c/U2WK8yuO0Fz+GgAFMlwbiOvtKP2gOPvFN/Kf9ssfzQLTUJielymRiIQMwRcLxICd0r
         5vWsukJlqW2hPs52EZr8lQwXw8oPAsWA9+dl5VFxU3au6T8vg/wiNjWwue/DimaYDzrJ
         HuEzl2ZM+qdu+wjEhKUdsTMGU8pqEqsElzuhUu/dQMgf3TjMsvB0rAsnm0GE4O++feNH
         0ANQ==
X-Gm-Message-State: AOAM532s7AiHGBdFf4JK5IJJs2HQzgSuaY/KqCx45fI5sYu/7qmF6XR2
        kQJ5ZlXFsrei2ppg2mIdOCkkQO7vao+fOqn1/ZS3hqsIqrRKRzrBJ5dkYKk/Lja/RQJEZ39Fug9
        xHvNLu0vFcA0J78Lut3DYj0CW
X-Received: by 2002:a17:90b:3849:: with SMTP id nl9mr63063510pjb.145.1636963389732;
        Mon, 15 Nov 2021 00:03:09 -0800 (PST)
X-Google-Smtp-Source: ABdhPJzQjMvgZ1MCPpUCZR/YQLBlvTIXbGd+IIXURYXaaXpubOwQw7QvuE9XXYAbw3RiQ73tuH1vFg==
X-Received: by 2002:a17:90b:3849:: with SMTP id nl9mr63063457pjb.145.1636963389453;
        Mon, 15 Nov 2021 00:03:09 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id x20sm11712603pjp.48.2021.11.15.00.03.01
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:03:08 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 18/23] mm/hugetlb: Handle uffd-wp during fork()
Date:   Mon, 15 Nov 2021 16:02:56 +0800
Message-Id: <20211115080256.75095-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Firstly, we'll need to pass in dst_vma into copy_hugetlb_page_range() because
for uffd-wp it's the dst vma that matters on deciding how we should treat
uffd-wp protected ptes.

We should recognize pte markers during fork and do the pte copy if needed.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 include/linux/hugetlb.h |  7 +++++--
 mm/hugetlb.c            | 41 +++++++++++++++++++++++++++--------------
 mm/memory.c             |  2 +-
 3 files changed, 33 insertions(+), 17 deletions(-)

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 4c3ea7ee8ce8..6935b02f1081 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -137,7 +137,8 @@ int move_hugetlb_page_tables(struct vm_area_struct *vma,
 			     struct vm_area_struct *new_vma,
 			     unsigned long old_addr, unsigned long new_addr,
 			     unsigned long len);
-int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
+int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *,
+			    struct vm_area_struct *, struct vm_area_struct *);
 long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			 struct page **, struct vm_area_struct **,
 			 unsigned long *, unsigned long *, long, unsigned int,
@@ -268,7 +269,9 @@ static inline struct page *follow_huge_addr(struct mm_struct *mm,
 }

 static inline int copy_hugetlb_page_range(struct mm_struct *dst,
-			struct mm_struct *src, struct vm_area_struct *vma)
+					  struct mm_struct *src,
+					  struct vm_area_struct *dst_vma,
+					  struct vm_area_struct *src_vma)
 {
 	BUG();
 	return 0;
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 16fb9cd8d9c5..cf9a0e8c32ba 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -4690,23 +4690,24 @@ hugetlb_install_page(struct vm_area_struct *vma, pte_t *ptep, unsigned long addr
 }

 int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
-			    struct vm_area_struct *vma)
+			    struct vm_area_struct *dst_vma,
+			    struct vm_area_struct *src_vma)
 {
 	pte_t *src_pte, *dst_pte, entry, dst_entry;
 	struct page *ptepage;
 	unsigned long addr;
-	bool cow = is_cow_mapping(vma->vm_flags);
-	struct hstate *h = hstate_vma(vma);
+	bool cow = is_cow_mapping(src_vma->vm_flags);
+	struct hstate *h = hstate_vma(src_vma);
 	unsigned long sz = huge_page_size(h);
 	unsigned long npages = pages_per_huge_page(h);
-	struct address_space *mapping = vma->vm_file->f_mapping;
+	struct address_space *mapping = src_vma->vm_file->f_mapping;
 	struct mmu_notifier_range range;
 	int ret = 0;

 	if (cow) {
-		mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, src,
-					vma->vm_start,
-					vma->vm_end);
+		mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, src_vma, src,
+					src_vma->vm_start,
+					src_vma->vm_end);
 		mmu_notifier_invalidate_range_start(&range);
 	} else {
 		/*
@@ -4718,12 +4719,12 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 		i_mmap_lock_read(mapping);
 	}

-	for (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {
+	for (addr = src_vma->vm_start; addr < src_vma->vm_end; addr += sz) {
 		spinlock_t *src_ptl, *dst_ptl;
 		src_pte = huge_pte_offset(src, addr, sz);
 		if (!src_pte)
 			continue;
-		dst_pte = huge_pte_alloc(dst, vma, addr, sz);
+		dst_pte = huge_pte_alloc(dst, dst_vma, addr, sz);
 		if (!dst_pte) {
 			ret = -ENOMEM;
 			break;
@@ -4758,6 +4759,7 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 		} else if (unlikely(is_hugetlb_entry_migration(entry) ||
 				    is_hugetlb_entry_hwpoisoned(entry))) {
 			swp_entry_t swp_entry = pte_to_swp_entry(entry);
+			bool uffd_wp = huge_pte_uffd_wp(entry);

 			if (is_writable_migration_entry(swp_entry) && cow) {
 				/*
@@ -4767,10 +4769,21 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 				swp_entry = make_readable_migration_entry(
 							swp_offset(swp_entry));
 				entry = swp_entry_to_pte(swp_entry);
+				if (userfaultfd_wp(src_vma) && uffd_wp)
+					entry = huge_pte_mkuffd_wp(entry);
 				set_huge_swap_pte_at(src, addr, src_pte,
 						     entry, sz);
 			}
+			if (!userfaultfd_wp(dst_vma) && uffd_wp)
+				entry = huge_pte_clear_uffd_wp(entry);
 			set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);
+		} else if (unlikely(is_pte_marker(entry))) {
+			/*
+			 * We copy the pte marker only if the dst vma has
+			 * uffd-wp enabled.
+			 */
+			if (userfaultfd_wp(dst_vma))
+				set_huge_pte_at(dst, addr, dst_pte, entry);
 		} else {
 			entry = huge_ptep_get(src_pte);
 			ptepage = pte_page(entry);
@@ -4785,20 +4798,20 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 			 * need to be without the pgtable locks since we could
 			 * sleep during the process.
 			 */
-			if (unlikely(page_needs_cow_for_dma(vma, ptepage))) {
+			if (unlikely(page_needs_cow_for_dma(src_vma, ptepage))) {
 				pte_t src_pte_old = entry;
 				struct page *new;

 				spin_unlock(src_ptl);
 				spin_unlock(dst_ptl);
 				/* Do not use reserve as it's private owned */
-				new = alloc_huge_page(vma, addr, 1);
+				new = alloc_huge_page(dst_vma, addr, 1);
 				if (IS_ERR(new)) {
 					put_page(ptepage);
 					ret = PTR_ERR(new);
 					break;
 				}
-				copy_user_huge_page(new, ptepage, addr, vma,
+				copy_user_huge_page(new, ptepage, addr, dst_vma,
 						    npages);
 				put_page(ptepage);

@@ -4808,13 +4821,13 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 				spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
 				entry = huge_ptep_get(src_pte);
 				if (!pte_same(src_pte_old, entry)) {
-					restore_reserve_on_error(h, vma, addr,
+					restore_reserve_on_error(h, dst_vma, addr,
 								new);
 					put_page(new);
 					/* dst_entry won't change as in child */
 					goto again;
 				}
-				hugetlb_install_page(vma, dst_pte, addr, new);
+				hugetlb_install_page(dst_vma, dst_pte, addr, new);
 				spin_unlock(src_ptl);
 				spin_unlock(dst_ptl);
 				continue;
diff --git a/mm/memory.c b/mm/memory.c
index 69a73d47513b..89715d1ec956 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1284,7 +1284,7 @@ copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
 		return 0;

 	if (is_vm_hugetlb_page(src_vma))
-		return copy_hugetlb_page_range(dst_mm, src_mm, src_vma);
+		return copy_hugetlb_page_range(dst_mm, src_mm, dst_vma, src_vma);

 	if (unlikely(src_vma->vm_flags & VM_PFNMAP)) {
 		/*
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 9EC56C433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:06:16 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 7D48B63218
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:06:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236955AbhKOIIn (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:08:43 -0500
Received: from us-smtp-delivery-124.mimecast.com ([216.205.24.124]:55677 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S234849AbhKOIGV (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:06:21 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963406;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=ILY47EUAX8YWGrwtSg12WPG1Xh7JfzVTNjA+1Egv88U=;
        b=cZ7ef8m71jqt5slTBUXB9MzyLT3tASANifzSiQP58oTRCZqbBGowF2BcC0A+kIjlq7D7up
        fsVA6FrqK9ng0s+Ku+62bS5Sst2wwosro5zYso0xbd5lt16GiqZAI9p5bEBy1/KM4/dU1L
        uPMNOHyEoY4NqDKTnMJ23isEtlfEYy4=
Received: from mail-pl1-f200.google.com (mail-pl1-f200.google.com
 [209.85.214.200]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-431-rYIIYGCJOUuIq3zRTmpzwQ-1; Mon, 15 Nov 2021 03:03:24 -0500
X-MC-Unique: rYIIYGCJOUuIq3zRTmpzwQ-1
Received: by mail-pl1-f200.google.com with SMTP id x18-20020a170902ec9200b00143c6409dbcso489678plg.5
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:03:24 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=ILY47EUAX8YWGrwtSg12WPG1Xh7JfzVTNjA+1Egv88U=;
        b=BRKdOjf4/euoVHmP/N7ROBa1nV3Iiaui/q2U1gNpdD+BObg7N1TRyX/5WR0C3h5eea
         CecRbrrHL/wc0zkWhKqHYfckL9rZ3l6DVRfBQWAFJvhLmnddSxvLfM7kmRIdoVnkadp7
         7wn2bZZ2eY79S4Y16AthXpBNazeHR/6eisQutQLS8lqP5ucAWojgXSJkolMJRQ7+Yivs
         hIMwtr/EKTOHG5bHHe7SnGSMnCZRH2R6Es2F7xVLL0VTVgDAT9J9UyXBYmPNGFkJmuP6
         bwbgGUqsaH9lh/LcVT8lIHxOTucnrWfMhJQS2FvcWlZV9/ATWtejUUuzeLp7hOQlcN2S
         vZSA==
X-Gm-Message-State: AOAM532esvAmfCoTCM+ilJrtUVb9LDocbTEYOvVZBsOcMQ/e3xeEEiZ2
        mK4D9o/jKdyjlljsysG8C8Z8BxyZO7nAbSL/0wNC7aqcz+Q77g1x80SaVtr2I7OUd6SlgpZhJpj
        HhctGRGXO/WHRudDFKuVG9OUX
X-Received: by 2002:a17:90b:1d0e:: with SMTP id on14mr62720941pjb.119.1636963403592;
        Mon, 15 Nov 2021 00:03:23 -0800 (PST)
X-Google-Smtp-Source: ABdhPJxn0Rp/rkm5Ee0ybXP2u5WW8WzuvnOkz9ms/eTShUG9S2uyIUVwsqBwYx3b+PT9/ChdhtyCSA==
X-Received: by 2002:a17:90b:1d0e:: with SMTP id on14mr62720896pjb.119.1636963403271;
        Mon, 15 Nov 2021 00:03:23 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id rm10sm12783901pjb.29.2021.11.15.00.03.14
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:03:22 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 19/23] mm/khugepaged: Don't recycle vma pgtable if uffd-wp registered
Date:   Mon, 15 Nov 2021 16:03:10 +0800
Message-Id: <20211115080310.75154-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

When we're trying to collapse a 2M huge shmem page, don't retract pgtable pmd
page if it's registered with uffd-wp, because that pgtable could have pte
markers installed.  Recycling of that pgtable means we'll lose the pte markers.
That could cause data loss for an uffd-wp enabled application on shmem.

Instead of disabling khugepaged on these files, simply skip retracting these
special VMAs, then the page cache can still be merged into a huge thp, and
other mm/vma can still map the range of file with a huge thp when proper.

Note that checking VM_UFFD_WP needs to be done with mmap_sem held for write,
that avoids race like:

         khugepaged                             user thread
         ==========                             ===========
     check VM_UFFD_WP, not set
                                       UFFDIO_REGISTER with uffd-wp on shmem
                                       wr-protect some pages (install markers)
     take mmap_sem write lock
     erase pmd and free pmd page
      --> pte markers are dropped unnoticed!

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/khugepaged.c | 14 +++++++++++++-
 1 file changed, 13 insertions(+), 1 deletion(-)

diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index e99101162f1a..9c75153a36de 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -1454,6 +1454,10 @@ void collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr)
 	if (!hugepage_vma_check(vma, vma->vm_flags | VM_HUGEPAGE))
 		return;

+	/* Keep pmd pgtable for uffd-wp; see comment in retract_page_tables() */
+	if (userfaultfd_wp(vma))
+		return;
+
 	hpage = find_lock_page(vma->vm_file->f_mapping,
 			       linear_page_index(vma, haddr));
 	if (!hpage)
@@ -1594,7 +1598,15 @@ static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)
 		 * reverse order. Trylock is a way to avoid deadlock.
 		 */
 		if (mmap_write_trylock(mm)) {
-			if (!khugepaged_test_exit(mm)) {
+			/*
+			 * When a vma is registered with uffd-wp, we can't
+			 * recycle the pmd pgtable because there can be pte
+			 * markers installed.  Skip it only, so the rest mm/vma
+			 * can still have the same file mapped hugely, however
+			 * it'll always mapped in small page size for uffd-wp
+			 * registered ranges.
+			 */
+			if (!khugepaged_test_exit(mm) && !userfaultfd_wp(vma)) {
 				spinlock_t *ptl = pmd_lock(mm, pmd);
 				/* assume page table is clear */
 				_pmd = pmdp_collapse_flush(vma, addr, pmd);
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 6F869C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:06:44 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 583FA6321B
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:06:44 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236867AbhKOIJQ (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:09:16 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:39972 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236511AbhKOIGf (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:06:35 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963420;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=l/H+oDixRZnd7dOwc6UEzkc+HkILBWlXcmJWYj576qI=;
        b=Y1s9AOV1hC8bG386KUBA9cXtrqX1p7Zaai4b97IcLsQDASy0U3jVfMegpJNEFLo6h4jPFg
        xpoqCcL/pZSl3oLAs+Ilv5KKMSImMbwrh6ckXiXkeXJ2KjqcKcrM6UoLTGjhONsTlph4H4
        +Z5RKg9Um/1AKtz9s6uXAV7TY2wNA+w=
Received: from mail-pl1-f198.google.com (mail-pl1-f198.google.com
 [209.85.214.198]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-360-81H0PCD_N92lL5GEXbK-MQ-1; Mon, 15 Nov 2021 03:03:39 -0500
X-MC-Unique: 81H0PCD_N92lL5GEXbK-MQ-1
Received: by mail-pl1-f198.google.com with SMTP id s16-20020a170902ea1000b00142728c2ccaso5776908plg.23
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:03:39 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=l/H+oDixRZnd7dOwc6UEzkc+HkILBWlXcmJWYj576qI=;
        b=aZdvBazyknXZDNJpFsrSKaBXH/OaVVQOOSkxqufk7Ont2bKRdSrMuPOrND2FXHY9Zg
         dp4V02JxCMGTbM7Dw791u2u4OB907TYch+hHf1GO76YVQMFRw+2mwOYqnQaXrW8dw1tA
         pfWEiRAhL05zLgtzdovQ8D4FGuau8kvPsESN2oZX1+41vPwLuA2I0kIxdhrWdh4eomox
         VgdDfGUDeFprCeyjV5KzDik7y1dFZVGyUPU1mQo4kYquwsVI0zUoYfNaa2BKGW9DkDA+
         yLUpX/qj1gBK64/RgLGYEZ0jRLzWIZXAYcgAl7l/84YaA3MIv8oa9MItRNSIvDcG+iXK
         AFIA==
X-Gm-Message-State: AOAM5310roNRp6b0mXxZp19wV+hMgN5ctj2dyHE/uvAG3KZfrNgtIL8Z
        VgfwW++dm0dxmk0QDyXX5vJpEsHKF32R5lpgBOgA5lSLJB0Nfd41cqzZhRxsdl2W9BYRajAFlH6
        Ly77WjflpUtM2qDczPqEYIYoa
X-Received: by 2002:a05:6a00:807:b0:49f:9a8d:23b4 with SMTP id m7-20020a056a00080700b0049f9a8d23b4mr30974706pfk.71.1636963417763;
        Mon, 15 Nov 2021 00:03:37 -0800 (PST)
X-Google-Smtp-Source: ABdhPJx7GarKkukoThonF99DLrh9L+TgpvqvwTOsDbfjBOYZ++8KkD2i4+zt+ZNLQP7HNUNlnOJUmQ==
X-Received: by 2002:a05:6a00:807:b0:49f:9a8d:23b4 with SMTP id m7-20020a056a00080700b0049f9a8d23b4mr30974675pfk.71.1636963417529;
        Mon, 15 Nov 2021 00:03:37 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id il13sm15351105pjb.52.2021.11.15.00.03.29
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:03:37 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 20/23] mm/pagemap: Recognize uffd-wp bit for shmem/hugetlbfs
Date:   Mon, 15 Nov 2021 16:03:23 +0800
Message-Id: <20211115080323.75209-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This requires the pagemap code to be able to recognize the newly introduced
swap special pte for uffd-wp, meanwhile the general case for hugetlb that we
recently start to support.  It should make pagemap uffd-wp support complete.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 fs/proc/task_mmu.c | 11 +++++++++++
 1 file changed, 11 insertions(+)

diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index ad667dbc96f5..5d2f73b2e63d 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -1390,6 +1390,12 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,
 		flags |= PM_SWAP;
 		if (is_pfn_swap_entry(entry))
 			page = pfn_swap_entry_to_page(entry);
+		if (is_pte_marker_entry(entry)) {
+			pte_marker marker = pte_marker_get(entry);
+
+			if (marker & PTE_MARKER_UFFD_WP)
+				flags |= PM_UFFD_WP;
+		}
 	}

 	if (page && !PageAnon(page))
@@ -1523,10 +1529,15 @@ static int pagemap_hugetlb_range(pte_t *ptep, unsigned long hmask,
 		if (page_mapcount(page) == 1)
 			flags |= PM_MMAP_EXCLUSIVE;

+		if (huge_pte_uffd_wp(pte))
+			flags |= PM_UFFD_WP;
+
 		flags |= PM_PRESENT;
 		if (pm->show_pfn)
 			frame = pte_pfn(pte) +
 				((addr & ~hmask) >> PAGE_SHIFT);
+	} else if (pte_swp_uffd_wp_any(pte)) {
+		flags |= PM_UFFD_WP;
 	}

 	for (; addr != end; addr += PAGE_SIZE) {
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 2AA38C433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:06:55 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 1319763218
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:06:55 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S237055AbhKOIJq (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:09:46 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.129.124]:52176 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236681AbhKOIGu (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:06:50 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963435;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=QYi6ss6i+qaMQh6aXf6FP47ZJ3XzTF5CNwYJPDp0yLw=;
        b=axqBG3XXlPc+XqEDm2XsoZQ/S0i0ygkoFHHbzXbLeTMxqI4pTu1GAUeEe3NsIUMtcBAw5d
        n0GDCBRizMlltZqlB6g/qj+Lrouin71Kbd5kcg/TkwY88qUs2aMRfiQwwJTGj0LkIzwQtL
        4J7vvyY6rtlslSdKj7cu0xesTj9hMM8=
Received: from mail-pj1-f71.google.com (mail-pj1-f71.google.com
 [209.85.216.71]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-167-6dSv2ayvMMW5daPEeSFWwQ-1; Mon, 15 Nov 2021 03:03:54 -0500
X-MC-Unique: 6dSv2ayvMMW5daPEeSFWwQ-1
Received: by mail-pj1-f71.google.com with SMTP id a16-20020a17090aa51000b001a78699acceso8215755pjq.8
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:03:53 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=QYi6ss6i+qaMQh6aXf6FP47ZJ3XzTF5CNwYJPDp0yLw=;
        b=ANAkFnrE7Xeak4c29shAJgCBhQATuXc65p1WNLb2eIr2foxmc5B4qft2zWI+M/X3Bz
         vbC6Ht/uJZzFzaKys3079ObUcDXBy3vousTxFEVm+7x323iQUK+g/WuGQgrD6/QRK/CG
         AAicbkl3ixGq31fuK3HO/6QkZaIekxoxqac2D4Q3eQxsWz1T5CC29sdTzqu91uHwPInv
         9tfYpS14eHapJHGEjrbNcVXfyBS2CRORvuTSvYXUdnS+J2WI+e1mEJEN74GIXhpfOeDu
         7pL/XWI0W4k4B4+ZfbdBt09t1rHtbIpW+GYHVyBXSyOR3dbx7NaBPW6jdjr5xE+Er+4C
         B9KA==
X-Gm-Message-State: AOAM531+hHZInAw2IyWyoFYc1j+AJlHBjfUAG2qfBusGz1sAXrVeiO6D
        oPiBz2BQ5FX1RlN8QRgiV7kYUQPKpeh5XLNwyTx18v2QNSMA8LNdUQeUKndSd6SSRCb+erNijwP
        PBdbdXrZ9JQS9TuZ8xmaIppix
X-Received: by 2002:a17:902:6905:b0:142:9e19:702e with SMTP id j5-20020a170902690500b001429e19702emr34025118plk.34.1636963433047;
        Mon, 15 Nov 2021 00:03:53 -0800 (PST)
X-Google-Smtp-Source: ABdhPJyvuw9vrvNG2L7cS/AHgrgJmEaz6hA9Bl56RBh77ZosYjxsN756UigcYazaskbxJjNEt9UMhA==
X-Received: by 2002:a17:902:6905:b0:142:9e19:702e with SMTP id j5-20020a170902690500b001429e19702emr34025099plk.34.1636963432806;
        Mon, 15 Nov 2021 00:03:52 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id i15sm14335793pfu.151.2021.11.15.00.03.43
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:03:52 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 21/23] mm/uffd: Enable write protection for shmem & hugetlbfs
Date:   Mon, 15 Nov 2021 16:03:38 +0800
Message-Id: <20211115080338.75264-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

We've had all the necessary changes ready for both shmem and hugetlbfs.  Turn
on all the shmem/hugetlbfs switches for userfaultfd-wp.

We can expand UFFD_API_RANGE_IOCTLS_BASIC with _UFFDIO_WRITEPROTECT too because
all existing types now support write protection mode.

Since vma_can_userfault() will be used elsewhere, move into userfaultfd_k.h.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 fs/userfaultfd.c                 | 21 ++-------------------
 include/linux/userfaultfd_k.h    | 12 ++++++++++++
 include/uapi/linux/userfaultfd.h | 10 ++++++++--
 mm/userfaultfd.c                 |  9 +++------
 4 files changed, 25 insertions(+), 27 deletions(-)

diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index fa24c72a849e..b74cad206d0a 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -1253,24 +1253,6 @@ static __always_inline int validate_range(struct mm_struct *mm,
 	return 0;
 }

-static inline bool vma_can_userfault(struct vm_area_struct *vma,
-				     unsigned long vm_flags)
-{
-	/* FIXME: add WP support to hugetlbfs and shmem */
-	if (vm_flags & VM_UFFD_WP) {
-		if (is_vm_hugetlb_page(vma) || vma_is_shmem(vma))
-			return false;
-	}
-
-	if (vm_flags & VM_UFFD_MINOR) {
-		if (!(is_vm_hugetlb_page(vma) || vma_is_shmem(vma)))
-			return false;
-	}
-
-	return vma_is_anonymous(vma) || is_vm_hugetlb_page(vma) ||
-	       vma_is_shmem(vma);
-}
-
 static int userfaultfd_register(struct userfaultfd_ctx *ctx,
 				unsigned long arg)
 {
@@ -1949,7 +1931,8 @@ static int userfaultfd_api(struct userfaultfd_ctx *ctx,
 		~(UFFD_FEATURE_MINOR_HUGETLBFS | UFFD_FEATURE_MINOR_SHMEM);
 #endif
 #ifndef CONFIG_HAVE_ARCH_USERFAULTFD_WP
-	uffdio_api.features &= ~UFFD_FEATURE_PAGEFAULT_FLAG_WP;
+	uffdio_api.features &=
+	    ~(UFFD_FEATURE_PAGEFAULT_FLAG_WP | UFFD_FEATURE_WP_HUGETLBFS_SHMEM);
 #endif
 	uffdio_api.ioctls = UFFD_API_IOCTLS;
 	ret = -EFAULT;
diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 05cec02140cb..ef9b70f6447e 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -18,6 +18,7 @@
 #include <linux/swap.h>
 #include <linux/swapops.h>
 #include <asm-generic/pgtable_uffd.h>
+#include <linux/hugetlb_inline.h>

 /* The set of all possible UFFD-related VM flags. */
 #define __VM_UFFD_FLAGS (VM_UFFD_MISSING | VM_UFFD_WP | VM_UFFD_MINOR)
@@ -140,6 +141,17 @@ static inline bool userfaultfd_armed(struct vm_area_struct *vma)
 	return vma->vm_flags & __VM_UFFD_FLAGS;
 }

+static inline bool vma_can_userfault(struct vm_area_struct *vma,
+				     unsigned long vm_flags)
+{
+	if (vm_flags & VM_UFFD_MINOR)
+		return is_vm_hugetlb_page(vma) || vma_is_shmem(vma);
+
+	return vma_is_anonymous(vma) || is_vm_hugetlb_page(vma) ||
+	       vma_is_shmem(vma);
+}
+
+
 extern int dup_userfaultfd(struct vm_area_struct *, struct list_head *);
 extern void dup_userfaultfd_complete(struct list_head *);

diff --git a/include/uapi/linux/userfaultfd.h b/include/uapi/linux/userfaultfd.h
index 05b31d60acf6..a67b5185a7a9 100644
--- a/include/uapi/linux/userfaultfd.h
+++ b/include/uapi/linux/userfaultfd.h
@@ -32,7 +32,8 @@
 			   UFFD_FEATURE_SIGBUS |		\
 			   UFFD_FEATURE_THREAD_ID |		\
 			   UFFD_FEATURE_MINOR_HUGETLBFS |	\
-			   UFFD_FEATURE_MINOR_SHMEM)
+			   UFFD_FEATURE_MINOR_SHMEM |		\
+			   UFFD_FEATURE_WP_HUGETLBFS_SHMEM)
 #define UFFD_API_IOCTLS				\
 	((__u64)1 << _UFFDIO_REGISTER |		\
 	 (__u64)1 << _UFFDIO_UNREGISTER |	\
@@ -46,7 +47,8 @@
 #define UFFD_API_RANGE_IOCTLS_BASIC		\
 	((__u64)1 << _UFFDIO_WAKE |		\
 	 (__u64)1 << _UFFDIO_COPY |		\
-	 (__u64)1 << _UFFDIO_CONTINUE)
+	 (__u64)1 << _UFFDIO_CONTINUE |		\
+	 (__u64)1 << _UFFDIO_WRITEPROTECT)

 /*
  * Valid ioctl command number range with this API is from 0x00 to
@@ -189,6 +191,9 @@ struct uffdio_api {
 	 *
 	 * UFFD_FEATURE_MINOR_SHMEM indicates the same support as
 	 * UFFD_FEATURE_MINOR_HUGETLBFS, but for shmem-backed pages instead.
+	 *
+	 * UFFD_FEATURE_WP_HUGETLBFS_SHMEM indicates that userfaultfd
+	 * write-protection mode is supported on both shmem and hugetlbfs.
 	 */
 #define UFFD_FEATURE_PAGEFAULT_FLAG_WP		(1<<0)
 #define UFFD_FEATURE_EVENT_FORK			(1<<1)
@@ -201,6 +206,7 @@ struct uffdio_api {
 #define UFFD_FEATURE_THREAD_ID			(1<<8)
 #define UFFD_FEATURE_MINOR_HUGETLBFS		(1<<9)
 #define UFFD_FEATURE_MINOR_SHMEM		(1<<10)
+#define UFFD_FEATURE_WP_HUGETLBFS_SHMEM		(1<<11)
 	__u64 features;

 	__u64 ioctls;
diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c
index 037f82719e64..6d8cd9f6b8a1 100644
--- a/mm/userfaultfd.c
+++ b/mm/userfaultfd.c
@@ -716,15 +716,12 @@ int mwriteprotect_range(struct mm_struct *dst_mm, unsigned long start,

 	err = -ENOENT;
 	dst_vma = find_dst_vma(dst_mm, start, len);
-	/*
-	 * Make sure the vma is not shared, that the dst range is
-	 * both valid and fully within a single existing vma.
-	 */
-	if (!dst_vma || (dst_vma->vm_flags & VM_SHARED))
+
+	if (!dst_vma)
 		goto out_unlock;
 	if (!userfaultfd_wp(dst_vma))
 		goto out_unlock;
-	if (!vma_is_anonymous(dst_vma))
+	if (!vma_can_userfault(dst_vma, dst_vma->vm_flags))
 		goto out_unlock;

 	if (is_vm_hugetlb_page(dst_vma)) {
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id A846CC433FE
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:07:00 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 9409763218
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:07:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S236575AbhKOIJw (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:09:52 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:52548 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S235166AbhKOIHE (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:07:04 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963448;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=c0+U3p8s/OABhw6SAh8P2t+DkAujdKbUA9hOXLSGPzE=;
        b=Sg8DmQ5O5dor0Bg+yNDJTVNtybLx/wknLH2r4sydO5MTxF2Zb/j4475Bnao/xeQNnd0yjY
        X1BZlsMOvMOE/CX2/dbF0+Yz5TtLWiV97mbiKbr2dkcBe5NNoIajpt95VoLlorqDG/ZKwH
        StM9KmJfE4J21brfM4sULq51xMpVItE=
Received: from mail-pl1-f200.google.com (mail-pl1-f200.google.com
 [209.85.214.200]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-278-p6i79mn8PTaHvF27TkBkeg-1; Mon, 15 Nov 2021 03:04:07 -0500
X-MC-Unique: p6i79mn8PTaHvF27TkBkeg-1
Received: by mail-pl1-f200.google.com with SMTP id k9-20020a170902ba8900b00141f601d5c8so5865829pls.1
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:04:06 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=c0+U3p8s/OABhw6SAh8P2t+DkAujdKbUA9hOXLSGPzE=;
        b=cV507/MPw4OY4wJ3psOan/EHbEfavAqB/aCw6HI/1guEjX44z9Zwpg8vYYU420J7nm
         7Jv6DplvKGRj+wq95hGKjEzDy1z9DFZfxP2ORIlTjV7sZoimdlMJVtugFXixfc9XWs2n
         T+6sfv59mbn9mmD8Q1uoTcedBHhBBtQ04OcQCT9h6gASItKh/87REMXNjb1ywbU+Xffy
         8i2e1hrQ780Tsx8lO9IEKq93mAoJiyeozyBJwJDLcgDC09npIycZTAFev2FIOIyeQne/
         U5AK+ic/Ouenb59oS64EUNVl+GDg7a8JFLa0tcosJmRXnpdE+QQGObZrp0LrKmqOwBjC
         J3SA==
X-Gm-Message-State: AOAM531y43QUBgRo8KLI0kuGYrNbCAY90EmwXWN6mt+1k3MDmdLdTgvA
        uHDNp1+n42YxGVP096XuanXxv/iihrsSE/VwjR8nRRCVUPLmlF0WisjcMUTxyldc+1uJEi0LzIh
        sjPNKXJ+HnFXC1A/2ANWKADRu
X-Received: by 2002:a17:902:b7cb:b0:141:b33a:9589 with SMTP id v11-20020a170902b7cb00b00141b33a9589mr33256272plz.9.1636963445978;
        Mon, 15 Nov 2021 00:04:05 -0800 (PST)
X-Google-Smtp-Source: ABdhPJyTzcWj62pK1w0Vg8ojQeBIVMs39QWhoC3KUqbkNjXYhPKFdP/63f8P+4EzznoW07+f/8ytsw==
X-Received: by 2002:a17:902:b7cb:b0:141:b33a:9589 with SMTP id v11-20020a170902b7cb00b00141b33a9589mr33256249plz.9.1636963445748;
        Mon, 15 Nov 2021 00:04:05 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id c9sm10948505pgq.58.2021.11.15.00.03.58
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:04:05 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 22/23] mm: Enable PTE markers by default
Date:   Mon, 15 Nov 2021 16:03:53 +0800
Message-Id: <20211115080353.75322-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Enable PTE markers by default.  On x86_64 it means it'll auto-enable
PTE_MARKER_UFFD_WP as well.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 mm/Kconfig | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/mm/Kconfig b/mm/Kconfig
index f01c8e0afadf..401e4dff5f42 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -898,7 +898,7 @@ config SECRETMEM
 	def_bool ARCH_HAS_SET_DIRECT_MAP && !EMBEDDED

 config PTE_MARKER
-	def_bool n
+	def_bool y
 	bool "Marker PTEs support"

 	help
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id C5CCEC433EF
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:07:47 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id A58E863217
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Nov 2021 08:07:47 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S237647AbhKOIK2 (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Nov 2021 03:10:28 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.129.124]:21608 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S236473AbhKOIHR (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Nov 2021 03:07:17 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1636963462;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:
         content-transfer-encoding:content-transfer-encoding:
         in-reply-to:in-reply-to:references:references;
        bh=4fjb0FATaPAZrC5jaWcL/YcRV73dv6wRjJXclwNYe0A=;
        b=TbXrwHecfoJ2dfJpwu8uDbDYDzHyw2fI/seeffoxm+ZO8CaLYilnap+Zz3cWwca0mJCIja
        lboWijRPm6malHDi2LiEbQbCuQQgNlMO0TfmSA+k2oE+DQwvLgNJhzpti5XuOcU0AMluFT
        HoSlTNdJZLu3Ilb0TftL7pp1l71SUi8=
Received: from mail-pj1-f72.google.com (mail-pj1-f72.google.com
 [209.85.216.72]) (Using TLS) by relay.mimecast.com with ESMTP id
 us-mta-109-uW_dCalTMI6C8AQivBJRNg-1; Mon, 15 Nov 2021 03:04:21 -0500
X-MC-Unique: uW_dCalTMI6C8AQivBJRNg-1
Received: by mail-pj1-f72.google.com with SMTP id x18-20020a17090a789200b001a7317f995cso8234480pjk.4
        for <linux-kernel@vger.kernel.org>; Mon, 15 Nov 2021 00:04:20 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=4fjb0FATaPAZrC5jaWcL/YcRV73dv6wRjJXclwNYe0A=;
        b=J1blzv6UFqn6UE60dakNdVSAssK0yu0bpu3RCASHL50QDzaZmXY3oZZQDdlnv521SL
         GjyGPF6xAmp+lhyBTauJWWmq01Dh/Nfv7WzfbayeSHo7bUj2RtHNbKob/Lqh91LW1X7n
         a0iMuL4BCsIXIivbPyGn2F/9LTF3uIpb6ssTwKppBtfsDAGQLflWHPH210sVyjLpMzx3
         rqiXLb3bbMygitkCiU34iIIwjAAgh1q25/s1Vf1+YBL/a699KzpsspzIMYlTCPQH3sBh
         r+IXWpU0L8HsUBtMoJe/U2taU3C6G5BppxAibHOw/lqh1oYABFzAm1h1RrcB6E/ElA5C
         v72Q==
X-Gm-Message-State: AOAM5337RgsdOz5+hyYVwMsHFPO3bNLgw6MjFJzmo8wRbXapMiMZ0XKM
        rVyEhcyIGsPP9l5XqALc/vMprXbKeHF2VRTI6Gx/J4+I7a7vrkgR/s+WXPY+ELxCkPKWPXP64zr
        PqbvE+UFykQ1FjHxRpsaQznTb
X-Received: by 2002:a05:6a00:1ad0:b0:49f:d04e:78da with SMTP id f16-20020a056a001ad000b0049fd04e78damr32260441pfv.77.1636963459928;
        Mon, 15 Nov 2021 00:04:19 -0800 (PST)
X-Google-Smtp-Source: ABdhPJwYov+lN99bqz8/ojCZuJTialWNO5qsxTAc2MyPy6CwRqTAK/GxA7hVywDwqPIq9fwS60hHOg==
X-Received: by 2002:a05:6a00:1ad0:b0:49f:d04e:78da with SMTP id f16-20020a056a001ad000b0049fd04e78damr32260406pfv.77.1636963459693;
        Mon, 15 Nov 2021 00:04:19 -0800 (PST)
Received: from localhost.localdomain ([94.177.118.89])
        by smtp.gmail.com with ESMTPSA id d24sm13594745pfn.62.2021.11.15.00.04.12
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 15 Nov 2021 00:04:19 -0800 (PST)
From:   Peter Xu <peterx@redhat.com>
To:     linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc:     Nadav Amit <nadav.amit@gmail.com>, peterx@redhat.com,
        Alistair Popple <apopple@nvidia.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Matthew Wilcox <willy@infradead.org>,
        Jerome Glisse <jglisse@redhat.com>,
        Axel Rasmussen <axelrasmussen@google.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Hugh Dickins <hughd@google.com>
Subject: [PATCH v6 23/23] selftests/uffd: Enable uffd-wp for shmem/hugetlbfs
Date:   Mon, 15 Nov 2021 16:04:06 +0800
Message-Id: <20211115080406.75377-1-peterx@redhat.com>
X-Mailer: git-send-email 2.32.0
In-Reply-To: <20211115075522.73795-1-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

After we added support for shmem and hugetlbfs, we can turn uffd-wp test on
always now.

Signed-off-by: Peter Xu <peterx@redhat.com>
---
 tools/testing/selftests/vm/userfaultfd.c | 4 +---
 1 file changed, 1 insertion(+), 3 deletions(-)

diff --git a/tools/testing/selftests/vm/userfaultfd.c b/tools/testing/selftests/vm/userfaultfd.c
index 64845be3971d..232cc6083039 100644
--- a/tools/testing/selftests/vm/userfaultfd.c
+++ b/tools/testing/selftests/vm/userfaultfd.c
@@ -81,7 +81,7 @@ static int test_type;
 static volatile bool test_uffdio_copy_eexist = true;
 static volatile bool test_uffdio_zeropage_eexist = true;
 /* Whether to test uffd write-protection */
-static bool test_uffdio_wp = false;
+static bool test_uffdio_wp = true;
 /* Whether to test uffd minor faults */
 static bool test_uffdio_minor = false;

@@ -1588,8 +1588,6 @@ static void set_test_type(const char *type)
 	if (!strcmp(type, "anon")) {
 		test_type = TEST_ANON;
 		uffd_test_ops = &anon_uffd_test_ops;
-		/* Only enable write-protect test for anonymous test */
-		test_uffdio_wp = true;
 	} else if (!strcmp(type, "hugetlb")) {
 		test_type = TEST_HUGETLB;
 		uffd_test_ops = &hugetlb_uffd_test_ops;
--
2.32.0


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 26855C433FE
	for <linux-kernel@archiver.kernel.org>; Fri,  3 Dec 2021 03:30:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1348174AbhLCDdh (ORCPT
        <rfc822;linux-kernel@archiver.kernel.org>);
        Thu, 2 Dec 2021 22:33:37 -0500
Received: from mail-dm6nam11on2050.outbound.protection.outlook.com ([40.107.223.50]:30038
        "EHLO NAM11-DM6-obe.outbound.protection.outlook.com"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1345105AbhLCDdd (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 2 Dec 2021 22:33:33 -0500
ARC-Seal: i=1; a=rsa-sha256; s=arcselector9901; d=microsoft.com; cv=none;
 b=mDjWi2U+g2TtkXCrnY+YyragdBMgrxQUbyG8ABEeqI1/AjoUXzoduYJM/NtuoxU9BGgbOilH9WzxMZjhRmSCMib4Jkkm8cMJkwWmTPZBMXJaySQeEbhJkKiNxnk7k85jK8zSbau8ZMmli+fOEEw7FfawpIwqBjOSMqLzIsZlfZ3zY/ODTmNDuXwcYDFs5ScOyULw+DBElwBIq0PFUJh4McJMAM039n5Sugbf12UiqJbdVUyoJ8dtMMvCfK6ATHv6/mwuymOvaHVRhkwB8uJem1zLMJZNtb0g4wRJ7Kzluq9oaEyEvheCIJKZKxiUUw/6oSHNkhWZWONhrVqsQxp9Dg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=microsoft.com;
 s=arcselector9901;
 h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-AntiSpam-MessageData-ChunkCount:X-MS-Exchange-AntiSpam-MessageData-0:X-MS-Exchange-AntiSpam-MessageData-1;
 bh=b2ss4Jf4fa4k2q9mpKnmYo1chIwyuFjgPaKo42Oyq7s=;
 b=DOen21eXCHzJf0iCuNh/ZdxFYeC5UkUQg9wqsf3NjubFADr4xSmlVJMVSKUnCi3I2x1S1hhk6tNoEzU5b4frkS719J6BD/QiFLzhlY/xnlCJSLxocPFWoBV+e49UmwcVXKPGGReo5WQ3nXHS3IY5+FpZhnPl/dccTLKHoClRACmZ6/MEVE2YhnKUmLosvMoFEv8gUEL1b+qz9j9n6BTHIfS4MVQ5lSqRWBTKMZfbT1A7f4vOHgG+GP/sNajTL/6pZ4N4qEmosCodsYdUOSK9o66AN963vYY2IRQbeAX0Jm6+xJcDfm6jmAjyMdUy+jwqZ8R9HMPeZ7wZIMJDQKv3YA==
ARC-Authentication-Results: i=1; mx.microsoft.com 1; spf=pass (sender ip is
 216.228.112.34) smtp.rcpttodomain=redhat.com smtp.mailfrom=nvidia.com;
 dmarc=pass (p=quarantine sp=quarantine pct=100) action=none
 header.from=nvidia.com; dkim=none (message not signed); arc=none
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=Nvidia.com;
 s=selector2;
 h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-SenderADCheck;
 bh=b2ss4Jf4fa4k2q9mpKnmYo1chIwyuFjgPaKo42Oyq7s=;
 b=Z3LVIYJRQug0L4+tuB6t1BsfA7wuDa3fLCiHEhlERkyu5KLoQzvZ9Rds3yyoSpMdUDPE8V6dxAhv9GjgsWYbZUpcvoSXSXowZsNZ0DbaiT2TfARxCaoQxZa1Zl2iPyeocxV1fCPAuHiPTHGN8BTXKj8v5AWZLSN++PuhTIJQT+aQAQzLrfg6SzfwR5DilXX1SHJ/Fm5vD3OmKb7BJvih37CSMnJ/RGJC0ZW/7YrHrf+dK0y9LeXLjg9Cz7l4nOvq0E9vMs+ex+LT45Sql0G04c2is8BAOOtf2tLC18Ei35ZchDLylinW0a5ihYJPoKg9k03zIRYNYKXWINmpzuvlpg==
Received: from MWHPR13CA0016.namprd13.prod.outlook.com (2603:10b6:300:16::26)
 by MN2PR12MB2894.namprd12.prod.outlook.com (2603:10b6:208:10a::27) with
 Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.4734.23; Fri, 3 Dec
 2021 03:30:08 +0000
Received: from CO1NAM11FT061.eop-nam11.prod.protection.outlook.com
 (2603:10b6:300:16:cafe::f0) by MWHPR13CA0016.outlook.office365.com
 (2603:10b6:300:16::26) with Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.4755.9 via Frontend
 Transport; Fri, 3 Dec 2021 03:30:08 +0000
X-MS-Exchange-Authentication-Results: spf=pass (sender IP is 216.228.112.34)
 smtp.mailfrom=nvidia.com; dkim=none (message not signed)
 header.d=none;dmarc=pass action=none header.from=nvidia.com;
Received-SPF: Pass (protection.outlook.com: domain of nvidia.com designates
 216.228.112.34 as permitted sender) receiver=protection.outlook.com;
 client-ip=216.228.112.34; helo=mail.nvidia.com;
Received: from mail.nvidia.com (216.228.112.34) by
 CO1NAM11FT061.mail.protection.outlook.com (10.13.175.200) with Microsoft SMTP
 Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384) id
 15.20.4755.13 via Frontend Transport; Fri, 3 Dec 2021 03:30:07 +0000
Received: from rnnvmail201.nvidia.com (10.129.68.8) by HQMAIL107.nvidia.com
 (172.20.187.13) with Microsoft SMTP Server (TLS) id 15.0.1497.18; Fri, 3 Dec
 2021 03:30:06 +0000
Received: from nvdebian.localnet (172.20.187.5) by rnnvmail201.nvidia.com
 (10.129.68.8) with Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384) id 15.2.986.9; Thu, 2 Dec 2021
 19:30:03 -0800
From:   Alistair Popple <apopple@nvidia.com>
To:     <linux-mm@kvack.org>, <linux-kernel@vger.kernel.org>,
        Peter Xu <peterx@redhat.com>
CC:     Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Jerome Glisse <jglisse@redhat.com>,
        "Matthew Wilcox" <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>, <peterx@redhat.com>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: Re: [PATCH v6 01/23] mm: Introduce PTE_MARKER swap entry
Date:   Fri, 3 Dec 2021 14:30:00 +1100
Message-ID: <11462319.U46FXHIEPT@nvdebian>
In-Reply-To: <20211115075522.73795-2-peterx@redhat.com>
References: <20211115075522.73795-1-peterx@redhat.com> <20211115075522.73795-2-peterx@redhat.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 7Bit
Content-Type: text/plain; charset="us-ascii"
X-Originating-IP: [172.20.187.5]
X-ClientProxiedBy: HQMAIL107.nvidia.com (172.20.187.13) To
 rnnvmail201.nvidia.com (10.129.68.8)
X-EOPAttributedMessage: 0
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-Correlation-Id: b6a01478-009b-44ed-7988-08d9b60d3465
X-MS-TrafficTypeDiagnostic: MN2PR12MB2894:
X-Microsoft-Antispam-PRVS: <MN2PR12MB2894BA9BAE5E4E08FE5549C7DF6A9@MN2PR12MB2894.namprd12.prod.outlook.com>
X-MS-Oob-TLC-OOBClassifiers: OLM:10000;
X-MS-Exchange-SenderADCheck: 1
X-MS-Exchange-AntiSpam-Relay: 0
X-Microsoft-Antispam: BCL:0;
X-Microsoft-Antispam-Message-Info: U/+1uMJG0vx9FC+m9tJzq6WHoscne/0W0hj6E1cl3vLfbHlDjxg37Ml2DS7qKiaLgHzeVe+5hPxb0KOxIQp/UDm5uRixUEFbQD28yAFNTSGoREddM93t4iyWKyaFeKCM+OB8UJMVdHAUFQvQDHab9R4XdxYcwTsE84e+Rr6YuoecZWagS5aZyPOkeFd+0ISRnm58e4YtPAiTeOtYiSv6Ak/LwX28Xomy7c1F/bIW2vWxTBRDFY7XeDGJzmwsaSnfx2+fFm6YdTtDAsTgPwoqIFrhhtiUrAHL1JR3JVXILY1XsJ5SS6fEgoOB3sW/rXw8kykSf5E1714L1iq9VcXrWBLkbAzVo1a8SJX7kkcs3xVHGsMOsMA4QNX8wmmeN5tco87LAkWTB0mgPzhDDPHTFU5rQjYMhSB1quj4reg7ahF0hn6egdgxwVcYuck9bFyx4Sjwwy95jnMEhDgA8qYu2wchten3lbVb+9r32OGdab3mwoXU5NQTOmcRBegrNu+HJf/Z8jjbDd0liSZtId2eUyYAfKd/EHpyz9S4f69rXJOLxJ00JAWbcc8xD0NQ1bE/ZV8BalhHyD79JNGgevU7R0hfb5fXhJofEwuneBR9wYSSe4rTk432/yVdm70ogx6udPUUIbXB41JMumqi+8gXvZLs02GaZ/HotT2vjibQlY1fRCa1CWa/fC4bnETpHKbbU/SirW6QRe8BinXWL+pNHp5/rHEl5zXTQM8f8lBUTBWrE0yYXI1Qqsz5tdvaI5wGgkk3HLsg2ROkMmZzOwqCJSKHuj74R1wUXaeVzNAf3yA=
X-Forefront-Antispam-Report: CIP:216.228.112.34;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:mail.nvidia.com;PTR:schybrid03.nvidia.com;CAT:NONE;SFS:(4636009)(36840700001)(46966006)(40470700001)(110136005)(33716001)(426003)(9576002)(5660300002)(8676002)(54906003)(26005)(2906002)(33656002)(7416002)(7636003)(70586007)(336012)(508600001)(9686003)(4326008)(16526019)(8936002)(47076005)(70206006)(186003)(86362001)(36860700001)(82310400004)(40460700001)(356005)(316002)(2101003);DIR:OUT;SFP:1101;
X-OriginatorOrg: Nvidia.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 03 Dec 2021 03:30:07.6719
 (UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: b6a01478-009b-44ed-7988-08d9b60d3465
X-MS-Exchange-CrossTenant-Id: 43083d15-7273-40c1-b7db-39efd9ccc17a
X-MS-Exchange-CrossTenant-OriginalAttributedTenantConnectingIp: TenantId=43083d15-7273-40c1-b7db-39efd9ccc17a;Ip=[216.228.112.34];Helo=[mail.nvidia.com]
X-MS-Exchange-CrossTenant-AuthSource: CO1NAM11FT061.eop-nam11.prod.protection.outlook.com
X-MS-Exchange-CrossTenant-AuthAs: Anonymous
X-MS-Exchange-CrossTenant-FromEntityHeader: HybridOnPrem
X-MS-Exchange-Transport-CrossTenantHeadersStamped: MN2PR12MB2894
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Monday, 15 November 2021 6:55:00 PM AEDT Peter Xu wrote:

[...]

> diff --git a/include/linux/swapops.h b/include/linux/swapops.h
> index d356ab4047f7..5103d2a4ae38 100644
> --- a/include/linux/swapops.h
> +++ b/include/linux/swapops.h
> @@ -247,6 +247,84 @@ static inline int is_writable_migration_entry(swp_entry_t entry)
>
>  #endif
>
> +typedef unsigned long pte_marker;
> +
> +#define  PTE_MARKER_MASK     (0)
> +
> +#ifdef CONFIG_PTE_MARKER
> +
> +static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
> +{
> +	return swp_entry(SWP_PTE_MARKER, marker);
> +}
> +
> +static inline bool is_pte_marker_entry(swp_entry_t entry)
> +{
> +	return swp_type(entry) == SWP_PTE_MARKER;
> +}
> +
> +static inline pte_marker pte_marker_get(swp_entry_t entry)
> +{
> +	return swp_offset(entry) & PTE_MARKER_MASK;

I'm not sure the PTE_MARKER_MASK adds much, especially as we only have one
user. I don't see a problem with open-coding these kind of checks (ie.
swp_offset(entry) & PTE_MARKER_UFFD_WP) as you kind of end up doing that anyway.
Alternatively if you want helper functions I think it would be better to define
them for each marker. Eg: is_pte_marker_uffd_wp().

> +}
> +
> +static inline bool is_pte_marker(pte_t pte)
> +{
> +	return is_swap_pte(pte) && is_pte_marker_entry(pte_to_swp_entry(pte));
> +}
> +
> +#else /* CONFIG_PTE_MARKER */
> +
> +static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
> +{
> +	/* This should never be called if !CONFIG_PTE_MARKER */

Can we leave this function undefined then? That way we will get an obvious
build error.

Overall I'm liking the swap entry approach a lot more than the special pte
approach, but maybe that's just because I'm more familiar with special swap
entries :-)

> +	WARN_ON_ONCE(1);
> +	return swp_entry(0, 0);
> +}
> +
> +static inline bool is_pte_marker_entry(swp_entry_t entry)
> +{
> +	return false;
> +}
> +
> +static inline pte_marker pte_marker_get(swp_entry_t entry)
> +{
> +	return 0;
> +}
> +
> +static inline bool is_pte_marker(pte_t pte)
> +{
> +	return false;
> +}
> +
> +#endif /* CONFIG_PTE_MARKER */
> +
> +static inline pte_t make_pte_marker(pte_marker marker)
> +{
> +	return swp_entry_to_pte(make_pte_marker_entry(marker));
> +}
> +
> +/*
> + * This is a special version to check pte_none() just to cover the case when
> + * the pte is a pte marker.  It existed because in many cases the pte marker
> + * should be seen as a none pte; it's just that we have stored some information
> + * onto the none pte so it becomes not-none any more.
> + *
> + * It should be used when the pte is file-backed, ram-based and backing
> + * userspace pages, like shmem.  It is not needed upon pgtables that do not
> + * support pte markers at all.  For example, it's not needed on anonymous
> + * memory, kernel-only memory (including when the system is during-boot),
> + * non-ram based generic file-system.  It's fine to be used even there, but the
> + * extra pte marker check will be pure overhead.
> + *
> + * For systems configured with !CONFIG_PTE_MARKER this will be automatically
> + * optimized to pte_none().
> + */
> +static inline int pte_none_mostly(pte_t pte)
> +{
> +	return pte_none(pte) || is_pte_marker(pte);
> +}
> +
>  static inline struct page *pfn_swap_entry_to_page(swp_entry_t entry)
>  {
>  	struct page *p = pfn_to_page(swp_offset(entry));
> diff --git a/mm/Kconfig b/mm/Kconfig
> index 068ce591a13a..66f23c6c2032 100644
> --- a/mm/Kconfig
> +++ b/mm/Kconfig
> @@ -897,6 +897,13 @@ config IO_MAPPING
>  config SECRETMEM
>  	def_bool ARCH_HAS_SET_DIRECT_MAP && !EMBEDDED
>
> +config PTE_MARKER
> +	def_bool n
> +	bool "Marker PTEs support"
> +
> +	help
> +	  Allows to create marker PTEs for file-backed memory.
> +
>  source "mm/damon/Kconfig"
>
>  endmenu
>





From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 196B3C433F5
	for <linux-kernel@archiver.kernel.org>; Fri,  3 Dec 2021 04:21:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S240141AbhLCEYt (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Thu, 2 Dec 2021 23:24:49 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.129.124]:39349 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S229777AbhLCEYr (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 2 Dec 2021 23:24:47 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1638505284;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:content-type:content-type:
         in-reply-to:in-reply-to:references:references;
        bh=U0DvXToWkjNdJ/hviORYDrqU29CNpTdKjgszLzYXKNA=;
        b=iKb4mh69S+oGa/ProVi3512lpH12IYk+0O886bw9gEz3h9pgLs6sbN1ZhTGA6tEK/2kUFZ
        crf+22x9/HMbrb13VFfg3WVN4ImaSEcjIo9kQkoBKbJePyfqfsZFebeRIeRRrrRz5FRk/z
        ul+Mi++6GHp/i4N9l2bO1Nhyze15LAw=
Received: from mail-wm1-f72.google.com (mail-wm1-f72.google.com
 [209.85.128.72]) by relay.mimecast.com with ESMTP with STARTTLS
 (version=TLSv1.2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id
 us-mta-501-JAKzsUNOO7qZ5VkvW0ux8w-1; Thu, 02 Dec 2021 23:21:22 -0500
X-MC-Unique: JAKzsUNOO7qZ5VkvW0ux8w-1
Received: by mail-wm1-f72.google.com with SMTP id 205-20020a1c00d6000000b003335d1384f1so2789770wma.3
        for <linux-kernel@vger.kernel.org>; Thu, 02 Dec 2021 20:21:22 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:date:from:to:cc:subject:message-id:references
         :mime-version:content-disposition:in-reply-to;
        bh=U0DvXToWkjNdJ/hviORYDrqU29CNpTdKjgszLzYXKNA=;
        b=P/SB64NhHiP2/PW2R5Z5sjUX9q6QPhGx6+YrPr9HgG+DrqxBDbWTMqpGG9XtSGiCS6
         xqhhvtKNcExOwO9d+WXa/W7qa4fXGAY33qF25X3hGHiAGuV6UuMdDjaGjxfL6QyDjyKq
         kB3gkinbY62JHdg0CprLHdWvirPoHivbmtg8isKjH8coG/A2uS3pGDpba7XURn1aIVJh
         nJTkSnBRIHwZF+jTcEk+Fvya38MpT1GURqCtXz9XbW1dYLkI82P0QA+l7LlheASuJxuE
         J+yI5XAkrjCKDoDDcdy016Wik/WwcbOisKKpHHLaTlI+YUOikTFXgLvC/9DV/eD3M9Bx
         Ps7Q==
X-Gm-Message-State: AOAM532nal9+gGY3yfJfPvjT5Gd7+mH5S5Imd+9W9eKkQODOnZ6MMY3t
        UL93pH4X8nrvqCe9P+T/NZA+AM1YFlbZEOO9PnBUzvD8Q67BqJDQW2aOpkNuKX06UoqmZJtCM5s
        VihdHAz4h3RQ2IPFyTk70Esa0
X-Received: by 2002:a7b:c017:: with SMTP id c23mr11778181wmb.137.1638505281697;
        Thu, 02 Dec 2021 20:21:21 -0800 (PST)
X-Google-Smtp-Source: ABdhPJylxWV4mpD2tDaOEiuslxoimzq4PYi2YQm8Z/wgHD2G7olVBGDTmTAVQuBSN9IENsBZdAI1cw==
X-Received: by 2002:a7b:c017:: with SMTP id c23mr11778152wmb.137.1638505281489;
        Thu, 02 Dec 2021 20:21:21 -0800 (PST)
Received: from xz-m1.local ([64.64.123.26])
        by smtp.gmail.com with ESMTPSA id u13sm4241484wmq.14.2021.12.02.20.21.15
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Thu, 02 Dec 2021 20:21:20 -0800 (PST)
Date:   Fri, 3 Dec 2021 12:21:12 +0800
From:   Peter Xu <peterx@redhat.com>
To:     Alistair Popple <apopple@nvidia.com>
Cc:     linux-mm@kvack.org, linux-kernel@vger.kernel.org,
        Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: Re: [PATCH v6 01/23] mm: Introduce PTE_MARKER swap entry
Message-ID: <YambOGGK/K7saiHM@xz-m1.local>
References: <20211115075522.73795-1-peterx@redhat.com>
 <20211115075522.73795-2-peterx@redhat.com>
 <11462319.U46FXHIEPT@nvdebian>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
In-Reply-To: <11462319.U46FXHIEPT@nvdebian>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Fri, Dec 03, 2021 at 02:30:00PM +1100, Alistair Popple wrote:
> On Monday, 15 November 2021 6:55:00 PM AEDT Peter Xu wrote:
>
> [...]
>
> > diff --git a/include/linux/swapops.h b/include/linux/swapops.h
> > index d356ab4047f7..5103d2a4ae38 100644
> > --- a/include/linux/swapops.h
> > +++ b/include/linux/swapops.h
> > @@ -247,6 +247,84 @@ static inline int is_writable_migration_entry(swp_entry_t entry)
> >
> >  #endif
> >
> > +typedef unsigned long pte_marker;
> > +
> > +#define  PTE_MARKER_MASK     (0)
> > +
> > +#ifdef CONFIG_PTE_MARKER
> > +
> > +static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
> > +{
> > +	return swp_entry(SWP_PTE_MARKER, marker);
> > +}
> > +
> > +static inline bool is_pte_marker_entry(swp_entry_t entry)
> > +{
> > +	return swp_type(entry) == SWP_PTE_MARKER;
> > +}
> > +
> > +static inline pte_marker pte_marker_get(swp_entry_t entry)
> > +{
> > +	return swp_offset(entry) & PTE_MARKER_MASK;
>
> I'm not sure the PTE_MARKER_MASK adds much, especially as we only have one
> user. I don't see a problem with open-coding these kind of checks (ie.

It's more or less a safety belt to make sure anything pte_marker_get() returned
will be pte_marker defined bits only.

> swp_offset(entry) & PTE_MARKER_UFFD_WP) as you kind of end up doing that anyway.
> Alternatively if you want helper functions I think it would be better to define
> them for each marker. Eg: is_pte_marker_uffd_wp().

Yes we can have something like is_pte_marker_uffd_wp(), I didn't do that
explicitly because I want us to be clear that pte_marker is a bitmask, so
calling "is_*" will be slightly opaque - strictly speaking it should be
"pte_marker_has_uffd_wp_bit()" if there will be more bits defined, but then the
name of the helper will look a bit odd too.  Hence I just keep the only
interface to fetch the whole marker and use "&" in the call sites to check.

>
> > +}
> > +
> > +static inline bool is_pte_marker(pte_t pte)
> > +{
> > +	return is_swap_pte(pte) && is_pte_marker_entry(pte_to_swp_entry(pte));
> > +}
> > +
> > +#else /* CONFIG_PTE_MARKER */
> > +
> > +static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
> > +{
> > +	/* This should never be called if !CONFIG_PTE_MARKER */
>
> Can we leave this function undefined then? That way we will get an obvious
> build error.

We can, but then we need more macros to cover the common code.  E.g. currently
in hugetlb_change_protection() we have:

        /* None pte */
        if (unlikely(uffd_wp))
                /* Safe to modify directly (none->non-present). */
                set_huge_pte_at(mm, address, ptep,
                                make_pte_marker(PTE_MARKER_UFFD_WP));

If we drop this definition, to let it compile with !PTE_MARKER, we'll need:

+#ifdef PTE_MARKER
        /* None pte */
        if (unlikely(uffd_wp))
                /* Safe to modify directly (none->non-present). */
                set_huge_pte_at(mm, address, ptep,
                                make_pte_marker(PTE_MARKER_UFFD_WP));
+#endif

Comparing to adding macro checks over a few other places, I figured maybe it's
easier to define them in the header once then we proper WARN_ON_ONCE() if
triggered (while they should just never).

>
> Overall I'm liking the swap entry approach a lot more than the special pte
> approach, but maybe that's just because I'm more familiar with special swap
> entries :-)

Swap entry solution is definitely cleaner to me if not considering wasting it
with one bit.

Operating on pte directly is actually slightly more challenging, because we
don't have the protection of is_swap_pte() anymore.  It can help shield out
quite some strange stuff due to the pte->swp level hierachy.

Thanks,

--
Peter Xu


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 9E881C433F5
	for <linux-kernel@archiver.kernel.org>; Fri,  3 Dec 2021 05:35:51 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1378509AbhLCFjO (ORCPT
        <rfc822;linux-kernel@archiver.kernel.org>);
        Fri, 3 Dec 2021 00:39:14 -0500
Received: from mail-bn8nam08on2057.outbound.protection.outlook.com ([40.107.100.57]:62335
        "EHLO NAM04-BN8-obe.outbound.protection.outlook.com"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1378490AbhLCFjL (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 3 Dec 2021 00:39:11 -0500
ARC-Seal: i=1; a=rsa-sha256; s=arcselector9901; d=microsoft.com; cv=none;
 b=h6f+32FXd0ZOIwbEQKXzs9cImJ/sCfBVR2vKSKplDWPqPVV8YK9H3kPjbabtPG1DQhzhI4xBtolxNT+94656sRF4fzIAKc9iZd+SX/JWpjp0NB9yEM0kdtRt9IZ921tk6E5JmSWaPCc2CTJ2DnXGkAJoUmWj1h5WDKM2ADVpeDnx8EFHk/nWrb/5h3XxLma2PyfMLs/JWTRCXy5cle+9MYg71yLZCgf+Zds+aAGYsypYxbKpVZ+XrCbsIi21rfWHaVHvYX3f+g1BhBAoPmZxYPyayr3JeTGq2ulSjNaOc9mBDDVAlxYDyLxpD2Dd6Rzq7xDJ73V1Kds/od5aPjDw6Q==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=microsoft.com;
 s=arcselector9901;
 h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-AntiSpam-MessageData-ChunkCount:X-MS-Exchange-AntiSpam-MessageData-0:X-MS-Exchange-AntiSpam-MessageData-1;
 bh=L6Tvs+yqSAQ519e+yGcETckQAOarcl+pD1wVIQFPauE=;
 b=EVc95BNhQwFU385GqsApcv1qgFOnhY2mzm8jFYmPjznIVGAuwIutwdwr+XLqMmcBFe2sVGttEkDK984Xvh56HcjWZHKPJylbbeYMI8NItAIhU1SfO5JyHb90I1wxls3o+DgDiCJJx8/aNpBL7UaEm9LzY2FaZR0Ro+PV5JJ+rydDZZt+VKeIKl/sRrcGSditerDjGvIJdiNF1y32OKNOPtkcL0bU5Dbpx+ZnTV+Wijk5sXAPuU8N3Legjup/XB8Wi2E2yXFul+MQtgGkPuqs3iPBb1IUYX5gDgxXK3cxWhe4LGNFvq6t8bexCPYAcnTDrqW6G6O/wCB4PEZ0eypXpw==
ARC-Authentication-Results: i=1; mx.microsoft.com 1; spf=pass (sender ip is
 216.228.112.35) smtp.rcpttodomain=redhat.com smtp.mailfrom=nvidia.com;
 dmarc=pass (p=quarantine sp=quarantine pct=100) action=none
 header.from=nvidia.com; dkim=none (message not signed); arc=none
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=Nvidia.com;
 s=selector2;
 h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-SenderADCheck;
 bh=L6Tvs+yqSAQ519e+yGcETckQAOarcl+pD1wVIQFPauE=;
 b=sMVCtWqMo1/5EhrojnDjsVFVrjQgEoK0uJd5D5LKcNLK8QzClG9EJgepiCqkh0cvf5yF2ZlUt0Rnrnw6J85tTwAZHC8az261SlQVON9s3n/fMEfi+0Ez+OFafwf8AsENH2INyzVLc3/+Zg3SBjxRq/1gxc4bcO65bzm8NbLG91fjpvmSPFZniRg4wywDIiJJhWpg3VOTcgklPhY0Zj0NJnuECNSnF81hnDctu4LVkBSplVME9QkQQWsk2vYkY1qQakONS2iHO5kerLMHvzmZPZ8HsNmE5pOrpfIhm3VJRwh6+PV0+M+zOqBM4Tz8g/5nkJnzxgT8Pdq1hhoFp8XqXg==
Received: from CO2PR04CA0188.namprd04.prod.outlook.com (2603:10b6:104:5::18)
 by DM5PR12MB1900.namprd12.prod.outlook.com (2603:10b6:3:10f::18) with
 Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.4734.23; Fri, 3 Dec
 2021 05:35:44 +0000
Received: from CO1NAM11FT022.eop-nam11.prod.protection.outlook.com
 (2603:10b6:104:5:cafe::3d) by CO2PR04CA0188.outlook.office365.com
 (2603:10b6:104:5::18) with Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.4755.11 via Frontend
 Transport; Fri, 3 Dec 2021 05:35:44 +0000
X-MS-Exchange-Authentication-Results: spf=pass (sender IP is 216.228.112.35)
 smtp.mailfrom=nvidia.com; dkim=none (message not signed)
 header.d=none;dmarc=pass action=none header.from=nvidia.com;
Received-SPF: Pass (protection.outlook.com: domain of nvidia.com designates
 216.228.112.35 as permitted sender) receiver=protection.outlook.com;
 client-ip=216.228.112.35; helo=mail.nvidia.com;
Received: from mail.nvidia.com (216.228.112.35) by
 CO1NAM11FT022.mail.protection.outlook.com (10.13.175.199) with Microsoft SMTP
 Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384) id
 15.20.4755.13 via Frontend Transport; Fri, 3 Dec 2021 05:35:44 +0000
Received: from rnnvmail201.nvidia.com (10.129.68.8) by HQMAIL111.nvidia.com
 (172.20.187.18) with Microsoft SMTP Server (TLS) id 15.0.1497.18; Fri, 3 Dec
 2021 05:35:43 +0000
Received: from nvdebian.localnet (172.20.187.5) by rnnvmail201.nvidia.com
 (10.129.68.8) with Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384) id 15.2.986.9; Thu, 2 Dec 2021
 21:35:40 -0800
From:   Alistair Popple <apopple@nvidia.com>
To:     Peter Xu <peterx@redhat.com>
CC:     <linux-mm@kvack.org>, <linux-kernel@vger.kernel.org>,
        Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: Re: [PATCH v6 01/23] mm: Introduce PTE_MARKER swap entry
Date:   Fri, 3 Dec 2021 16:35:38 +1100
Message-ID: <3832555.7SGzcYD3YQ@nvdebian>
In-Reply-To: <YambOGGK/K7saiHM@xz-m1.local>
References: <20211115075522.73795-1-peterx@redhat.com> <11462319.U46FXHIEPT@nvdebian> <YambOGGK/K7saiHM@xz-m1.local>
MIME-Version: 1.0
Content-Transfer-Encoding: 7Bit
Content-Type: text/plain; charset="us-ascii"
X-Originating-IP: [172.20.187.5]
X-ClientProxiedBy: HQMAIL101.nvidia.com (172.20.187.10) To
 rnnvmail201.nvidia.com (10.129.68.8)
X-EOPAttributedMessage: 0
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-Correlation-Id: 2215670e-a580-4e93-7e60-08d9b61ec06a
X-MS-TrafficTypeDiagnostic: DM5PR12MB1900:
X-Microsoft-Antispam-PRVS: <DM5PR12MB1900CA892D83773CA103D42FDF6A9@DM5PR12MB1900.namprd12.prod.outlook.com>
X-MS-Oob-TLC-OOBClassifiers: OLM:10000;
X-MS-Exchange-SenderADCheck: 1
X-MS-Exchange-AntiSpam-Relay: 0
X-Microsoft-Antispam: BCL:0;
X-Microsoft-Antispam-Message-Info: Ygp7fhs9hztPUaGxNRr0WNESfTFxgdOodxBT6EH32ujL65SCO7kWXz6OVvslwJefFaXqWiAoW5yGgz8sVam/WdXblv29NFtx+jczrmMrQI+Gx5eYGPCsg5kNE7eyrfHTOopq0X9Xp3gwOsBmu8abS5YgQGeK0JlBt2ZTcZpmI8Uv+lXSLFxnmb/4kdRlvmTcqTWyKqdsMYYuA8ymBCbrGagGNwaMW9w7ssD6KjZ33uY+i8SiSZPM3ozL1UGPABXqFCH3PJYRvaEIBShaGEr998GAoViUVW3e+NQsEJu0vILLHhCmX+xOHHM2u6mJCWcZsQkDjov0pc8/F4fvkY/f4O1q8ceSXqxtvnMLEQU7HSZIw81PyGKI7WhQbxSUzy77qxjNlXFPQiNABtwJzAGZDM14vwsEooU6Ku+jmNPAJcsHrPxPQt3qKS1/AMVTJQMeV3d37x8G0w4zYM2AL+uvxpXk361JbuJbiTK1Z6dapjsXa00b2kjQRCkipriSSQnDnAvnYo34IwgSheRC4BeOMvM3HwI08VV8OPmRxY2irrOX2n/2SgESYZ8tX3fBHvp99hM3I774CmTxdnM2PTsq8abKop2M9HtjvIzrSgH7V5pgTRZAeVkKW4DkresDpTasnQ+YS2fqDy4zSwWriifX3y+9YYqhxGvjmN+mfG7R0mvCkASQ4lBGPaWqGibxGIlAO/3RvKhojrxnDtldAIlRYv2oeJpJAbmY12VTcLw3RiuZxJkD6Zr7fr5rnTQj3IWtCMemTwL6+wJMfL4IQ9PjH0CsflERz7i17u4ryzNNtMY=
X-Forefront-Antispam-Report: CIP:216.228.112.35;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:mail.nvidia.com;PTR:schybrid02.nvidia.com;CAT:NONE;SFS:(4636009)(46966006)(36840700001)(40470700001)(47076005)(6916009)(5660300002)(33716001)(4326008)(86362001)(9576002)(8676002)(508600001)(70206006)(7636003)(16526019)(83380400001)(356005)(186003)(70586007)(36860700001)(336012)(426003)(7416002)(8936002)(26005)(2906002)(54906003)(316002)(9686003)(82310400004)(40460700001)(39026012);DIR:OUT;SFP:1101;
X-OriginatorOrg: Nvidia.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 03 Dec 2021 05:35:44.2020
 (UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 2215670e-a580-4e93-7e60-08d9b61ec06a
X-MS-Exchange-CrossTenant-Id: 43083d15-7273-40c1-b7db-39efd9ccc17a
X-MS-Exchange-CrossTenant-OriginalAttributedTenantConnectingIp: TenantId=43083d15-7273-40c1-b7db-39efd9ccc17a;Ip=[216.228.112.35];Helo=[mail.nvidia.com]
X-MS-Exchange-CrossTenant-AuthSource: CO1NAM11FT022.eop-nam11.prod.protection.outlook.com
X-MS-Exchange-CrossTenant-AuthAs: Anonymous
X-MS-Exchange-CrossTenant-FromEntityHeader: HybridOnPrem
X-MS-Exchange-Transport-CrossTenantHeadersStamped: DM5PR12MB1900
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Friday, 3 December 2021 3:21:12 PM AEDT Peter Xu wrote:
> On Fri, Dec 03, 2021 at 02:30:00PM +1100, Alistair Popple wrote:
> > On Monday, 15 November 2021 6:55:00 PM AEDT Peter Xu wrote:
> >
> > [...]
> >
> > > diff --git a/include/linux/swapops.h b/include/linux/swapops.h
> > > index d356ab4047f7..5103d2a4ae38 100644
> > > --- a/include/linux/swapops.h
> > > +++ b/include/linux/swapops.h
> > > @@ -247,6 +247,84 @@ static inline int is_writable_migration_entry(swp_entry_t entry)
> > >
> > >  #endif
> > >
> > > +typedef unsigned long pte_marker;
> > > +
> > > +#define  PTE_MARKER_MASK     (0)
> > > +
> > > +#ifdef CONFIG_PTE_MARKER
> > > +
> > > +static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
> > > +{
> > > +	return swp_entry(SWP_PTE_MARKER, marker);
> > > +}
> > > +
> > > +static inline bool is_pte_marker_entry(swp_entry_t entry)
> > > +{
> > > +	return swp_type(entry) == SWP_PTE_MARKER;
> > > +}
> > > +
> > > +static inline pte_marker pte_marker_get(swp_entry_t entry)
> > > +{
> > > +	return swp_offset(entry) & PTE_MARKER_MASK;
> >
> > I'm not sure the PTE_MARKER_MASK adds much, especially as we only have one
> > user. I don't see a problem with open-coding these kind of checks (ie.
>
> It's more or less a safety belt to make sure anything pte_marker_get() returned
> will be pte_marker defined bits only.
>
> > swp_offset(entry) & PTE_MARKER_UFFD_WP) as you kind of end up doing that anyway.
> > Alternatively if you want helper functions I think it would be better to define
> > them for each marker. Eg: is_pte_marker_uffd_wp().
>
> Yes we can have something like is_pte_marker_uffd_wp(), I didn't do that
> explicitly because I want us to be clear that pte_marker is a bitmask, so
> calling "is_*" will be slightly opaque - strictly speaking it should be
> "pte_marker_has_uffd_wp_bit()" if there will be more bits defined, but then the
> name of the helper will look a bit odd too.  Hence I just keep the only
> interface to fetch the whole marker and use "&" in the call sites to check.

Why does a caller need to care if it's a bitmask or not though? Isn't that an
implementation detail that could be left to the "is_*" functions? I must admit
I'm still working through the rest of this series though - is it because you
end up storing some kind of value in the upper bits of the PTE marker?

> >
> > > +}
> > > +
> > > +static inline bool is_pte_marker(pte_t pte)
> > > +{
> > > +	return is_swap_pte(pte) && is_pte_marker_entry(pte_to_swp_entry(pte));
> > > +}
> > > +
> > > +#else /* CONFIG_PTE_MARKER */
> > > +
> > > +static inline swp_entry_t make_pte_marker_entry(pte_marker marker)
> > > +{
> > > +	/* This should never be called if !CONFIG_PTE_MARKER */
> >
> > Can we leave this function undefined then? That way we will get an obvious
> > build error.
>
> We can, but then we need more macros to cover the common code.  E.g. currently
> in hugetlb_change_protection() we have:
>
>         /* None pte */
>         if (unlikely(uffd_wp))
>                 /* Safe to modify directly (none->non-present). */
>                 set_huge_pte_at(mm, address, ptep,
>                                 make_pte_marker(PTE_MARKER_UFFD_WP));
>
> If we drop this definition, to let it compile with !PTE_MARKER, we'll need:
>
> +#ifdef PTE_MARKER
>         /* None pte */
>         if (unlikely(uffd_wp))
>                 /* Safe to modify directly (none->non-present). */
>                 set_huge_pte_at(mm, address, ptep,
>                                 make_pte_marker(PTE_MARKER_UFFD_WP));
> +#endif
>
> Comparing to adding macro checks over a few other places, I figured maybe it's
> easier to define them in the header once then we proper WARN_ON_ONCE() if
> triggered (while they should just never).

Ok, makes sense. Agree that adding macro checks everywhere isn't great.

> >
> > Overall I'm liking the swap entry approach a lot more than the special pte
> > approach, but maybe that's just because I'm more familiar with special swap
> > entries :-)
>
> Swap entry solution is definitely cleaner to me if not considering wasting it
> with one bit.
>
> Operating on pte directly is actually slightly more challenging, because we
> don't have the protection of is_swap_pte() anymore.  It can help shield out
> quite some strange stuff due to the pte->swp level hierachy.

So I guess now we have the protection of is_swap_pte() there are probably a few
places where we need to check for marker pte entries when we find swap entries.
I'm not suggesting you haven't already found all of those cases of course, just
noting that it's something to review.

> Thanks,
>
>





From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 39DD3C433F5
	for <linux-kernel@archiver.kernel.org>; Fri,  3 Dec 2021 06:45:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1378606AbhLCGtO (ORCPT
        <rfc822;linux-kernel@archiver.kernel.org>);
        Fri, 3 Dec 2021 01:49:14 -0500
Received: from us-smtp-delivery-124.mimecast.com ([170.10.133.124]:37335 "EHLO
        us-smtp-delivery-124.mimecast.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S234449AbhLCGtN (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 3 Dec 2021 01:49:13 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=redhat.com;
        s=mimecast20190719; t=1638513949;
        h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
         to:to:cc:cc:mime-version:mime-version:content-type:content-type:
         in-reply-to:in-reply-to:references:references;
        bh=B0n7JcNAcYn+Bd11okNue8IqT9XrfBBrxPPRZCGg20A=;
        b=XZZGPtpeyVaHcfgnJq0Wqoq3RYIvLKVFTIdJ/z8yZScgR9zIHi1fbArBEv2vkKt7ovKHmT
        CqphCpcl9Yy1XOml9tIqTZJ+3U+ub/fOsYP+JqPS6JmUe11ryukRBw3BWkTqtS8AVouMys
        TMUlA0W7TccY/YnpbjuPWF44jmcnEzQ=
Received: from mail-wm1-f72.google.com (mail-wm1-f72.google.com
 [209.85.128.72]) by relay.mimecast.com with ESMTP with STARTTLS
 (version=TLSv1.2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id
 us-mta-314-jaBjDCGaNaqBb7bqisk7oA-1; Fri, 03 Dec 2021 01:45:48 -0500
X-MC-Unique: jaBjDCGaNaqBb7bqisk7oA-1
Received: by mail-wm1-f72.google.com with SMTP id g11-20020a1c200b000000b003320d092d08so1021244wmg.9
        for <linux-kernel@vger.kernel.org>; Thu, 02 Dec 2021 22:45:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:date:from:to:cc:subject:message-id:references
         :mime-version:content-disposition:in-reply-to;
        bh=B0n7JcNAcYn+Bd11okNue8IqT9XrfBBrxPPRZCGg20A=;
        b=v/C8tm9WIbTJSzzg23p55igBHfW/wwx0Fd4LYvonsorgPk60rwRsScphCdoaKQrpci
         jNwt7qeFOQ8+Jlu+qBv5eJstqqcx4l6XZzherSuVsNL9zJah7dcYOYXRRjIxlPqJTnVt
         3jj4UPshavF87/vQ/FtNiDaEqoaVp513xU+H1btvEVJM907yZ3LPjx2lfnNhRP/UyOdc
         N11VoAfLCj1FSrj7JETuFhMFAXRh5TLfoYGq2SvmoaF6T30IQcDkNCoV8Uft6jljQ+Eg
         WF9Hcfk7I9WoIQhOh241o2z6tik2Cmu82fDS2ZXN2vV2Ku1vydKt3yA0Vd+n0M4A5d+u
         aTzA==
X-Gm-Message-State: AOAM532S3+2sIufplxLlj5ikKtyBXQJgcBXnUpwBQXzoz1SDxXXANeC7
        C2JExqyGeAuuksJ+AwLjISDxKEomJXj0KLSrtxyUxDKfnfB6rGlgIR6LX1/9HllK+y1NbkiTqMb
        vBLkimKcUq3HtLVb0VMIdmQ/j
X-Received: by 2002:a05:600c:d0:: with SMTP id u16mr12211152wmm.7.1638513946725;
        Thu, 02 Dec 2021 22:45:46 -0800 (PST)
X-Google-Smtp-Source: ABdhPJw4QxLQdwPZWAy1sF4nI2kLftVNVchu51EysUwQMm50HtRmJU/m6AC98u7ekwAGwe+6P2l5Zg==
X-Received: by 2002:a05:600c:d0:: with SMTP id u16mr12211119wmm.7.1638513946428;
        Thu, 02 Dec 2021 22:45:46 -0800 (PST)
Received: from xz-m1.local ([64.64.123.26])
        by smtp.gmail.com with ESMTPSA id o9sm1810290wrs.4.2021.12.02.22.45.40
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Thu, 02 Dec 2021 22:45:46 -0800 (PST)
Date:   Fri, 3 Dec 2021 14:45:37 +0800
From:   Peter Xu <peterx@redhat.com>
To:     Alistair Popple <apopple@nvidia.com>
Cc:     linux-mm@kvack.org, linux-kernel@vger.kernel.org,
        Axel Rasmussen <axelrasmussen@google.com>,
        Nadav Amit <nadav.amit@gmail.com>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        Hugh Dickins <hughd@google.com>,
        Mike Kravetz <mike.kravetz@oracle.com>,
        "Kirill A . Shutemov" <kirill@shutemov.name>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Andrew Morton <akpm@linux-foundation.org>,
        David Hildenbrand <david@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: Re: [PATCH v6 01/23] mm: Introduce PTE_MARKER swap entry
Message-ID: <Yam9EezLTANRA+Rf@xz-m1.local>
References: <20211115075522.73795-1-peterx@redhat.com>
 <11462319.U46FXHIEPT@nvdebian>
 <YambOGGK/K7saiHM@xz-m1.local>
 <3832555.7SGzcYD3YQ@nvdebian>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
In-Reply-To: <3832555.7SGzcYD3YQ@nvdebian>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Fri, Dec 03, 2021 at 04:35:38PM +1100, Alistair Popple wrote:
> > > > +static inline pte_marker pte_marker_get(swp_entry_t entry)
> > > > +{
> > > > +	return swp_offset(entry) & PTE_MARKER_MASK;
> > >
> > > I'm not sure the PTE_MARKER_MASK adds much, especially as we only have one
> > > user. I don't see a problem with open-coding these kind of checks (ie.
> >
> > It's more or less a safety belt to make sure anything pte_marker_get() returned
> > will be pte_marker defined bits only.
> >
> > > swp_offset(entry) & PTE_MARKER_UFFD_WP) as you kind of end up doing that anyway.
> > > Alternatively if you want helper functions I think it would be better to define
> > > them for each marker. Eg: is_pte_marker_uffd_wp().
> >
> > Yes we can have something like is_pte_marker_uffd_wp(), I didn't do that
> > explicitly because I want us to be clear that pte_marker is a bitmask, so
> > calling "is_*" will be slightly opaque - strictly speaking it should be
> > "pte_marker_has_uffd_wp_bit()" if there will be more bits defined, but then the
> > name of the helper will look a bit odd too.  Hence I just keep the only
> > interface to fetch the whole marker and use "&" in the call sites to check.
>
> Why does a caller need to care if it's a bitmask or not though? Isn't that an
> implementation detail that could be left to the "is_*" functions? I must admit
> I'm still working through the rest of this series though - is it because you
> end up storing some kind of value in the upper bits of the PTE marker?

Nop. I'm just afraid the caller could overlook the fact that it's a bitmask,
then there can be code like:

  if (is_pte_marker_uffd_wp(*ptep) && drop_uffd_wp)
      pte_clear(ptep)

While we should only do:

  if (is_pte_marker_uffd_wp(*ptep) && drop_uffd_wp)
      // remove uffd-wp bit in the pte_marker, keep the reset bitmask

I could be worrying too much, there's no real user of it.  If you prefer the
helper a lot I can add it in the new version.  Thanks,

--
Peter Xu


