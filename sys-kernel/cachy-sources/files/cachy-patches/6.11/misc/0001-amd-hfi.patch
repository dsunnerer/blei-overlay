From 80301f181d59d7040f5a73e67e2f13c5be5eb935 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Tue, 27 Aug 2024 16:21:53 +0200
Subject: [PATCH] amd-hfi

Signed-off-by: Peter Jung <admin@ptr1337.dev>
---
 Documentation/arch/x86/amd-hfi.rst    | 116 +++++
 Documentation/arch/x86/index.rst      |   1 +
 MAINTAINERS                           |   9 +
 arch/x86/include/asm/cpufeatures.h    |   2 +
 arch/x86/include/asm/hreset.h         |   6 +
 arch/x86/include/asm/msr-index.h      |   5 +
 arch/x86/kernel/cpu/common.c          |  18 +
 arch/x86/kernel/cpu/scattered.c       |   2 +
 arch/x86/kernel/process_32.c          |   3 +
 arch/x86/kernel/process_64.c          |   3 +
 arch/x86/kernel/smpboot.c             |   3 +-
 drivers/platform/x86/amd/Kconfig      |   1 +
 drivers/platform/x86/amd/Makefile     |   1 +
 drivers/platform/x86/amd/hfi/Kconfig  |  21 +
 drivers/platform/x86/amd/hfi/Makefile |   7 +
 drivers/platform/x86/amd/hfi/hfi.c    | 665 ++++++++++++++++++++++++++
 16 files changed, 862 insertions(+), 1 deletion(-)
 create mode 100644 Documentation/arch/x86/amd-hfi.rst
 create mode 100644 arch/x86/include/asm/hreset.h
 create mode 100644 drivers/platform/x86/amd/hfi/Kconfig
 create mode 100644 drivers/platform/x86/amd/hfi/Makefile
 create mode 100644 drivers/platform/x86/amd/hfi/hfi.c

diff --git a/Documentation/arch/x86/amd-hfi.rst b/Documentation/arch/x86/amd-hfi.rst
new file mode 100644
index 0000000000000..351641ce28213
--- /dev/null
+++ b/Documentation/arch/x86/amd-hfi.rst
@@ -0,0 +1,116 @@
+.. SPDX-License-Identifier: GPL-2.0
+
+======================================================================
+Hardware Feedback Interface For Hetero Core Scheduling On AMD Platform
+======================================================================
+
+:Copyright (C) 2024 Advanced Micro Devices, Inc. All Rights Reserved.
+
+:Author: Perry Yuan <perry.yuan@amd.com>
+
+Overview
+--------
+
+AMD Heterogeneous Core implementations are comprised of more than one
+architectural class and CPUs are comprised of cores of various efficiency
+and power capabilities. Power management strategies must be designed to accommodate
+the complexities introduced by incorporating different core types.
+Heterogeneous systems can also extend to more than two architectural classes as well.
+The purpose of the scheduling feedback mechanism is to provide information to
+the operating system scheduler in real time such that the scheduler can direct
+threads to the optimal core.
+
+``Classic cores`` are generally more performant and ``Dense cores`` are generally more
+efficient.
+The goal of AMD's heterogeneous architecture is to attain power benefit by sending
+background thread to the dense cores while sending high priority threads to the classic
+cores. From a performance perspective, sending background threads to dense cores can free
+up power headroom and allow the classic cores to optimally service demanding threads.
+Furthermore, the area optimized nature of the dense cores allows for an increasing
+number of physical cores. This improved core density will have positive multithreaded
+performance impact.
+
+AMD Heterogeneous Core Driver
+-----------------------------
+
+The ``amd_hfi`` driver delivers the operating system a performance and energy efficiency
+capability data for each CPU in the system. The scheduler can use the ranking data
+from the HFI driver to make task placement decisions.
+
+Thread Classification and Ranking Table Interaction
+----------------------------------------------------
+
+The thread classification is used to select into a ranking table that describes
+an efficiency and performance ranking for each classification.
+
+Threads are classified during runtime into enumerated classes. The classes represent
+thread performance/power characteristics that may benefit from special scheduling behaviors.
+The below table depicts an example of thread classification and a preference where a given thread
+should be scheduled based on its thread class. The real time thread classification is consumed
+by the operating system and is used to inform the scheduler of where the thread should be placed.
+
+Thread Classification Example Table
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
++----------+----------------+-------------------------------+---------------------+---------+
+| class ID | Classification | Preferred scheduling behavior | Preemption priority | Counter |
++----------+----------------+-------------------------------+---------------------+---------+
+| 0        | Default        | Performant                    | Highest             |         |
++----------+----------------+-------------------------------+---------------------+---------+
+| 1        | Non-scalable   | Efficient                     | Lowest              | PMCx1A1 |
++----------+----------------+-------------------------------+---------------------+---------+
+| 2        | I/O bound      | Efficient                     | Lowest              | PMCx044 |
++----------+----------------+-------------------------------+---------------------+---------+
+
+
+AMD Hardware Feedback Interface
+--------------------------------
+
+The Hardware Feedback Interface provides to the operating system information
+about the performance and energy efficiency of each CPU in the system. Each
+capability is given as a unit-less quantity in the range [0-255]. A higher
+performance value indicates higher performance capability, and a higher
+efficiency value indicates more efficiency. Energy efficiency and performance
+are reported in separate capabilities in the shared memory based ranking table.
+
+These capabilities may change at runtime as a result of changes in the
+operating conditions of the system or the action of external factors.
+Power Management FW is responsible for detecting events that would require
+a reordering of the performance and efficiency ranking. Table updates would
+happen relatively infrequently and occur on the time scale of seconds or more.
+
+The mechanism used to trigger a table update like below events:
+    * Thermal Stress Events
+    * Silent Compute
+    * Extreme Low Battery Scenarios
+
+The kernel or a userspace policy daemon can use these capabilities to modify
+task placement decisions. For instance, if either the performance or energy
+capabilities of a given logical processor becomes zero, it is an indication that
+the hardware recommends to the operating system to not schedule any tasks on
+that processor for performance or energy efficiency reasons, respectively.
+
+Implementation details for Linux
+--------------------------------
+
+The implementation of threads scheduling consists of the following steps:
+
+1. A thread is spawned and scheduled to the ideal core using the default
+   heterogeneous scheduling policy.
+2. The processor profiles thread execution and assigns an enumerated classification ID.
+   This classification is communicated to the OS via logical processor scope MSR.
+3. During the thread context switch out the operating system consumes the workload(WL)
+   classification which resides in a logical processor scope MSR.
+4. The OS triggers the hardware to clear its history by writing to an MSR,
+   after consuming the WL classification and before switching in the new thread.
+5. If due to the classification, ranking table, and processor availability,
+   the thread is not on its ideal processor, the OS will then consider scheduling
+   the thread on its ideal processor (if available).
+
+Ranking Table update
+---------------------------
+The power management firmware issues an platform interrupt after updating the ranking
+table and is ready for the operating system to consume it. CPUs receive such interrupt
+and read new ranking table from shared memory which PCCT table has provided, then
+``amd_hfi`` driver parse the new table to provide new consume data for scheduling decisions.
+
+
diff --git a/Documentation/arch/x86/index.rst b/Documentation/arch/x86/index.rst
index 8ac64d7de4dc9..7f47229f3104e 100644
--- a/Documentation/arch/x86/index.rst
+++ b/Documentation/arch/x86/index.rst
@@ -43,3 +43,4 @@ x86-specific Documentation
    features
    elf_auxvec
    xstate
+   amd_hfi
diff --git a/MAINTAINERS b/MAINTAINERS
index f1a4df03b2bfa..d0ddef1365f02 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -1052,6 +1052,15 @@ F:	arch/x86/include/asm/amd_hsmp.h
 F:	arch/x86/include/uapi/asm/amd_hsmp.h
 F:	drivers/platform/x86/amd/hsmp.c
 
+AMD HETERO CORE HARDWARE FEEDBACK DRIVER
+M:	Perry Yuan <perry.yuan@amd.com>
+M:	Mario Limonciello <mario.limonciello@amd.com>
+L:	platform-driver-x86@vger.kernel.org
+S:	Supported
+B:	https://gitlab.freedesktop.org/drm/amd/-/issues
+F:	drivers/platform/x86/amd/hfi/
+F:	Documentation/arch/x86/amd-hfi.rst
+
 AMD IOMMU (AMD-VI)
 M:	Joerg Roedel <joro@8bytes.org>
 R:	Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h
index dd4682857c120..f29856ae9e558 100644
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@ -473,6 +473,8 @@
 #define X86_FEATURE_CLEAR_BHB_HW	(21*32+ 3) /* BHI_DIS_S HW control enabled */
 #define X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT (21*32+ 4) /* Clear branch history at vmexit using SW loop */
 #define X86_FEATURE_FAST_CPPC		(21*32 + 5) /* AMD Fast CPPC */
+#define X86_FEATURE_HETERO_CORE_TOPOLOGY       (21*32+ 6) /* "" Heterogeneous Core Topology */
+#define X86_FEATURE_WORKLOAD_CLASS		(21*32+ 7) /* Workload Classification */
 
 /*
  * BUG word(s)
diff --git a/arch/x86/include/asm/hreset.h b/arch/x86/include/asm/hreset.h
new file mode 100644
index 0000000000000..ae1f72602bbd0
--- /dev/null
+++ b/arch/x86/include/asm/hreset.h
@@ -0,0 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_X86_HRESET_H
+
+void reset_hardware_history_hetero(void);
+
+#endif /* _ASM_X86_HRESET_H */
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 82c6a4d350e09..a70c1475725af 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -690,6 +690,11 @@
 #define MSR_AMD64_PERF_CNTR_GLOBAL_CTL		0xc0000301
 #define MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR	0xc0000302
 
+/* AMD Hardware Feedback Support MSRs */
+#define AMD_WORKLOAD_CLASS_CONFIG      0xc0000500
+#define AMD_WORKLOAD_CLASS_ID          0xc0000501
+#define AMD_WORKLOAD_HRST              0xc0000502
+
 /* AMD Last Branch Record MSRs */
 #define MSR_AMD64_LBR_SELECT			0xc000010e
 
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d4e539d4e158c..2ef34669fcb68 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -57,6 +57,7 @@
 #include <asm/mce.h>
 #include <asm/msr.h>
 #include <asm/cacheinfo.h>
+#include <asm/hreset.h>
 #include <asm/memtype.h>
 #include <asm/microcode.h>
 #include <asm/intel-family.h>
@@ -398,6 +399,14 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 	cr4_clear_bits(X86_CR4_UMIP);
 }
 
+static u32 hardware_history_features __ro_after_init;
+
+static __always_inline void setup_hreset(struct cpuinfo_x86 *c)
+{
+	if (cpu_feature_enabled(X86_FEATURE_WORKLOAD_CLASS))
+		hardware_history_features = 1;
+}
+
 /* These bits should not change their value after CPU init is finished. */
 static const unsigned long cr4_pinned_mask = X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP |
 					     X86_CR4_FSGSBASE | X86_CR4_CET | X86_CR4_FRED;
@@ -1839,6 +1848,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	setup_smep(c);
 	setup_smap(c);
 	setup_umip(c);
+	setup_hreset(c);
 
 	/* Enable FSGSBASE instructions if available. */
 	if (cpu_has(c, X86_FEATURE_FSGSBASE)) {
@@ -2392,3 +2402,11 @@ void __init arch_cpu_finalize_init(void)
 	 */
 	mem_encrypt_init();
 }
+
+__always_inline void reset_hardware_history_hetero()
+{
+	if (!hardware_history_features)
+		return;
+
+	wrmsrl(AMD_WORKLOAD_HRST, 0x1);
+}
diff --git a/arch/x86/kernel/cpu/scattered.c b/arch/x86/kernel/cpu/scattered.c
index c84c30188fdf2..8343e2e449d63 100644
--- a/arch/x86/kernel/cpu/scattered.c
+++ b/arch/x86/kernel/cpu/scattered.c
@@ -49,9 +49,11 @@ static const struct cpuid_bit cpuid_bits[] = {
 	{ X86_FEATURE_MBA,		CPUID_EBX,  6, 0x80000008, 0 },
 	{ X86_FEATURE_SMBA,		CPUID_EBX,  2, 0x80000020, 0 },
 	{ X86_FEATURE_BMEC,		CPUID_EBX,  3, 0x80000020, 0 },
+	{ X86_FEATURE_WORKLOAD_CLASS,   CPUID_EAX,  22, 0x80000021, 0 },
 	{ X86_FEATURE_PERFMON_V2,	CPUID_EAX,  0, 0x80000022, 0 },
 	{ X86_FEATURE_AMD_LBR_V2,	CPUID_EAX,  1, 0x80000022, 0 },
 	{ X86_FEATURE_AMD_LBR_PMC_FREEZE,	CPUID_EAX,  2, 0x80000022, 0 },
+	{ X86_FEATURE_HETERO_CORE_TOPOLOGY,     CPUID_EAX,  30, 0x80000026, 0 },
 	{ 0, 0, 0, 0, 0 }
 };
 
diff --git a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c
index 0917c7f25720b..6a3a1339f7a77 100644
--- a/arch/x86/kernel/process_32.c
+++ b/arch/x86/kernel/process_32.c
@@ -52,6 +52,7 @@
 #include <asm/switch_to.h>
 #include <asm/vm86.h>
 #include <asm/resctrl.h>
+#include <asm/hreset.h>
 #include <asm/proto.h>
 
 #include "process.h"
@@ -213,6 +214,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* Load the Intel cache allocation PQR MSR. */
 	resctrl_sched_in(next_p);
 
+	reset_hardware_history_hetero();
+
 	return prev_p;
 }
 
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6d3d20e3e43a9..096ac69bb8dbd 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -54,6 +54,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
 #include <asm/resctrl.h>
+#include <asm/hreset.h>
 #include <asm/unistd.h>
 #include <asm/fsgsbase.h>
 #include <asm/fred.h>
@@ -709,6 +710,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	/* Load the Intel cache allocation PQR MSR. */
 	resctrl_sched_in(next_p);
 
+	reset_hardware_history_hetero();
+
 	return prev_p;
 }
 
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 90a6fb54a1283..7681037d8942f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -504,7 +504,8 @@ static int x86_cluster_flags(void)
 
 static int x86_die_flags(void)
 {
-	if (cpu_feature_enabled(X86_FEATURE_HYBRID_CPU))
+	if (cpu_feature_enabled(X86_FEATURE_HYBRID_CPU) ||
+			cpu_feature_enabled(X86_FEATURE_HETERO_CORE_TOPOLOGY))
 	       return x86_sched_itmt_flags();
 
 	return 0;
diff --git a/drivers/platform/x86/amd/Kconfig b/drivers/platform/x86/amd/Kconfig
index f88682d36447c..c3f69dbe3037d 100644
--- a/drivers/platform/x86/amd/Kconfig
+++ b/drivers/platform/x86/amd/Kconfig
@@ -5,6 +5,7 @@
 
 source "drivers/platform/x86/amd/pmf/Kconfig"
 source "drivers/platform/x86/amd/pmc/Kconfig"
+source "drivers/platform/x86/amd/hfi/Kconfig"
 
 config AMD_HSMP
 	tristate "AMD HSMP Driver"
diff --git a/drivers/platform/x86/amd/Makefile b/drivers/platform/x86/amd/Makefile
index dcec0a46f8af1..2676fc81fee54 100644
--- a/drivers/platform/x86/amd/Makefile
+++ b/drivers/platform/x86/amd/Makefile
@@ -9,3 +9,4 @@ amd_hsmp-y			:= hsmp.o
 obj-$(CONFIG_AMD_HSMP)		+= amd_hsmp.o
 obj-$(CONFIG_AMD_PMF)		+= pmf/
 obj-$(CONFIG_AMD_WBRF)		+= wbrf.o
+obj-$(CONFIG_AMD_HFI)		+= hfi/
diff --git a/drivers/platform/x86/amd/hfi/Kconfig b/drivers/platform/x86/amd/hfi/Kconfig
new file mode 100644
index 0000000000000..4671cc1037a0b
--- /dev/null
+++ b/drivers/platform/x86/amd/hfi/Kconfig
@@ -0,0 +1,21 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# AMD Hardware Feedback Interface Driver
+#
+
+config AMD_HFI
+	bool "AMD Hetero Core Hardware Feedback Driver"
+	depends on ACPI
+	depends on CPU_SUP_AMD
+	select IPC_CLASSES
+	help
+	 Select this option to enable the AMD Heterogeneous Core Hardware Feedback Interface. If
+	 selected, hardware provides runtime thread classification guidance to the operating system
+	 on the performance and energy efficiency capabilities of each heterogeneous CPU core.
+	 These capabilities may vary due to the inherent differences in the core types and can
+	 also change as a result of variations in the operating conditions of the system such
+	 as power and thermal limits. If selected, the kernel relays updates in heterogeneous
+	 CPUs' capabilities to userspace, allowing for more optimal task scheduling and
+	 resource allocation, leveraging the diverse set of cores available.
+
+
diff --git a/drivers/platform/x86/amd/hfi/Makefile b/drivers/platform/x86/amd/hfi/Makefile
new file mode 100644
index 0000000000000..672c6ac106e95
--- /dev/null
+++ b/drivers/platform/x86/amd/hfi/Makefile
@@ -0,0 +1,7 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# AMD Hardware Feedback Interface Driver
+#
+
+obj-$(CONFIG_AMD_HFI) += amd_hfi.o
+amd_hfi-objs := hfi.o
diff --git a/drivers/platform/x86/amd/hfi/hfi.c b/drivers/platform/x86/amd/hfi/hfi.c
new file mode 100644
index 0000000000000..c3da2edf85905
--- /dev/null
+++ b/drivers/platform/x86/amd/hfi/hfi.c
@@ -0,0 +1,665 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * AMD Hardware Feedback Interface Driver
+ *
+ * Copyright (C) 2024 Advanced Micro Devices, Inc. All Rights Reserved.
+ *
+ * Author: Perry Yuan <Perry.Yuan@amd.com>
+ *
+ */
+
+#define pr_fmt(fmt)  "amd-hfi: " fmt
+
+#include <linux/acpi.h>
+#include <linux/bitops.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/gfp.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mailbox_client.h>
+#include <linux/mutex.h>
+#include <linux/percpu-defs.h>
+#include <linux/platform_device.h>
+#include <linux/printk.h>
+#include <linux/smp.h>
+#include <linux/string.h>
+#include <linux/topology.h>
+#include <linux/workqueue.h>
+
+#include <asm/cpu_device_id.h>
+
+#include <acpi/pcc.h>
+#include <acpi/cppc_acpi.h>
+
+#define AMD_HFI_DRIVER		"amd_hfi"
+#define AMD_HETERO_CPUID_27	0x80000027
+#define AMD_HETERO_RANKING_TABLE_VER	2
+static struct platform_device *device;
+
+/**
+ * struct amd_core_rank_table - HFI capabilities for the logical
+ * processors in the memory mappep table.
+ *
+ * @signature:	The PCC signature. The signature of a subspace is computed by
+ *		a bitwise of the value 0x50434300 with the subspace ID.
+ * @flags:	Notify on completion
+ * @length:	Length of payload being transmitted including command field
+ * @command:	Command being sent over the subspace
+ * @version_number:		Version number of the table
+ * @n_logical_processors:	Number of logical processors
+ * @n_capabilities:		Number of ranking dimensions (performance, efficiency, etc)
+ * @table_update_context:	Command being sent over the subspace
+ * @n_bitmaps:			Number of 32-bit bitmaps to enumerate all the APIC IDs
+ *				This is based on the maximum apic ID enumerated in the system
+ * @reserved:			The 24bit spare
+ * @bitmap_of_apic_id0:		Bit Map of enabled logical processors APIC ID for 31:0
+ * @bitmap_of_apic_id1:		Bit Map of enabled logical processors APIC ID for 64:32
+ * @n_classes:			Number of workload class
+ * @dynamic_rank_feature:	Table update mode
+ * @diagnostics:		Reserved space for diagnostics
+ * @timestamp:			Timestamp for last table update
+ * @table_size:			Table length of shared memory
+ * @shmem_info:			The table data read from shared memory
+ * @bitmap_data:		Bitmap data read from table
+ * @max_index:			The max data array in the table
+ *
+ * A memory mapped table is used to express the capabilities of each logical
+ * processor for each thread classification. with dynamic table update feature
+ * supported, the table will be notified to update for all the cores while system
+ * running, each table update can reorder the cores for much better performance and
+ * power efficiency.
+ *
+ */
+struct amd_hfi_metadata {
+	u32	signature;
+	u32	flags:1;
+	u32	length;
+	u32	command;
+	u8	version_number;
+	u8	n_logical_processors;
+	u8	n_capabilities;
+	u8	table_update_context;
+	u8	n_bitmaps;
+	u32	reserved:24;
+	u32	bitmap_of_apic_id0;
+	u32	bitmap_of_apic_id1;
+	u8	n_classes;
+	bool	dynamic_rank_feature;
+	int	diagnostics;
+	u64	timestamp;
+	u64	table_size;
+	u32	shmem_info[64];
+	u32	bitmap_data;
+	u32	max_index;
+};
+
+struct amd_hfi_data {
+	const char	*name;
+	struct device	*dev;
+	struct mutex	lock;
+	acpi_handle	dhandle;
+
+	/* PCCT table related*/
+	int		plat_irq;
+	struct pcc_mbox_chan	*pcc_chan;
+	void __iomem		*pcc_comm_addr;
+	struct completion	done;
+	struct mbox_client	cl;
+	raw_spinlock_t		table_lock;
+	struct acpi_subtable_header	*pcct_entry;
+	struct amd_hfi_metadata		*hfi_meta;
+	raw_spinlock_t		hfi_data_lock;
+};
+
+/**
+ * struct amd_hfi_classes - HFI class capabilities per CPU
+ * @perf:		Performance capability
+ * @eff:		Power efficiency capability
+ *
+ * Capabilities of a logical processor in the rank table. These capabilities are
+ * unitless and specific to each HFI class.
+ */
+struct amd_hfi_classes {
+	u32	perf;
+	u32	eff;
+} __packed;
+
+/**
+ * struct amd_hfi_cpuinfo - HFI workload class info per CPU
+ * @cpu:		cpu index
+ * @cpus:		cpu mask of cpus
+ * @apic_id:		apic id of the current cpu
+ * @class_index:	workload class ID index
+ * @nr_classa:		max number of workload class supported
+ * @amd_hfi_classes:	current cpu workload class ranking data
+ *
+ * Parameters of a logical processor linked with hardware feedback class
+ */
+struct amd_hfi_cpuinfo {
+	int		cpu;
+	cpumask_var_t	cpus;
+	u32		apic_id;
+	s16		class_index;
+	u8		nr_class;
+	struct amd_hfi_classes	*amd_hfi_classes;
+};
+
+static DEFINE_PER_CPU(struct amd_hfi_cpuinfo, amd_hfi_cpuinfo) = {.class_index = -1};
+
+static DEFINE_MUTEX(hfi_cpuinfo_lock);
+static int __percpu *amd_hfi_ipcc_scores;
+
+static int amd_set_hfi_ipcc_score(struct amd_hfi_cpuinfo *info, int cpu);
+static int update_hfi_ipcc_scores(struct amd_hfi_data *amd_hfi_data);
+static int amd_hfi_set_state(unsigned int cpu, bool state);
+
+static int find_cpu_index_by_apicid(unsigned int target_apicid)
+{
+	int cpu_index;
+
+	for_each_possible_cpu(cpu_index) {
+		struct cpuinfo_x86 *info = &cpu_data(cpu_index);
+
+		if (info->topo.apicid == target_apicid) {
+			pr_debug(" match apic id %d for cpu index: %d",
+						info->topo.apicid, cpu_index);
+			return cpu_index;
+		}
+	}
+
+	return -ENODEV;
+}
+
+static int amd_hfi_fill_metadata(struct amd_hfi_data *amd_hfi_data)
+{
+	struct pcc_mbox_chan *pcc_chan = amd_hfi_data->pcc_chan;
+	struct acpi_subtable_header *pcct_entry = amd_hfi_data->pcct_entry;
+	struct acpi_pcct_ext_pcc_slave *pcct_ext =
+		(struct acpi_pcct_ext_pcc_slave *)pcct_entry;
+	struct amd_hfi_metadata *meta = amd_hfi_data->hfi_meta;
+	u32 header_index = 0, data_index = 0;
+	struct amd_hfi_cpuinfo *info;
+	u32 offset, offset_begin;
+	void __iomem *pcc_comm_addr;
+	int idx, ret, length;
+	u32 *shmem_info;
+
+	length = pcct_ext->length;
+	if (length <= 0) {
+		pr_err("length is less than min table length required\n");
+		return -EINVAL;
+	}
+
+	shmem_info = devm_kmalloc_array(amd_hfi_data->dev, length, sizeof(u32), GFP_KERNEL);
+	if (!shmem_info) {
+		pr_err("failed to allocate memory %x\n", length);
+		return -ENOMEM;
+	}
+
+	pcc_chan->shmem_base_addr = pcct_ext->base_address;
+	pcc_chan->shmem_size = pcct_ext->length;
+
+	amd_hfi_data->plat_irq = pcct_ext->platform_interrupt;
+	if (amd_hfi_data->plat_irq < 0) {
+		pr_err("invalid irq allocated in pcct table\n");
+		return -EINVAL;
+	}
+
+	pcc_comm_addr = acpi_os_ioremap(pcc_chan->shmem_base_addr, pcc_chan->shmem_size);
+	if (!pcc_comm_addr) {
+		pr_err("failed to ioremap PCC common region mem\n");
+		return -ENOMEM;
+	}
+
+	raw_spin_lock(&amd_hfi_data->table_lock);
+
+	memcpy_fromio(shmem_info, (u32 __iomem *)pcc_comm_addr, length);
+
+	/* extended PCC subspace shared memory region */
+	meta->signature = shmem_info[header_index];
+	meta->flags = shmem_info[++header_index];
+	meta->length = shmem_info[++header_index];
+	meta->command = shmem_info[++header_index];
+	idx = header_index + 1;
+
+	/* shared memory region for cores ranking data */
+	meta->version_number = shmem_info[idx] & 0xFF;
+	meta->n_logical_processors = (shmem_info[idx] >> 8) & 0xFF;
+	meta->n_capabilities = (shmem_info[idx] >> 16) & 0xFF;
+	meta->table_update_context = (shmem_info[idx] >> 24) & 0xFF;
+	meta->n_bitmaps = shmem_info[++idx] & 0xFF;
+	meta->n_classes = (shmem_info[idx] >> 8) & 0xFF;
+	meta->bitmap_data = shmem_info[++idx];
+	meta->max_index = meta->n_bitmaps * 32;
+
+	if (meta->version_number == AMD_HETERO_RANKING_TABLE_VER)
+		offset_begin = idx + 1;
+
+	for (u32 bit_idx = 0; bit_idx < meta->max_index; bit_idx++) {
+		if (meta->bitmap_data & (1u << bit_idx)) {
+			int cpu_index = find_cpu_index_by_apicid(bit_idx);
+			if (cpu_index < 0) {
+				ret = -ENODEV;
+				goto err_map;
+			}
+
+			info = per_cpu_ptr(&amd_hfi_cpuinfo, cpu_index);
+
+			offset = data_index * 6 + offset_begin;
+			for (int i = 0; i < meta->n_classes; i++) {
+				info->amd_hfi_classes[i].eff = shmem_info[offset + 2 * i];
+				info->amd_hfi_classes[i].perf = shmem_info[offset + 2 * i + 1];
+			}
+		} else {
+			continue;
+		}
+		data_index++;
+	}
+	raw_spin_unlock(&amd_hfi_data->table_lock);
+	iounmap(pcc_comm_addr);
+
+	return 0;
+
+err_map:
+	raw_spin_unlock(&amd_hfi_data->table_lock);
+	return ret;
+}
+
+static int amd_hfi_alloc_class_data(struct platform_device *pdev)
+{
+	struct amd_hfi_cpuinfo *hfi_cpuinfo;
+	struct device *dev = &pdev->dev;
+	int idx;
+	int nr_class_id;
+
+	nr_class_id = cpuid_eax(AMD_HETERO_CPUID_27);
+	if (nr_class_id < 0 || nr_class_id > 255) {
+		dev_warn(dev, "failed to get supported class number from CPUID %d\n",
+				AMD_HETERO_CPUID_27);
+		return -EINVAL;
+	}
+
+	for_each_possible_cpu(idx) {
+		hfi_cpuinfo = per_cpu_ptr(&amd_hfi_cpuinfo, idx);
+		hfi_cpuinfo->amd_hfi_classes = devm_kmalloc(dev, nr_class_id *
+				sizeof(struct amd_hfi_classes), GFP_KERNEL);
+		if (!hfi_cpuinfo->amd_hfi_classes) {
+			pr_err("failed to allocate memory\n");
+			return -ENOMEM;
+		}
+
+		hfi_cpuinfo->nr_class = nr_class_id;
+	}
+
+	return 0;
+}
+
+static void amd_hfi_remove(struct platform_device *pdev)
+{
+	struct amd_hfi_data *dev = platform_get_drvdata(pdev);
+
+	mutex_destroy(&dev->lock);
+}
+
+static int amd_hfi_pm_resume(struct device *dev)
+{
+	int cpu, err;
+
+	for_each_present_cpu(cpu) {
+		err = amd_hfi_set_state(cpu, true);
+		if (err < 0) {
+			dev_err(dev, "failed to enable workload class config: %d\n", err);
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static int amd_hfi_pm_suspend(struct device *dev)
+{
+	int err, cpu;
+
+	for_each_possible_cpu(cpu) {
+		err = amd_hfi_set_state(cpu, false);
+		if (err < 0) {
+			dev_err(dev, "failed to disable workload class config: %d\n", err);
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static DEFINE_SIMPLE_DEV_PM_OPS(amd_hfi_pm_ops,
+		amd_hfi_pm_suspend, amd_hfi_pm_resume);
+
+static int amd_set_hfi_ipcc_score(struct amd_hfi_cpuinfo *hfi_cpuinfo, int cpu)
+{
+	int i, *hfi_scores;
+	u8 nr_classes = hfi_cpuinfo->nr_class;
+
+	hfi_scores = per_cpu_ptr(amd_hfi_ipcc_scores, cpu);
+	if (!hfi_scores)
+		return -ENODEV;
+
+	for (i = 0;  i < nr_classes; i++)
+		WRITE_ONCE(hfi_scores[i], hfi_cpuinfo->amd_hfi_classes[i].perf);
+
+	return 0;
+}
+
+static int amd_hfi_set_state(unsigned int cpu, bool state)
+{
+	int ret;
+
+	ret = wrmsrl_on_cpu(cpu, AMD_WORKLOAD_CLASS_CONFIG, state);
+	if (ret)
+		return ret;
+
+	return wrmsrl_on_cpu(cpu, AMD_WORKLOAD_HRST, 0x1);
+}
+
+/*
+ * amd_hfi_online() - Enable workload classification on @cpu
+ * @cpu: CPU in which the workload classification will be enabled
+ *
+ */
+static int amd_hfi_online(unsigned int cpu)
+{
+	struct amd_hfi_cpuinfo *hfi_info = per_cpu_ptr(&amd_hfi_cpuinfo, cpu);
+	struct amd_hfi_classes *hfi_classes;
+	int ret;
+
+	if (WARN_ON_ONCE(!hfi_info))
+		return -EINVAL;
+
+	if (!zalloc_cpumask_var(&hfi_info->cpus, GFP_KERNEL))
+		return -ENOMEM;
+
+	mutex_lock(&hfi_cpuinfo_lock);
+	cpumask_set_cpu(cpu, hfi_info->cpus);
+
+	/*
+	 * Check if @cpu as an associated, initialized and ranking data must be filled
+	 */
+	hfi_classes = hfi_info->amd_hfi_classes;
+	if (!hfi_classes)
+		goto unlock;
+
+	/* Enable the workload classification interface */
+	ret = amd_hfi_set_state(cpu, true);
+	if (ret)
+		pr_err("wct enable failed for cpu %d\n", cpu);
+
+	mutex_unlock(&hfi_cpuinfo_lock);
+	return 0;
+
+unlock:
+	free_cpumask_var(hfi_info->cpus);
+	mutex_unlock(&hfi_cpuinfo_lock);
+	return ret;
+}
+
+/*
+ * amd_hfi_offline() - Disable workload classification on @cpu
+ * @cpu: CPU in which the workload classification will be disabled
+ *
+ * Remove @cpu from those covered by its HFI instance.
+ *
+ */
+static int amd_hfi_offline(unsigned int cpu)
+{
+	struct amd_hfi_cpuinfo *hfi_info = &per_cpu(amd_hfi_cpuinfo, cpu);
+	int ret;
+
+	if (WARN_ON_ONCE(!hfi_info))
+		return -EINVAL;
+
+	mutex_lock(&hfi_cpuinfo_lock);
+
+	/* Disable the workload classification interface */
+	ret = amd_hfi_set_state(cpu, false);
+	if (ret)
+		pr_err("wct disable failed for cpu %d\n", cpu);
+
+	mutex_unlock(&hfi_cpuinfo_lock);
+
+	free_cpumask_var(hfi_info->cpus);
+
+	return 0;
+}
+
+static int update_hfi_ipcc_scores(struct amd_hfi_data *amd_hfi_data)
+{
+	int cpu;
+	int ret;
+
+	raw_spin_lock_irq(&amd_hfi_data->hfi_data_lock);
+	for_each_online_cpu(cpu) {
+		struct amd_hfi_cpuinfo *hfi_cpuinfo = per_cpu_ptr(&amd_hfi_cpuinfo, cpu);
+
+		ret = amd_set_hfi_ipcc_score(hfi_cpuinfo, cpu);
+		if (ret)
+			return ret;
+	}
+	raw_spin_unlock_irq(&amd_hfi_data->hfi_data_lock);
+
+	return 0;
+}
+
+static int amd_hfi_metadata_parser(struct platform_device *pdev,
+		struct amd_hfi_data *amd_hfi_data)
+{
+	struct mbox_chan *pcc_mbox_channels;
+	struct pcc_mbox_chan *pcc_chan;
+	struct acpi_subtable_header *pcct_entry;
+	struct acpi_table_header *pcct_tbl;
+	struct device *dev = &pdev->dev;
+	acpi_status status;
+	int ret = 0, count = 1;
+
+	status = acpi_get_table(ACPI_SIG_PCCT, 0, &pcct_tbl);
+	if (ACPI_FAILURE(status) || !pcct_tbl) {
+		pr_err("acpi_get_table failed!\n");
+		return -ENODEV;
+	}
+
+	pcc_mbox_channels = devm_kcalloc(dev, count,
+			sizeof(*pcc_mbox_channels), GFP_KERNEL);
+	if (!pcc_mbox_channels) {
+		ret = -ENOMEM;
+		goto exit_err;
+	}
+
+	pcc_chan = devm_kcalloc(dev, count, sizeof(*pcc_chan), GFP_KERNEL);
+	if (!pcc_chan) {
+		ret = -ENOMEM;
+		goto exit_err;
+	}
+
+	/* get pointer to the first PCC subspace entry */
+	pcct_entry = (struct acpi_subtable_header *) (
+			(unsigned long) pcct_tbl + sizeof(struct acpi_table_pcct));
+
+	pcc_chan->mchan = &pcc_mbox_channels[0];
+
+	amd_hfi_data->pcc_chan = pcc_chan;
+	amd_hfi_data->pcct_entry = pcct_entry;
+
+	/* parse the shared memory info from the pcct table */
+	ret = amd_hfi_fill_metadata(amd_hfi_data);
+	if (ret) {
+		pr_err("failed to parse core ranking table\n");
+		ret = -ENODATA;
+	}
+
+exit_err:
+	acpi_put_table(pcct_tbl);
+	return ret;
+}
+
+static int alloc_amd_hfi_ipcc_scores(struct amd_hfi_data *amd_hfi_data)
+{
+	struct amd_hfi_metadata *hfi_meta = amd_hfi_data->hfi_meta;
+
+	amd_hfi_ipcc_scores = __alloc_percpu(sizeof(*amd_hfi_ipcc_scores) *
+			hfi_meta->n_classes,
+			sizeof(*amd_hfi_ipcc_scores));
+	if (WARN_ON(!amd_hfi_ipcc_scores))
+		return -ENOMEM;
+
+	return 0;
+}
+
+static const struct acpi_device_id amd_hfi_platform_match[] = {
+	{ "AMDI0104", 0},
+	{ }
+};
+MODULE_DEVICE_TABLE(acpi, amd_hfi_platform_match);
+
+static int amd_hfi_probe(struct platform_device *pdev)
+{
+	struct amd_hfi_data *amd_hfi_data;
+	const struct acpi_device_id *id;
+	struct acpi_device *acpi_dev;
+	acpi_handle dhandle;
+	int ret;
+
+	id = acpi_match_device(amd_hfi_platform_match, &pdev->dev);
+	if (!id)
+		return -ENODEV;
+
+	amd_hfi_data = devm_kzalloc(&pdev->dev,
+			sizeof(*amd_hfi_data), GFP_KERNEL);
+	if (!amd_hfi_data)
+		return -ENOMEM;
+
+	amd_hfi_data->hfi_meta = devm_kzalloc(&pdev->dev,
+					sizeof(*amd_hfi_data->hfi_meta), GFP_KERNEL);
+	if (!amd_hfi_data->hfi_meta)
+		return -ENOMEM;
+	amd_hfi_data->dev = &pdev->dev;
+	dhandle = ACPI_HANDLE(&pdev->dev);
+	if (!dhandle) {
+		dev_err(&pdev->dev, "dhandle is null\n");
+		return -ENODEV;
+	}
+
+	acpi_dev = acpi_fetch_acpi_dev(dhandle);
+	if (!acpi_dev)
+		return -ENODEV;
+
+	amd_hfi_data->dhandle = dhandle;
+
+	raw_spin_lock_init(&amd_hfi_data->table_lock);
+	raw_spin_lock_init(&amd_hfi_data->hfi_data_lock);
+	mutex_init(&amd_hfi_data->lock);
+
+	platform_set_drvdata(pdev, amd_hfi_data);
+
+	/* alloc data array for hardware feedback class data */
+	ret = amd_hfi_alloc_class_data(pdev);
+	if (ret)
+		return -ENOMEM;
+
+	ret = amd_hfi_metadata_parser(pdev, amd_hfi_data);
+	if (ret) {
+		dev_err(&pdev->dev, "failed to parse PCCT table data with %d.\n", ret);
+		goto err_exit;
+	}
+
+	amd_hfi_data->hfi_meta->dynamic_rank_feature =
+					cpuid_ebx(AMD_HETERO_CPUID_27) & 0xF;
+
+	if (alloc_amd_hfi_ipcc_scores(amd_hfi_data))
+		goto err_exit;
+
+	ret = update_hfi_ipcc_scores(amd_hfi_data);
+	if (ret)
+		goto err_exit;
+
+	ret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, "x86/amd_hfi:online",
+			amd_hfi_online,
+			amd_hfi_offline);
+	if (ret < 0) {
+		pr_warn("failed to setup cpuhp state! (%d)\n", ret);
+		return ret;
+	}
+
+	dev_dbg(&pdev->dev, "%s driver registered.\n", pdev->name);
+
+	return 0;
+
+err_exit:
+	return ret;
+}
+
+static struct platform_driver amd_hfi_driver = {
+	.driver = {
+		.name = AMD_HFI_DRIVER,
+		.owner = THIS_MODULE,
+		.pm	= &amd_hfi_pm_ops,
+		.acpi_match_table = ACPI_PTR(amd_hfi_platform_match),
+	},
+	.probe = amd_hfi_probe,
+	.remove_new = amd_hfi_remove,
+};
+
+static int amd_platform_hfi_init(void)
+{
+	struct platform_device *pdev;
+	int ret;
+
+	pdev = platform_device_register_simple(AMD_HFI_DRIVER, -1, NULL, 0);
+	if (IS_ERR(pdev)) {
+		pr_err("unable to register hfi platform device\n");
+		return PTR_ERR(pdev);
+	}
+
+	ret = platform_driver_register(&amd_hfi_driver);
+	if (ret) {
+		pr_err("Failed to register hfi driver\n");
+	}
+
+	return ret;
+}
+
+static int __init amd_hfi_init(void)
+{
+	int ret;
+
+	if (acpi_disabled)
+		return -ENODEV;
+
+	if (!boot_cpu_has(X86_FEATURE_HETERO_CORE_TOPOLOGY)) {
+		pr_debug("amd Hetero Core feature reporting not supported!\n");
+		return -ENODEV;
+	}
+
+	if (!boot_cpu_has(X86_FEATURE_WORKLOAD_CLASS)) {
+		pr_debug("workload class reporting not supported!\n");
+		return -ENODEV;
+	}
+
+	/* platform PCC Subspace Type 4 driver init */
+	ret = amd_platform_hfi_init();
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static __exit void amd_hfi_exit(void)
+{
+	platform_device_unregister(device);
+	platform_driver_unregister(&amd_hfi_driver);
+}
+module_init(amd_hfi_init);
+module_exit(amd_hfi_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("AMD Hardware Feedback Interface Driver");
-- 
2.46.0

