commit 778c558f49a2cb3dc7b18a80ff515e82aa813627
Author: Barry Song <song.bao.hua@hisilicon.com>
Date:   Fri Sep 24 20:51:03 2021 +1200

    sched: Add cluster scheduler level in core and related Kconfig for ARM64
    
    This patch adds scheduler level for clusters and automatically enables
    the load balance among clusters. It will directly benefit a lot of
    workload which loves more resources such as memory bandwidth, caches.
    
    Testing has widely been done in two different hardware configurations of
    Kunpeng920:
    
     24 cores in one NUMA(6 clusters in each NUMA node);
     32 cores in one NUMA(8 clusters in each NUMA node)
    
    Workload is running on either one NUMA node or four NUMA nodes, thus,
    this can estimate the effect of cluster spreading w/ and w/o NUMA load
    balance.
    
    * Stream benchmark:
    
    4threads stream (on 1NUMA * 24cores = 24cores)
                    stream                 stream
                    w/o patch              w/ patch
    MB/sec copy     29929.64 (   0.00%)    32932.68 (  10.03%)
    MB/sec scale    29861.10 (   0.00%)    32710.58 (   9.54%)
    MB/sec add      27034.42 (   0.00%)    32400.68 (  19.85%)
    MB/sec triad    27225.26 (   0.00%)    31965.36 (  17.41%)
    
    6threads stream (on 1NUMA * 24cores = 24cores)
                    stream                 stream
                    w/o patch              w/ patch
    MB/sec copy     40330.24 (   0.00%)    42377.68 (   5.08%)
    MB/sec scale    40196.42 (   0.00%)    42197.90 (   4.98%)
    MB/sec add      37427.00 (   0.00%)    41960.78 (  12.11%)
    MB/sec triad    37841.36 (   0.00%)    42513.64 (  12.35%)
    
    12threads stream (on 1NUMA * 24cores = 24cores)
                    stream                 stream
                    w/o patch              w/ patch
    MB/sec copy     52639.82 (   0.00%)    53818.04 (   2.24%)
    MB/sec scale    52350.30 (   0.00%)    53253.38 (   1.73%)
    MB/sec add      53607.68 (   0.00%)    55198.82 (   2.97%)
    MB/sec triad    54776.66 (   0.00%)    56360.40 (   2.89%)
    
    Thus, it could help memory-bound workload especially under medium load.
    Similar improvement is also seen in lkp-pbzip2:
    
    * lkp-pbzip2 benchmark
    
    2-96 threads (on 4NUMA * 24cores = 96cores)
                      lkp-pbzip2              lkp-pbzip2
                      w/o patch               w/ patch
    Hmean     tput-2   11062841.57 (   0.00%)  11341817.51 *   2.52%*
    Hmean     tput-5   26815503.70 (   0.00%)  27412872.65 *   2.23%*
    Hmean     tput-8   41873782.21 (   0.00%)  43326212.92 *   3.47%*
    Hmean     tput-12  61875980.48 (   0.00%)  64578337.51 *   4.37%*
    Hmean     tput-21 105814963.07 (   0.00%) 111381851.01 *   5.26%*
    Hmean     tput-30 150349470.98 (   0.00%) 156507070.73 *   4.10%*
    Hmean     tput-48 237195937.69 (   0.00%) 242353597.17 *   2.17%*
    Hmean     tput-79 360252509.37 (   0.00%) 362635169.23 *   0.66%*
    Hmean     tput-96 394571737.90 (   0.00%) 400952978.48 *   1.62%*
    
    2-24 threads (on 1NUMA * 24cores = 24cores)
                     lkp-pbzip2               lkp-pbzip2
                     w/o patch                w/ patch
    Hmean     tput-2   11071705.49 (   0.00%)  11296869.10 *   2.03%*
    Hmean     tput-4   20782165.19 (   0.00%)  21949232.15 *   5.62%*
    Hmean     tput-6   30489565.14 (   0.00%)  33023026.96 *   8.31%*
    Hmean     tput-8   40376495.80 (   0.00%)  42779286.27 *   5.95%*
    Hmean     tput-12  61264033.85 (   0.00%)  62995632.78 *   2.83%*
    Hmean     tput-18  86697139.39 (   0.00%)  86461545.74 (  -0.27%)
    Hmean     tput-24 104854637.04 (   0.00%) 104522649.46 *  -0.32%*
    
    In the case of 6 threads and 8 threads, we see the greatest performance
    improvement.
    
    Similar improvement can be seen on lkp-pixz though the improvement is
    smaller:
    
    * lkp-pixz benchmark
    
    2-24 threads lkp-pixz (on 1NUMA * 24cores = 24cores)
                      lkp-pixz               lkp-pixz
                      w/o patch              w/ patch
    Hmean     tput-2   6486981.16 (   0.00%)  6561515.98 *   1.15%*
    Hmean     tput-4  11645766.38 (   0.00%) 11614628.43 (  -0.27%)
    Hmean     tput-6  15429943.96 (   0.00%) 15957350.76 *   3.42%*
    Hmean     tput-8  19974087.63 (   0.00%) 20413746.98 *   2.20%*
    Hmean     tput-12 28172068.18 (   0.00%) 28751997.06 *   2.06%*
    Hmean     tput-18 39413409.54 (   0.00%) 39896830.55 *   1.23%*
    Hmean     tput-24 49101815.85 (   0.00%) 49418141.47 *   0.64%*
    
    * SPECrate benchmark
    
    4,8,16 copies mcf_r(on 1NUMA * 32cores = 32cores)
                    Base                    Base
                    Run Time                Rate
                    -------                 ---------
    4 Copies        w/o 580 (w/ 570)        w/o 11.1 (w/ 11.3)
    8 Copies        w/o 647 (w/ 605)        w/o 20.0 (w/ 21.4, +7%)
    16 Copies       w/o 844 (w/ 844)        w/o 30.6 (w/ 30.6)
    
    32 Copies(on 4NUMA * 32 cores = 128cores)
    [w/o patch]
                     Base     Base        Base
    Benchmarks       Copies  Run Time     Rate
    --------------- -------  ---------  ---------
    500.perlbench_r      32        584       87.2  *
    502.gcc_r            32        503       90.2  *
    505.mcf_r            32        745       69.4  *
    520.omnetpp_r        32       1031       40.7  *
    523.xalancbmk_r      32        597       56.6  *
    525.x264_r            1         --            CE
    531.deepsjeng_r      32        336      109    *
    541.leela_r          32        556       95.4  *
    548.exchange2_r      32        513      163    *
    557.xz_r             32        530       65.2  *
     Est. SPECrate2017_int_base              80.3
    
    [w/ patch]
                      Base     Base        Base
    Benchmarks       Copies  Run Time     Rate
    --------------- -------  ---------  ---------
    500.perlbench_r      32        580      87.8 (+0.688%)  *
    502.gcc_r            32        477      95.1 (+5.432%)  *
    505.mcf_r            32        644      80.3 (+13.574%) *
    520.omnetpp_r        32        942      44.6 (+9.58%)   *
    523.xalancbmk_r      32        560      60.4 (+6.714%%) *
    525.x264_r            1         --           CE
    531.deepsjeng_r      32        337      109  (+0.000%) *
    541.leela_r          32        554      95.6 (+0.210%) *
    548.exchange2_r      32        515      163  (+0.000%) *
    557.xz_r             32        524      66.0 (+1.227%) *
     Est. SPECrate2017_int_base              83.7 (+4.062%)
    
    On the other hand, it is slightly helpful to CPU-bound tasks like
    kernbench:
    
    * 24-96 threads kernbench (on 4NUMA * 24cores = 96cores)
                         kernbench              kernbench
                         w/o cluster            w/ cluster
    Min       user-24    12054.67 (   0.00%)    12024.19 (   0.25%)
    Min       syst-24     1751.51 (   0.00%)     1731.68 (   1.13%)
    Min       elsp-24      600.46 (   0.00%)      598.64 (   0.30%)
    Min       user-48    12361.93 (   0.00%)    12315.32 (   0.38%)
    Min       syst-48     1917.66 (   0.00%)     1892.73 (   1.30%)
    Min       elsp-48      333.96 (   0.00%)      332.57 (   0.42%)
    Min       user-96    12922.40 (   0.00%)    12921.17 (   0.01%)
    Min       syst-96     2143.94 (   0.00%)     2110.39 (   1.56%)
    Min       elsp-96      211.22 (   0.00%)      210.47 (   0.36%)
    Amean     user-24    12063.99 (   0.00%)    12030.78 *   0.28%*
    Amean     syst-24     1755.20 (   0.00%)     1735.53 *   1.12%*
    Amean     elsp-24      601.60 (   0.00%)      600.19 (   0.23%)
    Amean     user-48    12362.62 (   0.00%)    12315.56 *   0.38%*
    Amean     syst-48     1921.59 (   0.00%)     1894.95 *   1.39%*
    Amean     elsp-48      334.10 (   0.00%)      332.82 *   0.38%*
    Amean     user-96    12925.27 (   0.00%)    12922.63 (   0.02%)
    Amean     syst-96     2146.66 (   0.00%)     2122.20 *   1.14%*
    Amean     elsp-96      211.96 (   0.00%)      211.79 (   0.08%)
    
    Note this patch isn't an universal win, it might hurt those workload
    which can benefit from packing. Though tasks which want to take
    advantages of lower communication latency of one cluster won't
    necessarily been packed in one cluster while kernel is not aware of
    clusters, they have some chance to be randomly packed. But this
    patch will make them more likely spread.
    
    Signed-off-by: Barry Song <song.bao.hua@hisilicon.com>
    Tested-by: Yicong Yang <yangyicong@hisilicon.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 5c7ae4c3954b..d13677f4731d 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -989,6 +989,15 @@ config SCHED_MC
 	  making when dealing with multi-core CPU chips at a cost of slightly
 	  increased overhead in some places. If unsure say N here.
 
+config SCHED_CLUSTER
+	bool "Cluster scheduler support"
+	help
+	  Cluster scheduler support improves the CPU scheduler's decision
+	  making when dealing with machines that have clusters of CPUs.
+	  Cluster usually means a couple of CPUs which are placed closely
+	  by sharing mid-level caches, last-level cache tags or internal
+	  busses.
+
 config SCHED_SMT
 	bool "SMT scheduler support"
 	help
diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index 8f0f778b7c91..2f9166f6dec8 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -42,6 +42,13 @@ static inline int cpu_smt_flags(void)
 }
 #endif
 
+#ifdef CONFIG_SCHED_CLUSTER
+static inline int cpu_cluster_flags(void)
+{
+	return SD_SHARE_PKG_RESOURCES;
+}
+#endif
+
 #ifdef CONFIG_SCHED_MC
 static inline int cpu_core_flags(void)
 {
diff --git a/include/linux/topology.h b/include/linux/topology.h
index 80d27d717631..0b3704ad13c8 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -212,6 +212,13 @@ static inline const struct cpumask *cpu_smt_mask(int cpu)
 }
 #endif
 
+#if defined(CONFIG_SCHED_CLUSTER) && !defined(cpu_cluster_mask)
+static inline const struct cpumask *cpu_cluster_mask(int cpu)
+{
+	return topology_cluster_cpumask(cpu);
+}
+#endif
+
 static inline const struct cpumask *cpu_cpu_mask(int cpu)
 {
 	return cpumask_of_node(cpu_to_node(cpu));
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 5af3edd34d6d..c1729f9a715f 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -1638,6 +1638,11 @@ static struct sched_domain_topology_level default_topology[] = {
 #ifdef CONFIG_SCHED_SMT
 	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
 #endif
+
+#ifdef CONFIG_SCHED_CLUSTER
+	{ cpu_clustergroup_mask, cpu_cluster_flags, SD_INIT_NAME(CLS) },
+#endif
+
 #ifdef CONFIG_SCHED_MC
 	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
 #endif
commit 66558b730f2533cc2bf2b74d51f5f80b81e2bad0
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Fri Sep 24 20:51:04 2021 +1200

    sched: Add cluster scheduler level for x86
    
    There are x86 CPU architectures (e.g. Jacobsville) where L2 cahce is
    shared among a cluster of cores instead of being exclusive to one
    single core.
    
    To prevent oversubscription of L2 cache, load should be balanced
    between such L2 clusters, especially for tasks with no shared data.
    On benchmark such as SPECrate mcf test, this change provides a boost
    to performance especially on medium load system on Jacobsville.  on a
    Jacobsville that has 24 Atom cores, arranged into 6 clusters of 4
    cores each, the benchmark number is as follow:
    
     Improvement over baseline kernel for mcf_r
     copies         run time        base rate
     1              -0.1%           -0.2%
     6              25.1%           25.1%
     12             18.8%           19.0%
     24             0.3%            0.3%
    
    So this looks pretty good. In terms of the system's task distribution,
    some pretty bad clumping can be seen for the vanilla kernel without
    the L2 cluster domain for the 6 and 12 copies case. With the extra
    domain for cluster, the load does get evened out between the clusters.
    
    Note this patch isn't an universal win as spreading isn't necessarily
    a win, particually for those workload who can benefit from packing.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Barry Song <song.bao.hua@hisilicon.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20210924085104.44806-4-21cnbao@gmail.com

diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index ab83c22d274e..349e59b2f0e3 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1001,6 +1001,17 @@ config NR_CPUS
 	  This is purely to save memory: each supported CPU adds about 8KB
 	  to the kernel image.
 
+config SCHED_CLUSTER
+	bool "Cluster scheduler support"
+	depends on SMP
+	default y
+	help
+	  Cluster scheduler support improves the CPU scheduler's decision
+	  making when dealing with machines that have clusters of CPUs.
+	  Cluster usually means a couple of CPUs which are placed closely
+	  by sharing mid-level caches, last-level cache tags or internal
+	  busses.
+
 config SCHED_SMT
 	def_bool y if SMP
 
diff --git a/arch/x86/include/asm/smp.h b/arch/x86/include/asm/smp.h
index 630ff08532be..08b0e90623ad 100644
--- a/arch/x86/include/asm/smp.h
+++ b/arch/x86/include/asm/smp.h
@@ -16,7 +16,9 @@ DECLARE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_core_map);
 DECLARE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_die_map);
 /* cpus sharing the last level cache: */
 DECLARE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
+DECLARE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_l2c_shared_map);
 DECLARE_PER_CPU_READ_MOSTLY(u16, cpu_llc_id);
+DECLARE_PER_CPU_READ_MOSTLY(u16, cpu_l2c_id);
 DECLARE_PER_CPU_READ_MOSTLY(int, cpu_number);
 
 static inline struct cpumask *cpu_llc_shared_mask(int cpu)
@@ -24,6 +26,11 @@ static inline struct cpumask *cpu_llc_shared_mask(int cpu)
 	return per_cpu(cpu_llc_shared_map, cpu);
 }
 
+static inline struct cpumask *cpu_l2c_shared_mask(int cpu)
+{
+	return per_cpu(cpu_l2c_shared_map, cpu);
+}
+
 DECLARE_EARLY_PER_CPU_READ_MOSTLY(u16, x86_cpu_to_apicid);
 DECLARE_EARLY_PER_CPU_READ_MOSTLY(u32, x86_cpu_to_acpiid);
 DECLARE_EARLY_PER_CPU_READ_MOSTLY(u16, x86_bios_cpu_apicid);
diff --git a/arch/x86/include/asm/topology.h b/arch/x86/include/asm/topology.h
index 9239399e5491..cc164777e661 100644
--- a/arch/x86/include/asm/topology.h
+++ b/arch/x86/include/asm/topology.h
@@ -103,6 +103,7 @@ static inline void setup_node_to_cpumask_map(void) { }
 #include <asm-generic/topology.h>
 
 extern const struct cpumask *cpu_coregroup_mask(int cpu);
+extern const struct cpumask *cpu_clustergroup_mask(int cpu);
 
 #define topology_logical_package_id(cpu)	(cpu_data(cpu).logical_proc_id)
 #define topology_physical_package_id(cpu)	(cpu_data(cpu).phys_proc_id)
@@ -113,7 +114,9 @@ extern const struct cpumask *cpu_coregroup_mask(int cpu);
 extern unsigned int __max_die_per_package;
 
 #ifdef CONFIG_SMP
+#define topology_cluster_id(cpu)		(per_cpu(cpu_l2c_id, cpu))
 #define topology_die_cpumask(cpu)		(per_cpu(cpu_die_map, cpu))
+#define topology_cluster_cpumask(cpu)		(cpu_clustergroup_mask(cpu))
 #define topology_core_cpumask(cpu)		(per_cpu(cpu_core_map, cpu))
 #define topology_sibling_cpumask(cpu)		(per_cpu(cpu_sibling_map, cpu))
 
diff --git a/arch/x86/kernel/cpu/cacheinfo.c b/arch/x86/kernel/cpu/cacheinfo.c
index b5e36bd0425b..fe98a1465be6 100644
--- a/arch/x86/kernel/cpu/cacheinfo.c
+++ b/arch/x86/kernel/cpu/cacheinfo.c
@@ -846,6 +846,7 @@ void init_intel_cacheinfo(struct cpuinfo_x86 *c)
 		l2 = new_l2;
 #ifdef CONFIG_SMP
 		per_cpu(cpu_llc_id, cpu) = l2_id;
+		per_cpu(cpu_l2c_id, cpu) = l2_id;
 #endif
 	}
 
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0f8885949e8c..1c7897c33327 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -85,6 +85,9 @@ u16 get_llc_id(unsigned int cpu)
 }
 EXPORT_SYMBOL_GPL(get_llc_id);
 
+/* L2 cache ID of each logical CPU */
+DEFINE_PER_CPU_READ_MOSTLY(u16, cpu_l2c_id) = BAD_APICID;
+
 /* correctly size the local cpu masks */
 void __init setup_cpu_local_masks(void)
 {
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 85f6e242b6b4..5094ab0bae58 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -101,6 +101,8 @@ EXPORT_PER_CPU_SYMBOL(cpu_die_map);
 
 DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);
 
+DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_l2c_shared_map);
+
 /* Per CPU bogomips and other parameters */
 DEFINE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);
 EXPORT_PER_CPU_SYMBOL(cpu_info);
@@ -464,6 +466,21 @@ static bool match_die(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 	return false;
 }
 
+static bool match_l2c(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
+{
+	int cpu1 = c->cpu_index, cpu2 = o->cpu_index;
+
+	/* Do not match if we do not have a valid APICID for cpu: */
+	if (per_cpu(cpu_l2c_id, cpu1) == BAD_APICID)
+		return false;
+
+	/* Do not match if L2 cache id does not match: */
+	if (per_cpu(cpu_l2c_id, cpu1) != per_cpu(cpu_l2c_id, cpu2))
+		return false;
+
+	return topology_sane(c, o, "l2c");
+}
+
 /*
  * Unlike the other levels, we do not enforce keeping a
  * multicore group inside a NUMA node.  If this happens, we will
@@ -523,7 +540,7 @@ static bool match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
 }
 
 
-#if defined(CONFIG_SCHED_SMT) || defined(CONFIG_SCHED_MC)
+#if defined(CONFIG_SCHED_SMT) || defined(CONFIG_SCHED_CLUSTER) || defined(CONFIG_SCHED_MC)
 static inline int x86_sched_itmt_flags(void)
 {
 	return sysctl_sched_itmt_enabled ? SD_ASYM_PACKING : 0;
@@ -541,12 +558,21 @@ static int x86_smt_flags(void)
 	return cpu_smt_flags() | x86_sched_itmt_flags();
 }
 #endif
+#ifdef CONFIG_SCHED_CLUSTER
+static int x86_cluster_flags(void)
+{
+	return cpu_cluster_flags() | x86_sched_itmt_flags();
+}
+#endif
 #endif
 
 static struct sched_domain_topology_level x86_numa_in_package_topology[] = {
 #ifdef CONFIG_SCHED_SMT
 	{ cpu_smt_mask, x86_smt_flags, SD_INIT_NAME(SMT) },
 #endif
+#ifdef CONFIG_SCHED_CLUSTER
+	{ cpu_clustergroup_mask, x86_cluster_flags, SD_INIT_NAME(CLS) },
+#endif
 #ifdef CONFIG_SCHED_MC
 	{ cpu_coregroup_mask, x86_core_flags, SD_INIT_NAME(MC) },
 #endif
@@ -557,6 +583,9 @@ static struct sched_domain_topology_level x86_topology[] = {
 #ifdef CONFIG_SCHED_SMT
 	{ cpu_smt_mask, x86_smt_flags, SD_INIT_NAME(SMT) },
 #endif
+#ifdef CONFIG_SCHED_CLUSTER
+	{ cpu_clustergroup_mask, x86_cluster_flags, SD_INIT_NAME(CLS) },
+#endif
 #ifdef CONFIG_SCHED_MC
 	{ cpu_coregroup_mask, x86_core_flags, SD_INIT_NAME(MC) },
 #endif
@@ -584,6 +613,7 @@ void set_cpu_sibling_map(int cpu)
 	if (!has_mp) {
 		cpumask_set_cpu(cpu, topology_sibling_cpumask(cpu));
 		cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
+		cpumask_set_cpu(cpu, cpu_l2c_shared_mask(cpu));
 		cpumask_set_cpu(cpu, topology_core_cpumask(cpu));
 		cpumask_set_cpu(cpu, topology_die_cpumask(cpu));
 		c->booted_cores = 1;
@@ -602,6 +632,9 @@ void set_cpu_sibling_map(int cpu)
 		if ((i == cpu) || (has_mp && match_llc(c, o)))
 			link_mask(cpu_llc_shared_mask, cpu, i);
 
+		if ((i == cpu) || (has_mp && match_l2c(c, o)))
+			link_mask(cpu_l2c_shared_mask, cpu, i);
+
 		if ((i == cpu) || (has_mp && match_die(c, o)))
 			link_mask(topology_die_cpumask, cpu, i);
 	}
@@ -652,6 +685,11 @@ const struct cpumask *cpu_coregroup_mask(int cpu)
 	return cpu_llc_shared_mask(cpu);
 }
 
+const struct cpumask *cpu_clustergroup_mask(int cpu)
+{
+	return cpu_l2c_shared_mask(cpu);
+}
+
 static void impress_friends(void)
 {
 	int cpu;
@@ -1335,6 +1373,7 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_die_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_llc_shared_map, i), GFP_KERNEL);
+		zalloc_cpumask_var(&per_cpu(cpu_l2c_shared_map, i), GFP_KERNEL);
 	}
 
 	/*
@@ -1564,7 +1603,10 @@ static void remove_siblinginfo(int cpu)
 
 	for_each_cpu(sibling, cpu_llc_shared_mask(cpu))
 		cpumask_clear_cpu(cpu, cpu_llc_shared_mask(sibling));
+	for_each_cpu(sibling, cpu_l2c_shared_mask(cpu))
+		cpumask_clear_cpu(cpu, cpu_l2c_shared_mask(sibling));
 	cpumask_clear(cpu_llc_shared_mask(cpu));
+	cpumask_clear(cpu_l2c_shared_mask(cpu));
 	cpumask_clear(topology_sibling_cpumask(cpu));
 	cpumask_clear(topology_core_cpumask(cpu));
 	cpumask_clear(topology_die_cpumask(cpu));
